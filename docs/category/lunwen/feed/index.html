<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>论文 &#8211; 有组织在!</title>
	<atom:link href="https://uzzz.org/category/lunwen/feed" rel="self" type="application/rss+xml" />
	<link>https://uzzz.org/</link>
	<description></description>
	<lastBuildDate>Mon, 08 Jul 2019 02:55:43 +0000</lastBuildDate>
	<language>zh-CN</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.4</generator>

<image>
	<url>https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png</url>
	<title>论文 &#8211; 有组织在!</title>
	<link>https://uzzz.org/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Learning sparse network using target dropout（文末有代码链接）</title>
		<link>https://uzzz.org/article/2322.html</link>
				<pubDate>Mon, 08 Jul 2019 02:55:43 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[论文]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2322.html</guid>
				<description><![CDATA[摘要 当神经网络的权值的数量超过了从输入映射到输出需要的权值数量，神经网络会更容易优化。这里暗存了一个两个阶段的学习进程：首先学习一个大的网络，然后删除连接或隐藏的单元。但是，标准的训练并不一定会使得网络易于修剪。于是，我们介绍了一种训练神经网络的方法——target dropout（定向dropout），使其对后续剪枝具有较强的鲁棒性。在计算每次权值更新的]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<h4><a id="_0"></a>摘要</h4>
<p>当神经网络的权值的数量超过了从输入映射到输出需要的权值数量，神经网络会更容易优化。这里暗存了一个两个阶段的学习进程：首先学习一个大的网络，然后删除连接或隐藏的单元。但是，标准的训练并不一定会使得网络易于修剪。于是，我们介绍了一种训练神经网络的方法——target dropout（定向dropout），使其对后续剪枝具有较强的鲁棒性。在计算每次权值更新的梯度之前，定向dropout使用简单的自增强稀疏性准则，推测地选择一组要删除的单元或权重，然后计算剩余权重的梯度。所得到的网络对于删除集中经常出现的权值或单位的事后剪枝具有很强的鲁棒性。该方法改进了更复杂的稀疏正则器，实现简单，易于调优。</p>
<h4><a id="_2"></a>介绍</h4>
<p>神经网络是一种功能强大的模型，可以在对象识别、语音识别和机器翻译等广泛任务上达到最新水平。其中原因之一是，神经网络强大的灵活性，因为它们有许多可学习的参数。然而，这种灵活性随之而来的是过拟合，并且会增加不必要的计算和存储压力。<br /> 在模型压缩的方法方面已经做了大量的工作。一种直观的策略是稀疏化:从网络中删除权重或整个神经元。在学习过程中，可以使用稀疏性诱导规则来鼓励网络稀疏化，比如L1，L0正则化等。也可以使用post hoc剪枝：训练一个全尺寸的网络，然后根据一些修剪策略进行稀疏化。理想情况下，从任务性能方面考虑，我们将删除为任务提供最少增益的权重或单元。一般来说，寻找最优集是一个困难的组合问题，即使是一个贪婪策略也存在不现实的任务评估数量，因为通常模型有数百万个参数。因此，常用的剪枝策略侧重于快速逼近，例如删除幅度最小的权重，或者根据任务性能相对于权重的敏感性对权重进行排序，然后删除最不敏感的权重。我们希望这些近似与任务性能很好地相关，这样剪枝就会产生高度压缩的网络，同时对任务性能几乎没有负面影响，但情况可能并非总是如此。<br /> 我们的方法是基于观察到dropout正则化，在训练过程中，它通过每次向前传递时对网络进行稀疏化来增强稀疏性容忍度。这鼓励网络学习对特定形式的事后稀疏化具有鲁棒性的表示形式，在本例中，是删除随机节点集。我们的假设是，如果我们计划进行显式的事后稀疏化，那么我们可以通过将dropout应用于我们之前认为是最没用的节点集来做得更好。我们称之为定向dropout。其思想是根据一些快速的、近似的重要性度量(如大小)对权重或单位进行排序，然后将dropout主要应用于那些被认为不重要的因素。与dropout正则的观察相似，我们表明，这使得网络去学习一种表示，其中权重或单位的重要性更接近我们的近似。换句话说，网络学会了对我们选择的事后剪枝策略保持了鲁棒性。<br /> 与其他方法相比，定向dropout的优势在于，它使网络对所选择的事后剪枝策略有鲁棒性，能够对所需的稀疏模式进行密切控制，并且容易实施。该方法在广泛的体系结构和数据集上实现了令人印象深刻的稀疏率;值得注意的是，在cifar10上，ResNet-32体系结构的稀疏性为99%，测试集精度下降不到4%。</p>
<h4><a id="_7"></a>背景</h4>
<p>为了展示定向dropout，我们首先简要介绍了一些符号，并回顾了dropout和基于数值大小的剪枝的概念。</p>
<h5><a id="_9"></a>符号</h5>
<p>假设我们正处理一个特定的网络架构，用 代表从候选集 中提取的神经网络参数向量， 是参数的个数， 表示参数为 的神经网络中权值矩阵的集合。因此， 是在网络连接层与层的权重矩阵。我们只考虑权重，为了收敛而忽略偏置，在剪枝过程中，偏置并没有移除。为简洁起见，我们使用符号 ，下标o表示连接下一层到第 个输出节点的权重（即权矩阵的第 列）， 表示W中的列数， 指的是行数。每一列对应的是一个隐藏神经元，或者是或者卷积层的特征图。注意,压扁和连接所有的在 中的权重矩会恢复。</p>
<p><strong>Dropout</strong></p>
<p>我们的工作使用了两种最流行的伯努利dropout，分别是Hintion等人提出的节点dropout，和Wan等人提出的权重dropout（dropconnect），对于全连接层，输入为张量X，权重矩阵是W，输出张量为Y，掩模<br /> ，我们定义两种技术如下：</p>
<p><strong>Unit dropout：</strong></p>
<pre><code>  节点dropout在训练的每一步中，随机drop节点（通常认为是神经元）以降低以减少节点之间的依赖，防止过度拟合。
</code></pre>
<p><strong>Weight dropout:</strong></p>
<p>权重dropout在训练的每一步中随机drop单独的权重。直观地说，这是在减少层之间的连接，迫使网络在每个训练步骤中适应不同的连接。</p>
<p><strong>基于数值的剪枝</strong></p>
<p>一个比较常用的剪枝方式是基于数值的剪枝策略，这些策略认为，只有值最大的k个权重才是重要的，我们调用argmax-k从而可以获得考虑的所有元素中的top-k元素（节点或者是权重）。</p>
<p><strong>节点剪枝：</strong></p>
<p>考虑L2范数下权重矩阵的节点(列向量)（根据每一层的权重L2范数判定节点重不重要？）</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190708105358154.png" alt="在这里插入图片描述"></p>
<p>权重剪枝：（根据权重的绝对值，判定权重重不重要）<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070810541781.png" alt="在这里插入图片描述"><br /> 而在较粗的剪枝下，权值剪枝倾向于保留更多的任务性能，节点剪枝允许相当大的计算节省。特别是权值剪枝网络可以通过稀疏线性代数运算来实现，它只在足够稀疏的条件下提供加速；当节点剪枝网络在较低维张量上执行标准线性代数运算时，对于给定的固定稀疏率，这往往是一个更快的选择。</p>
<p><strong>定向dropout</strong></p>
<p>考虑一个神经网络的参数为 ，重要性判别标准（上述定义的w( )），我们希望找到一个最优参数 ，这样loss函数 就会变小，同时， ，也就是我们只保留网络最大的k个权重。确定的剪枝函数，会挑出 个，并把它们dropout。但是我们希望的是，如果低的权重值在训练期间变得重要，那么它的权重值就应该得到增加。因为，我们在这个过程中引入了随机变量，一个是定向概率 和drop概率 ，定向概率指的是我们挑选最小的 个权重作为dropout的候选集，在里面我们独立地drop掉 %的元素。这意味着每一轮定向dropout后保持的节点数为 。接下来我们会看到，结果是减少了重要子网络对不重要子网络的依赖，从而减少了由于在训练结束时进行修剪而导致的性能下降。</p>
<p><strong>重要子网络和不重要子网络之间的依赖关系</strong></p>
<p>定向dropout的目标是降低重要子集对其补集的依赖。一个常用的看法是，dropout的目的是防止节点之间相互适应，也就是，当某一节点被dropout后，剩余的网络不能再依赖于该节点对函数的贡献，必须学会通过更可靠的渠道传播该节点的信息。另一种描述是dropout最大化了同一层中节点之间的相互信息，从而减少了节点丢失的影响。与我们的方法类似，dropout可用于指导表示的属性。例如，嵌套的dropout，根据与每个单元相关联的特定下降速率，在单元之间强加“层次结构”。Dropout本身也可以解释为贝叶斯近似。<br /> 在我们特定的剪枝场景中，我们可以从一个演示的例子中得到更相关的直觉，在这个例子中，重要的子网与不重要的子网完全分离。假设一个神经网络有两个互不重叠的子网络构成，每个子网络都能够自己产生正确的输出，网络输出作为两个子网络输出的平均值。如果我们的重要性准则认为第一个子网络重要，第二个子网不重要（更具体地说，它的权值更小），然后，给不重要的子网络（第二个）加上噪声（也就是，dropout）意味着在非零概率下，我们将破坏网络输出。因为重要已经可以预测出正确的结果，为了降低loss，我们必须将不重要的子网络输出层的权值降低到零，实际上，“杀死”了那个子网，并加强了重要子网和不重要子网之间的分离。<br /> 这些解释清楚了为什么dropout应该被认为是修剪应用的自然工具。我们可以通过比较训练好的网络和没有经过定向dropout的网络，并且检查黑森矩阵和梯度来确定网络对要修剪的权值/单位的依赖关系，从而实证地确定定向dropout对权重依赖的影响。如LeCun提出，我们可以通过考虑损失变化的二阶泰勒展开来估计剪枝权值的影响，<br /> （要去掉的权重）否则就为0,。 是loss的梯度，H是黑塞矩阵。在训练结束时，如果我们找到了临界点， ，并且， ，就只留黑塞矩阵一项。在我们的实验中，我们证明了目标target降低了重要子网络和不重要子网络之间的依赖性。</p>
<h4><a id="_49"></a>相关工作</h4>
<p>神经网络的剪枝和稀疏化研究已有近30年的历史，由于在移动电话和asic等资源有限的设备上实现了剪枝和稀疏化，因此人们对神经网络的兴趣大增。早期的工作，如脑损伤（OBD）和optimal brain surgeon；最近的工作，有的利用二阶泰勒展开式对训练到局部最小值的权值周围的损失函数进行展开式，以收集选择参数剪枝顺序的策略。Han等人将权值量化与剪枝相结合，得到了令人印象深刻的网络压缩结果，大幅降低网络的空间成本。Dong等人通过各个分层独立的假设，提高了最优脑外科手术的效率。Wen等人提出在卷积滤波器上使用组Lasso，并且能够从ResNet-20网络中删除多达6层，从而增加1%的误差。<br /> 为了开发改进的剪枝启发式算法和稀疏化正则化器，已经付出了大量的努力，这些一般包括两个部分：第一种是将正则化方案纳入训练中，使重要的子网络易于识别为事后剪枝策略; 第二种是一种特殊的事后剪枝策略，它在预先训练好的网络上运行，去掉不重要的子网络。<br /> 和我们工作最相关的两个是L0正则化和变分dropout。Louizos等人改编了concrete dropout对网络权重进行调整，并调节dropout率，以使网络稀疏化。类似地，Molchanov等人将变分dropout应用于网络的权重上，并注意到前者通过选择较大的drop率隐式地稀疏了参数。除了我们的方法能更有效地缩小重要子网的大小，有针对性的dropout使用了两个直观的超参数， 和 ，并直接控制整个训练的稀疏性（即，达到预定的稀疏阈值）。相比之下，Louizos使用的hard-concrete使用了三个超参数，并且为每一个参数引入了门参数，使得可训练参数量翻倍。Molchanov等人增加了两个超参数，并将可训练参数的数量增加了一倍。在我们的实验中，我们也和L1正则化进行了比较，L1正则化的目的是使得不重要的权重趋近于零。<br /> 彩票假设验证了子网络的存在，这个子网是孤立的，其他的子网都被剪掉了，剩下的和减掉的子网络都对梯度下降法找到的函数有影响，并且可以在有或没有剩余网络的情况下训练到相同的任务性能水平。在我们的符号中，w( )就是中奖网络。该方法的有效性表明，通过规范彩票网络可以减小中奖彩票的规模。</p>
<h4><a id="_54"></a>实验</h4>
<p>我们的实验采用了resnet和wide resnet以及transfomer架构，数据库采用了cifar10,imagenet, 和WMT英德翻译数据集。对于每个基线实验，我们验证我们的网络在适当的测试集上达到了报告的精度；我们报告了不同剪枝比例的测试准确度，并比较了不同的规范化策略。此外，我们还将我们的定向dropout与标准dropout进行了比较，两种技术之间期望的下降权重的数量是匹配的（即，标准drop率等于 ）<br /> 对于我们的剪枝过程，我们对所有的权重矩阵，除了那些导致logits的，执行贪婪的层间基于的数值的剪枝，如第2.3节描述的那样。在我们的实验汇总，我们比较了目标target和以下几种方法：<br /> <strong>L1正则</strong>：将复杂成本函数 加到目标函数中，希望这一项能将不重要的权重降至零。在本次实验中，我们用 表示这个loss函数， 是L1的权重。<br /> <strong>L0正则</strong>：Louizos等人对神经网络的参数应用了concrete dropout的增强版——hard concrete dropout。将掩码应用于那些遵循hard concrete的权重，其中每个权值与决定下降速率的门控参数相关联。Concrete 分布使得L0目标函数可微，所以我们可以直接把它和我们的任务目标一起最小化。当将这些网络稀疏化到所需的稀疏率时，我们根据学习到的剪枝概率（ ），dropout那些保持最低概率权重。<br /> <strong>变分dropout</strong>:和L0正则技术很像，Molchanov等人将具有可训练dropout率的高斯dropout应用于网络权值，并将模型解释为具有特定先验的变分后验。作者指出，在训练中使用的较低的变分下界有利于更高的dropout概率，并通过实验证实，以这种方式训练的网络确实是稀疏的。<br /> <strong>Smallify</strong>：Leclerc等人在权重/节点上使用可训练的门，并使用L1正则化将门正则化为零。该技术的关键是在线剪枝条件：Smallify保持门符号的移动方差，当方差超过某一预设的阈值是，将权重/单元的关联门设置为零（使得剪权重或者是节点更有效）。在VGG网络上，这种技术被证明是非常有效的，可以达到很高的修剪率。<br /> 具体来说，我们比较了以下技术：<br /> <strong>Dropout</strong>: 标准权重或者节点dropout，dropout率为<br /> <strong>Trargeted：</strong> 定向dropout（权重变量在a表中，节点变量在b表中），剪掉 最小权重值的<br /> <strong>Variaitonal:</strong> 变分dropout，成本系数为0.01/50000<br /> <strong>：L0正则</strong>，成本系数Wie<br /> <strong>：L1正则</strong>，成本系数为<br /> <strong>Smallify</strong>:smallify 变换层用的成本系数为 ，指数移动平均衰减为0.9，方差阈值为0.5。</p>
<p>图一 没有用drop（左）和用了定向dropout（右）的矩阵，构成方式为 。权重的排序是这样的，即具有最小的幅度最后的75%的权重 (我们打算修剪的那些)。（右边的右下角基本没有剩什么了，可以减掉）右下角元素的近似于剪枝后误差的变化。请注意这两个网络之间的明显差异，有针对性的drop &#8211; out将其依赖集中在左上角，从而导致修剪后的错误变化要小得多。</p>
<h4><a id="_70"></a>分析重要子网络</h4>
<p>为了分析定向dropout的作用，我们利用小的稠密网络构造了一个玩具实验，分析了网络的权值依赖性。我们考虑的模型是一个由10个节点和ReLU激活的单隐层密集连接的网络。我们在cifar10上训练了两个这样的网络，第一个没有正则化，第二个采用的是 的定向dropout。两个网络均训练200epoch，使用学习率为0.001，没有使用冲量的随机梯度下降。<br /> 然后我们计算测试集上的梯度和黑森量，以便从方程3估计误差的变化。如下图所示<br /> 此外，我们还计算了由典型元素 构成的黑塞权积矩阵，作为权重相关性和网络独立性的估计，如图一。该矩阵是非常重要的可视化工具，由于将与要删除的权重相关的项求和，对应于计算式(1)中的第二项——这将成为训练结束时的主导项，此时梯度近似为零。图一解释清楚了定向dropout对于网络的显著作用。在图一中，我们对矩阵的行和列重新排序，使矩阵行/列的前25%对应于我们确定为重要子网络的权重的25%（即，权重值最大的），最后的75%的权值代表不重要的子网络（即，权重最小的）。定向dropout训练的网络几乎完全依赖于训练结束时最大权重的25%。然而，未经正则化训练的网络依赖于更大比例的权重，并且在标记为剪枝的参数中有许多依赖关系。</p>
<h5><a id="_74"></a>残差网络</h5>
<p>我们在残差网络上测试了定向dropout，数据集为cifar10，使用了基础的随机中心，随机水平翻转，标准化等数据增强，这种结构在计算机视觉中已经变得无处不在，并且在语言，音频领域越来越受欢迎。我们的baseline模型在256个epoch后，达到了93%的准确性，这与此前报道的ResNet-32的结果相符。<br /> 我们的权重剪枝实验表明，与目标方案相比，标准的删除方案相对较弱;标准dropout的表现比我们的没有正则化的basline模型差。我们发现，一个较高的目标dropout率适用于较大的权重比例，产生的网络只占没有正则化网络的40%的参数。<br /> 在权重和单位剪枝的情况下，变分缺失似乎比非正则化基线稍微改善了一些，但是仍然没有定向dropout做得好。L0正则化对其复杂度项系数不敏感，我们研究了 ，发现在0.1附近不能收敛，而低于 的数值则没有规律的迹象。和变分dropout相同的是，L0正则化并没有规定在网络中实现特定的修剪百分比的方法，因此，为了找到导致所需稀疏性的值，需要进行广泛的超参数搜索。作为妥协，我们搜索上述范围，并选择最有竞争力与dropout最有竞争力的设置。接下来，我们将接下来，我们将基于大小的剪枝应用于Louizos等人提供的公式13中。不幸的是，L0正则化似乎迫使模型偏离了我们用参数大小描述重要性的假设。<br /> 在表三中，如下所示，我们例举了resnet-102在Imagnet上的的剪枝结。我们注意到，这与在cifar10上训练resnet是一样的，尽管很明显，该任务使用了更多的网络容量，使得它对相对于CIFAR-10的修剪更加敏感。</p>
<h5><a id="Wide_resnet_79"></a>Wide resnet</h5>
<p>为了确保与L0规范化基线进行公平比较，我们采用了作者自己的代码库来支持定向dropout，并比较了在给定的L0实现和定向dropout下，网络对稀疏化的鲁棒性。在表四中，如下图，我们注意到，L0正则化不能真正使网络稀疏化，但对网络的精度有很强的正则化影响（证实了Louizos等人的说法。）。这进一步验证了上面的观察结果，表明L0正则化没有使ResNet体系结构变得稀疏。</p>
<h6><a id="Transformer_81"></a>Transformer</h6>
<p>Transformer网络结果代表了现在NLP任务的最好结果。为了评估我们的方法的普遍适用性，我们测量了transormer在不经过正则化的情况下对权重级剪枝的鲁棒性，并将其与应用于网络的两种目标删除设置进行了比较。<br /> Transformer结构包含了多层头的注意层和前馈(紧密连接)层，这两者我们都需要稀疏化；在多头注意层中，每一个输入的每一个头都有一个唯一的线性变换，这就是我们要稀疏化的权重矩阵。<br /> 表五a例举了在WMT上训练transformer结构的剪枝结果。在没有使用任何正则化情况下，tansformer结构不能对剪枝保持鲁棒性，但是在使用定向dropout时，我们可以在稀疏性为70%时，增加BLEU得分15分；在稀疏性为80%时，增加BLEU得分12分。<br /> 调整目标比例<br /> 在评价权重水平的Smallify我们发现，通过调优，在非常高的修剪百分比时，Smallify的性能超过了dropout (见表6)。也许有人会认为,一个像Smallify的稀疏化方案——允许不同层中有不同的剪枝率-会更灵活,更适合寻找最佳剪枝掩模;然而，我们证明了一种被我们称为Ramping targeted dropou，定向dropout变体同样也具有类似的高剪枝率的剪枝能力。此外，ramping target dropout保留了目标退出的主要好处:对稀疏率的精细控制。<br /> Ramping targeted dropou仅仅是将目标份额从0增加到 ，用指定的最终的 完成整个训练过程。在我们resnet实验中，为我们ResNet实验中,在49epoch时，我们从0增加到 的95%。在98epoch时，退火到 的100%。用相同的方式，在89个epoch中，我们把 从0线性增加到100%。（类似于中奖彩票的预热？）<br /> 使用Ramping targeted dropou我们可以实现稀疏率为99%的ResNet32时，CIFAR-10数据集上的准确性为87.03%;然而smallify最好的稀疏性为98.8%时，精度为88.13%，当我们在所有权重矩阵中执行剪枝以强制执行相等的剪枝率时，网络会迅速退化，如下图所说。</p>
<h4><a id="_90"></a>未来的工作</h4>
<p>有针对性的退出提出了一个非常简单的程序，以解耦子网络之间的依赖关系。虽然这项工作的重点是对网络稀疏化的影响，但对定向dropout的一般性解释表明，针对目标策略的潜力在于提高可解释性和结构化表示等特性。<br /> 网dropout的一个主要特性是提高节点间的层次化，这是以一个昂贵的培训过程为代价的，该过程要求对loss求和，loss等于所有可能的截断掩模的数量（对于嵌套的单层dropout，这相当于该层中的节点数; 对于多层，这相当于所有层节点的乘积）。定向的dropout可以通过简单的调整，以一种计算成本较低的方式在每一层的单元之间强加层次结构。<br /> 为了提高量化的鲁棒性，还可以对权值的位和激活的数字表示进行有针对性的删除。<br /> 应用于网络参数的定向dropout受到每个批处理只能应用一个掩码样本的影响，因此，合并flipout将会受益，它将批内梯度去除，同时保持它们的无偏。</p>
<h4><a id="_95"></a>总结</h4>
<p>我们提出了一个target drop，作为一个训练神经网络的简单高效的正则工具，对事后的剪枝有很强的鲁棒性。定向dropout的主要好处之一是实现的简单性、直观的超参数和对稀疏性的细粒度控制(包括在训练和推理期间)。定向dropout在一系列网络架构和任务中表现良好，证明了其广泛的适用性。重要的是，就像在网dropout中，我们展示了如何使用dropout作为一种工具，将先前的结构假设编码到神经网络中。这个方法为许多有趣的应用程序</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-526ced5128.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>Graph Convolutional Neural Networks for Web-Scale Recommender Systems（用于Web级推荐系统的图形卷积神经网络）</title>
		<link>https://uzzz.org/article/1588.html</link>
				<pubDate>Sun, 07 Jul 2019 01:52:26 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[DeepLearning]]></category>
		<category><![CDATA[人工智能]]></category>
		<category><![CDATA[大数据]]></category>
		<category><![CDATA[搜索]]></category>
		<category><![CDATA[论文]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1588.html</guid>
				<description><![CDATA[Graph Convolutional Neural Networks for Web-Scale Recommender Systems 用于Web级推荐系统的图形卷积神经网络 ABSTRACT Recent advancements in deep neural networks for graph-structured data have led to]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<h4><a id="Graph_Convolutional_Neural_Networks_for_WebScale_Recommender_Systems_0"></a>Graph Convolutional Neural Networks for Web-Scale Recommender Systems</h4>
<h3><a id="Web_2"></a>用于Web级推荐系统的图形卷积神经网络</h3>
<h6><a id="ABSTRACT_4"></a>ABSTRACT</h6>
<p>Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge.</p>
<p>Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model.</p>
<p>We deploy PinSage at Pinterest and train it on 7.5 billion exam-ples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embed-dings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.<br /> 用于图形结构数据的深度神经网络的最新进展已经在推荐器系统基准上产生了最先进的性能。然而，使这些方法实用且可扩展到具有数十亿项目和数亿用户的网络规模推荐任务仍然是一项挑战。</p>
<p>在这里，我们描述了我们在Pinterest开发和部署的大规模深度推荐引擎。我们开发了一种数据有效的图形卷积网络（GCN）算法PinSage，它结合了有效的随机游走和图形卷积，以生成包含图形结构和节点特征信息的节点（即项目）的嵌入。与之前的GCN方法相比，我们开发了一种基于高效随机游走的新方法来构建卷积并设计一种新的训练策略，该策略依赖于更难和更难的训练示例来提高模型的鲁棒性和收敛性。</p>
<p>我们在Pinterest部署了PinSage，并在图表上培训了75亿个例子，其中30亿个节点代表引脚和电路板，以及180亿个边缘。根据离线指标，用户研究和A / B测试，PinSage可提供比可比较的深度学习和基于图形的替代方案更高质量的建议。据我们所知，这是迄今为止最大的深度图嵌入应用，并为基于图卷积结构的新一代Web级推荐系统铺平了道路。</p>
<h4><a id="1	INTRODUCTION_17"></a>1 INTRODUCTION</h4>
<p>Deep learning methods have an increasingly critical role in rec-ommender system applications, being used to learn useful low-dimensional embeddings of images, text, and even individual users [9, 12]. The representations learned using deep models can be used to complement, or even replace, traditional recommendation algo-rithms like collaborative filtering. and these learned representations have high utility because they can be re-used in various recom-mendation tasks. For example, item embeddings learned using a deep model can be used for item-item recommendation and also to recommended themed collections (e.g., playlists, or “feed” content).</p>
<p>Recent years have seen significant developments in this space— especially the development of new deep learning methods that are capable of learning on graph-structured data, which is fundamen-tal for recommendation applications (e.g., to exploit user-to-item interaction graphs as well as social graphs) [6, 19, 21, 24, 29, 30].</p>
<p>Most prominent among these recent advancements is the suc-cess of deep learning architectures known as Graph Convolutional Networks (GCNs) [19, 21, 24, 29]. The core idea behind GCNs is to learn how to iteratively aggregate feature information from lo-cal graph neighborhoods using neural networks (Figure 1). Here a single “convolution” operation transforms and aggregates feature information from a node’s one-hop graph neighborhood, and by stacking multiple such convolutions information can be propagated across far reaches of a graph. Unlike purely content-based deep models (e.g., recurrent neural networks [3]), GCNs leverage both content information as well as graph structure. GCN-based methods have set a new standard on countless recommender system bench-marks (see [19] for a survey). However, these gains on benchmark tasks have yet to be translated to gains in real-world production environments.</p>
<p>深度学习方法在调用系统应用程序中具有越来越重要的作用，用于学习图像，文本甚至个人用户的有用的低维嵌入[9,12]。使用深度模型学习的表示可用于补充甚至替代传统的推荐算法，如协同过滤。并且这些学习的表示具有很高的实用性，因为它们可以在各种推荐任务中重复使用。例如，使用深度模型学习的项目嵌入可以用于项目项目推荐以及推荐的主题集合（例如，播放列表或“馈送”内容）。</p>
<p>近年来，这一领域取得了重大进展 &#8211; 尤其是新的深度学习方法的开发，这些方法能够学习图形结构数据，这是推荐应用的基础（例如，利用用户到项目的交互图形以及社交图表）[6,19,21,24,29,30]。</p>
<p>这些最新进展中最突出的是深度学习架构的成功，称为图形卷积网络（GCN）[19,21,24,29]。 GCN背后的核心思想是学习如何使用神经网络从lo-cal图形邻域迭代地聚合特征信息（图1）。这里，单个“卷积”操作转换并聚合来自节点的单跳图邻域的特征信息，并且通过堆叠多个这样的卷积信息可以在图的远端传播。与纯粹基于内容的深层模型（例如，递归神经网络[3]）不同，GCN利用内容信息和图形结构。基于GCN的方法为无数的推荐系统基准设定了新的标准（参见[19]的一项调查）。但是，基准任务的这些收益尚未转化为实际生产环境中的收益。</p>
<p>The main challenge is to scale both the training as well as in-ference of GCN-based node embeddings to graphs with billions of nodes and tens of billions of edges. Scaling up GCNs is difficult because many of the core assumptions underlying their design are violated when working in a big data environment. For example, all existing GCN-based recommender systems require operating on the full graph Laplacian during training—an assumption that is infeasible when the underlying graph has billions of nodes and whose structure is constantly evolving.</p>
<p>Present work. Here we present a highly-scalable GCN framework that we have developed and deployed in production at Pinterest. Our framework, a random-walk-based GCN named PinSage, operates on a massive graph with 3 billion nodes and 18 billion edges—a graph that is 10, 000× larger than typical applications of GCNs. PinSage leverages several key insights to drastically improve the scalability of GCNs:<br /> 主要的挑战是将基于GCN的节点嵌入的训练和推理扩展到具有数十亿个节点和数百亿个边缘的图形。 扩展GCN很困难，因为在大数据环境中工作时，其设计的许多核心假设都会受到侵犯。 例如，所有现有的基于GCN的推荐系统都需要在训练期间对完整图拉普拉斯算子进行操作 &#8211; 当基础图具有数十亿个节点且其结构不断发展时，这种假设是不可行的。</p>
<p>目前的工作。 在这里，我们提出了一个高度可扩展的GCN框架，我们在Pinterest的生产中开发和部署了该框架。 我们的框架是一个名为PinSage的基于随机游走的GCN，它运行在一个包含30亿个节点和180亿个边缘的大型图形上 &#8211; 图形比GCN的典型应用程序大10,000倍。 PinSage利用几个关键见解来大幅提高GCN的可扩展性：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707091551510.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707091641450.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 图1：使用深度2卷积的模型架构概述（最好以彩色查看）。 左：一个小例子输入</p>
<p>图形。 右：使用前一层表示计算节点A的嵌入hA（2）的2层神经网络，<br /> 节点A的hA（1）和其邻域N（A）（节点B，C，D）的hA（1）。 （然而，邻域的概念是通用的，并不是所有邻居都需要包括在内（第3.2节）。）底部：计算输入图的每个节点的嵌入的神经网络。 虽然神经网络在节点之间不同，但它们都共享相同的参数集（即，卷积（1）和卷积（2）函数的参数;算法1）。 具有相同阴影图案的框共享参数; γ表示重要性汇集函数; 薄矩形框表示密集连接的多层神经网络。</p>
<ul>
<li>
<p>•On-the-fly convolutions: Traditional GCN algorithms per-form graph convolutions by multiplying feature matrices by powers of the full graph Laplacian. In contrast, our PinSage algo-rithm performs efficient, localized convolutions by sampling the neighborhood around a node and dynamically constructing a computation graph from this sampled neighborhood. These dy-namically constructed computation graphs (Fig. 1) specify how to perform a localized convolution around a particular node, and alleviate the need to operate on the entire graph during training.</p>
</li>
<li>
<p>•Producer-consumer minibatch construction: We develop a producer-consumer architecture for constructing minibatches that ensures maximal GPU utilization during model training. A large-memory, CPU-bound producer efficiently samples node network neighborhoods and fetches the necessary features to define local convolutions, while a GPU-bound TensorFlow model consumes these pre-defined computation graphs to efficiently run stochastic gradient decent.<br /> •Efficient MapReduce inference: Given a fully-trained GCN model, we design an efficient MapReduce pipeline that can dis-tribute the trained model to generate embeddings for billions of nodes, while minimizing repeated computations.</p>
</li>
</ul>
<p>In addition to these fundamental advancements in scalability, we also introduce new training techniques and algorithmic innova-tions. These innovations improve the quality of the representations learned by PinSage, leading significant performance gains in down-stream recommender system tasks:</p>
<ul>
<li>Constructing convolutions via random walks: Taking full neighborhoods of nodes to perform convolutions (Fig. 1) would result in huge computation graphs, so we resort to sampling. However, random sampling is suboptimal, and we develop a new technique using short random walks to sample the computa-tion graph. An additional benefit is that each node now has an importance score, which we use in the pooling/aggregation step.</li>
<li>•Importance pooling: A core component of graph convolutions is the aggregation of feature information from local neighbor-hoods in the graph. We introduce a method to weigh the impor-tance of node features in this aggregation based upon random-walk similarity measures, leading to a 46% performance gain in offline evaluation metrics.</li>
<li>•Curriculum training: We design a curriculum training scheme, where the algorithm is fed harder-and-harder examples during training, resulting in a 12% performance gain.<br /> We have deployed PinSage for a variety of recommendation tasks at Pinterest, a popular content discovery and curation appli-cation where users interact with pins, which are visual bookmarks to online content (e.g., recipes they want to cook, or clothes they want to purchase). Users organize these pins into boards, which con-tain collections of similar pins. Altogether, Pinterest is the world’s largest user-curated graph of images, with over 2 billion unique pins collected into over 1 billion boards.</li>
</ul>
<p>Through extensive offline metrics, controlled user studies, and A/B tests, we show that our approach achieves state-of-the-art performance compared to other scalable deep content-based rec-ommendation algorithms, in both an item-item recommendation task (i.e., related-pin recommendation), as well as a “homefeed” recommendation task. In offline ranking metrics we improve over the best performing baseline by more than 40%, in head-to-head human evaluations our recommendations are preferred about 60% of the time, and the A/B tests show 30% to 100% improvements in user engagement across various settings.</p>
<p>To our knowledge, this is the largest-ever application of deep graph embeddings and paves the way for new generation of rec-ommendation systems based on graph convolutional architectures.</p>
<ul>
<li>
<p>动态卷积：传统的GCN算法通过将特征矩阵乘以完整图拉普拉斯算子的幂来进行每个图形卷积。相比之下，我们的PinSage算法通过对节点周围的邻域进行采样并从该采样邻域动态构建计算图来执行高效的局部卷积。这些动态构建的计算图（图1）指定了如何在特定节点周围执行局部卷积，并减少了在训练期间对整个图进行操作的需要。</p>
</li>
<li>
<p>生产者 &#8211; 消费者小批量建设：我们开发了一个生产者 &#8211; 消费者体系结构，用于构建微型计算机，确保在模型培训期间最大限度地利用GPU。一个大内存，受CPU限制的生产者有效地采样节点网络邻域并获取必要的特征来定义局部卷积，而受GPU约束的TensorFlow模型使用这些预定义的计算图来有效地运行随机梯度体面。</p>
</li>
<li>
<p>高效的MapReduce推理：给定一个完全训练的GCN模型，我们设计了一个有效的MapReduce管道，可以分解训练的模型，为数十亿个节点生成嵌入，同时最大限度地减少重复计算。</p>
</li>
</ul>
<p>除了可扩展性的这些基本进步之外，我们还引入了新的培训技术和算法创新。这些创新提高了PinSage所学习的表示质量，在下游推荐系统任务中带来了显着的性能提升：</p>
<ul>
<li>通过随机游走构建卷积：使用完整的节点邻域来执行卷积（图1）将导致巨大的计算图，因此我们求助于采样。然而，随机抽样不是最理想的，我们开发了一种使用短随机游走来抽样计算图的新技术。另一个好处是每个节点现在都有一个重要性分数，我们在池化/聚合步骤中使用它。</li>
<li>•重要性池：图卷的核心组件是图中本地邻居的特征信息的聚合。我们引入了一种方法来基于随机游走相似性度量来衡量此聚合中节点特征的重要性，从而使离线评估指标的性能提高46％。</li>
<li>课程培训：我们设计了一个课程培训方案，在培训过程中，算法得到了越来越难的实例，从而使性能提高了12％。<br /> 我们在Pinterest上部署了PinSage用于各种推荐任务，Pinterest是一种流行的内容发现和策展应用，用户可以在其中与引脚交互，引脚是在线内容的可视书签（例如，他们想要烹饪的食谱，或者他们想要购买的衣服） ）。用户将这些引脚组织成板，其中包含类似引脚的集合。总而言之，Pinterest是世界上最大的用户策划图像图形，超过20亿个独特的引脚被收集到超过10亿个电路板中。</li>
</ul>
<p>通过广泛的离线指标，受控用户研究和A / B测试，我们表明，与项目项目推荐任务中的其他可扩展的基于深度内容的推荐算法相比，我们的方法实现了最先进的性能（即相关引脚推荐），以及“主页”推荐任务。在离线排名指标中，我们在最佳绩效基线上的表现提高了40％以上，在人机对话评估中，我们的建议在60％的时间内是首选，A / B测试显示在30％到100％的情况下，用户参与各种设置。</p>
<p>据我们所知，这是有史以来最大的深度图嵌入应用，并为基于图卷积结构的新一代推荐系统铺平了道路。</p>
<h3><a id="2	RELATED_WORK_81"></a>2 RELATED WORK</h3>
<p>Our work builds upon a number of recent advancements in deep learning methods for graph-structured data.</p>
<p>The notion of neural networks for graph data was first outlined in Gori et al. (2005) [15] and further elaborated on in Scarselli et al. (2009) [27]. However, these initial approaches to deep learning on graphs required running expensive neural “message-passing” algorithms to convergence and were prohibitively expensive on large graphs. Some limitations were addressed by Gated Graph Sequence Neural Networks [22]—which employs modern recurrent neural architectures—but the approach remains computationally expensive and has mainly been used on graphs with &lt;10, 000 nodes.</p>
<p>More recently, there has been a surge of methods that rely on the notion of “graph convolutions” or Graph Convolutional Net-works (GCNs). This approach originated with the work of Bruna et al. (2013), which developed a version of graph convolutions based on spectral graph thery [7]. Following on this work, a number of authors proposed improvements, extensions, and approximations of these spectral convolutions [6, 10, 11, 13, 18, 21, 24, 29, 31], lead-ing to new state-of-the-art results on benchmarks such as node classification, link prediction, as well as recommender system tasks (e.g., the MovieLens benchmark [24]). These approaches have con-sistently outperformed techniques based upon matrix factorization or random walks (e.g., node2vec [17] and DeepWalk [26]), and their success has led to a surge of interest in applying GCN-based methods to applications ranging from recommender systems [24] to drug design [20, 31]. Hamilton et al. (2017b) [19] and Bronstein et al. (2017) [6] provide comprehensive surveys of recent advancements.</p>
<p>However, despite the successes of GCN algorithms, no previous works have managed to apply them to production-scale data with billions of nodes and edges—a limitation that is primarily due to the fact that traditional GCN methods require operating on the entire graph Laplacian during training. Here we fill this gap and show that GCNs can be scaled to operate in a production-scale recommender system setting involving billions of nodes/items. Our work also demonstrates the substantial impact that GCNs have on recommendation performance in a real-world environment.</p>
<p>In terms of algorithm design, our work is most closely related to Hamilton et al. (2017a)’s GraphSAGE algorithm [18] and the closely related follow-up work of Chen et al. (2018) [8]. GraphSAGE is an inductive variant of GCNs that we modify to avoid operating on the entire graph Laplacian. We fundamentally improve upon GraphSAGE by removing the limitation that the whole graph be stored in GPU memory, using low-latency random walks to sample<br /> 我们的工作建立在图形结构数据深度学习方法的最新进展之上。</p>
<p>Gori等人首次概述了图数据的神经网络概念。 （2005）[15]并在Scarselli等人的文章中进一步阐述。 （2009）[27]。然而，这些在图上进行深度学习的初始方法需要运行昂贵的神经“消息传递”算法来收敛，并且在大图上非常昂贵。门控图形序列神经网络[22]解决了一些局限性 &#8211; 它采用了现代的递归神经架构 &#8211; 但该方法计算成本仍然很高，主要用于节点数&lt;10,000的图形。</p>
<p>最近，出现了大量依赖“图形卷积”或图形卷积网络（GCN）概念的方法。这种方法起源于Bruna等人的工作。 （2013），基于光谱图[7]开发了一个图形卷积的版本。继这项工作之后，许多作者提出了这些光谱卷积的改进，扩展和近似[6,10,11,13,18,21,24,29,31]，导致了新的状态。 -art结果基于节点分类，链接预测以及推荐系统任务（例如，MovieLens基准测试[24]）等基准测试。这些方法始终优于基于矩阵分解或随机游走的技术（例如，node2vec [17]和DeepWalk [26]），并且它们的成功引起了人们对将基于GCN的方法应用于推荐器等应用的兴趣。系统[24]到药物设计[20,31]。汉密尔顿等人。 （2017b）[19]和Bronstein等。 （2017）[6]提供了最近进展的综合调查。</p>
<p>然而，尽管GCN算法取得了成功，但之前的工作没有设法将它们应用于具有数十亿节点和边缘的生产规模数据 &#8211; 这主要是因为传统的GCN方法需要在整个图形拉普拉斯算子上运行。训练。在这里，我们填补了这一空白，并表明GCN可以扩展到在涉及数十亿节点/项目的生产规模推荐系统设置中运行。我们的工作还证明了GCN对现实环境中的推荐性能产生的重大影响。</p>
<p>在算法设计方面，我们的工作与Hamilton等人的关系最为密切。 （2017a）的GraphSAGE算法[18]和陈等人的密切相关的后续工作。 （2018）[8]。 GraphSAGE是GCN的归纳变体，我们修改它以避免在整个图拉普拉斯算子上运行。我们通过消除整个图存储在GPU内存中的限制，从而使用低延迟随机游走来获取样本，从根本上改进了GraphSAGE</p>
<p>graph neighborhoods in a producer-consumer architecture. We also introduce a number of new training techniques to improve performance and a MapReduce inference pipeline to scale up to graphs with billions of nodes.</p>
<p>Lastly, also note that graph embedding methods like node2vec</p>
<p>[17]and DeepWalk [26] cannot be applied here. First, these are unsupervised methods. Second, they cannot include node feature information. Third, they directly learn embeddings of nodes and thus the number of model parameters is linear with the size of the graph, which is prohibitive for our setting.<br /> 生产者 &#8211; 消费者体系结构中的图形邻域。 我们还介绍了许多用于提高性能的新培训技术和MapReduce推理管道，以扩展到具有数十亿节点的图形。</p>
<p>最后，还要注意图形嵌入方法，如node2vec</p>
<p>[17]和DeepWalk [26]不能在这里应用。 首先，这些是无监督的方法。 其次，它们不能包含节点特征信息。 第三，它们直接学习节点的嵌入，因此模型参数的数量与图形的大小成线性关系，这对我们的设置来说是禁止的。</p>
<h3><a id="3_METHOD_114"></a>3 METHOD</h3>
<p>In this section, we describe the technical details of the PinSage archi-tecture and training, as well as a MapReduce pipeline to efficiently generate embeddings using a trained PinSage model.</p>
<p>The key computational workhorse of our approach is the notion of localized graph convolutions.1 To generate the embedding for a node (i.e., an item), we apply multiple convolutional modules that aggregate feature information (e.g., visual, textual features) from the node’s local graph neighborhood (Figure 1). Each module learns how to aggregate information from a small graph neighbor-hood, and by stacking multiple such modules, our approach can gain information about the local network topology. Importantly, parameters of these localized convolutional modules are shared across all nodes, making the parameter complexity of our approach independent of the input graph size.</p>
<p>在本节中，我们将介绍PinSage架构和培训的技术细节，以及使用经过训练的PinSage模型高效生成嵌入的MapReduce管道。</p>
<p>我们方法的关键计算主力是局部图卷积的概念.1为了生成节点（即项目）的嵌入，我们应用多个卷积模块来聚合来自节点的特征信息（例如，视觉，文本特征）。 局部图邻域（图1）。 每个模块都学习如何聚合来自小图邻居的信息，并通过堆叠多个这样的模块，我们的方法可以获得有关本地网络拓扑的信息。 重要的是，这些局部卷积模块的参数在所有节点之间共享，使得我们方法的参数复杂度与输入图形大小无关。</p>
<h5><a id="31	Problem_Setup_124"></a>3.1 Problem Setup</h5>
<p>Pinterest is a content discovery application where users interact with pins, which are visual bookmarks to online content (e.g., recipes they want to cook, or clothes they want to purchase). Users organize these pins into boards, which contain collections of pins that the user deems to be thematically related. Altogether, the Pinterest graph contains 2 billion pins, 1 billion boards, and over 18 billion edges (i.e., memberships of pins to their corresponding boards).</p>
<p>Our task is to generate high-quality embeddings or representa-tions of pins that can be used for recommendation (e.g., via nearest-neighbor lookup for related pin recommendation, or for use in a downstream re-ranking system). In order to learn these embed-dings, we model the Pinterest environment as a bipartite graph consisting of nodes in two disjoint sets, I (containing pins) and C (containing boards). Note, however, that our approach is also naturally generalizable, with I being viewed as a set of items and C as a set of user-defined contexts or collections.</p>
<p>In addition to the graph structure, we also assume that the pins/items u ∈ I are associated with real-valued attributes, xu ∈ Rd . In general, these attributes may specify metadata or content information about an item, and in the case of Pinterest, we have that pins are associated with both rich text and image features. Our goal is to leverage both these input attributes as well as the structure of the bipartite graph to generate high-quality embed-dings. These embeddings are then used for recommender system candidate generation via nearest neighbor lookup (i.e., given a pin, find related pins) or as features in machine learning systems for ranking the candidates.<br /> Pinterest是一种内容发现应用程序，其中用户与引脚交互，引脚是在线内容的可视书签（例如，他们想要烹饪的食谱，或者他们想要购买的衣服）。用户将这些引脚组织成电路板，其中包含用户认为与主题相关的引脚集合。总而言之，Pinterest图包含20亿个引脚，10亿个电路板和超过180亿个边沿（即，引脚到其相应电路板的成员资格）。</p>
<p>我们的任务是生成可用于推荐的高质量嵌入或引脚表示（例如，通过最近邻居查找以获得相关引脚推荐，或用于下游重新排序系统）。为了学习这些嵌入，我们将Pinterest环境建模为由两个不相交集合中的节点组成的二分图，I（包含引脚）和C（包含板）。但请注意，我们的方法也可以自然地推广，我将其视为一组项目，将C视为一组用户定义的上下文或集合。</p>
<p>除了图形结构之外，我们还假设引脚/项u∈I与实值属性xu∈Rd相关联。通常，这些属性可以指定关于项目的元数据或内容信息，并且在Pinterest的情况下，我们将这些引脚与富文本和图像特征相关联。我们的目标是利用这些输入属性以及二分图的结构来生成高质量的嵌入式数据。然后，这些嵌入用于通过最近邻居查找生成推荐系统候选（即，给定引脚，找到相关引脚）或用作机器学习系统中用于对候选进行排序的特征。</p>
<p>For notational convenience and generality, when we describe the PinSage algorithm, we simply refer to the node set of the full graph with V = I ∪ C and do not explicitly distinguish between pin and board nodes (unless strictly necessary), using the more general term “node” whenever possible.<br /> 为了符号方便性和通用性，当我们描述PinSage算法时，我们简单地引用完整图的节点集，其中V =I∪C并且没有明确区分引脚和板节点（除非严格必要），使用更一般 尽可能使用术语“节点”。</p>
<h5><a id="32	Model_Architecture_139"></a>3.2 Model Architecture</h5>
<p>We use localized convolutional modules to generate embeddings for nodes. We start with input node features and then learn neural networks that transform and aggregate features over the graph to compute the node embeddings (Figure 1).</p>
<p><strong>Forward propagation algorithm.</strong> We consider the task of gener-ating an embedding, zu for a node u, which depends on the node’s input features and the graph structure around this node.<br /> 我们使用局部卷积模块为节点生成嵌入。 我们从输入节点特征开始，然后学习神经网络，在图形上转换和聚合特征以计算节点嵌入（图1）。</p>
<p><strong>前向传播算法</strong> 我们考虑为节点u生成嵌入，zu的任务，这取决于节点的输入特征和该节点周围的图形结构。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707091950238.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>The core of our PinSage algorithm is a localized convolution operation, where we learn how to aggregate information from u’s neighborhood (Figure 1). This procedure is detailed in Algorithm 1 convolve. The basic idea is that we transform the representations zv , ∀v ∈ N(u) of u’s neighbors through a dense neural network and then apply a aggregator/pooling fuction (e.g., a element-wise mean or weighted sum, denoted as γ ) on the resulting set of vectors (Line 1). This aggregation step provides a vector representation, nu , of u’s local neighborhood, N(u). We then concatenate the ag-gregated neighborhood vector nu with u’s current representation hu and transform the concatenated vector through another dense neural network layer (Line 2). Empirically we observe significant performance gains when using concatenation operation instead of the average operation as in [21]. Additionally, the normalization in Line 3 makes training more stable, and it is more efficient to perform approximate nearest neighbor search for normalized embeddings (Section 3.5). The output of the algorithm is a representation of u that incorporates both information about itself and its local graph neighborhood.</p>
<p>Importance-based neighborhoods. An important innovation in our approach is how we define node neighborhoods N(u), i.e., how we select the set of neighbors to convolve over in Algorithm 1. Whereas previous GCN approaches simply examine k-hop graph neighborhoods, in PinSage we define importance-based neighbor-hoods, where the neighborhood of a node u is defined as the T nodes that exert the most influence on node u. Concretely, we simulate random walks starting from node u and compute the L1-normalized</p>
<p>visit count of nodes visited by the random walk [14].2 The neigh-borhood of u is then defined as the top T nodes with the highest normalized visit counts with respect to node u.</p>
<p>The advantages of this importance-based neighborhood defi-nition are two-fold. First, selecting a fixed number of nodes to aggregate from allows us to control the memory footprint of the algorithm during training [18]. Second, it allows Algorithm 1 to take into account the importance of neighbors when aggregating the vector representations of neighbors. In particular, we imple-ment γ in Algorithm 1 as a weighted-mean, with weights defined according to the L1 normalized visit counts. We refer to this new approach as importance pooling.</p>
<p>Stacking convolutions. Each time we apply the convolve opera-tion (Algorithm 1) we get a new representation for a node, and we can stack multiple such convolutions on top of each other in order to gain more information about the local graph structure around node u. In particular, we use multiple layers of convolutions, where the inputs to the convolutions at layer k depend on the representa-tions output from layer k − 1 (Figure 1) and where the initial (i.e., “layer 0”) representations are equal to the input node features. Note that the model parameters in Algorithm 1 (Q, q, W, and w) are shared across the nodes but differ between layers.<br /> 我们的PinSage算法的核心是局部卷积运算，我们学习如何从u的邻域聚合信息（图1）。此过程在算法1卷入中详细说明。基本思想是我们通过密集神经网络转换u邻居的表示zv，∀v∈N（u）然后应用聚合器/池化函数（例如，元素方式或加权和，表示为γ）在得到的矢量集上（第1行）。该聚合步骤提供了u的局部邻域N（u）的向量表示nu。然后，我们将ag-gregated邻域向量nu与u的当前表示hu连接，并将连接的向量转换为另一个密集的神经网络层（第2行）。根据经验，我们在使用串联操作而不是[21]中的平均操作时观察到显着的性能提升。此外，第3行中的归一化使训练更加稳定，并且对归一化嵌入执行近似最近邻搜索更有效（第3.5节）。算法的输出是u的表示，其包含关于其自身及其局部图邻域的信息。</p>
<p>基于重要性的社区。我们方法的一个重要创新是我们如何定义节点邻域N（u），即我们如何选择在算法1中进行卷积的邻居集合。而以前的GCN方法只是检查k-hop图形邻域，在PinSage中我们定义重要性基于邻居的邻居，其中节点u的邻域被定义为对节点u施加最大影响的T节点。具体地说，我们模拟从节点u开始的随机游走并计算L1标准化</p>
<p>访问随机游走所访问的节点数[14] .2然后将u的邻域定义为相对于节点u具有最高归一化访问计数的前T个节点。</p>
<p>这种基于重要性的邻域定义的优势是双重的。首先，选择要聚合的固定数量的节点允许我们在训练期间控制算法的内存占用[18]。其次，它允许算法1在聚合邻居的向量表示时考虑邻居的重要性。特别地，我们在算法1中实现γ作为加权平均值，其中权重根据L1归一化访问计数来定义。我们将这种新方法称为重要性池。</p>
<p>堆叠卷积。每次我们应用卷积运算（算法1）时，我们得到一个节点的新表示，并且我们可以将多个这样的卷积堆叠在彼此之上，以便获得关于节点u周围的局部图结构的更多信息。特别地，我们使用多层卷积，其中层k处的卷积的输入取决于层k-1（图1）的表示和初始（即“层0”）表示相等的表示。到输入节点功能。请注意，算法1（Q，q，W和w）中的模型参数在节点之间共享，但层之间不同。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707092102872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070709213627.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>We first describe our margin-based loss function in detail. Follow-ing this, we give an overview of several techniques we developed that lead to the computation efficiency and fast convergence rate of PinSage, allowing us to train on billion node graphs and billions training examples. And finally, we describe our curriculum-training scheme, which improves the overall quality of the recommendations.</p>
<p>我们首先详细描述基于保证金的损失函数。 接下来，我们概述了我们开发的几种技术，这些技术可以提高PinSage的计算效率和快速收敛速度，使我们能够训练数十亿个节点图和数十亿个训练样例。 最后，我们描述了我们的课程培训计划，该计划提高了建议的整体质量。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070709224222.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Loss function. In order to train the parameters of the model, we use a max-margin-based loss function. The basic idea is that we want to maximize the inner product of positive examples, i.e., the embedding of the query item and the corresponding related item. At the same time we want to ensure that the inner product of negative examples—i.e., the inner product between the embedding of the query item and an unrelated item—is smaller than that of the positive sample by some pre-defined margin. The loss function for a<br /> 损失功能。 为了训练模型的参数，我们使用基于最大边际的损失函数。 基本思想是我们希望最大化正例的内积，即嵌入查询项和相应的相关项。 同时，我们希望确保否定示例的内积 &#8211; 即，查询项的嵌入与不相关项之间的内积 &#8211; 比正样本的内积小一些预定义的余量。 a的损失函数<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070709234097.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Multi-GPU training with large minibatches. To make full use of multiple GPUs on a single machine for training, we run the for-ward and backward propagation in a multi-tower fashion. With multiple GPUs, we first divide each minibatch (Figure 1 bottom) into equal-sized portions. Each GPU takes one portion of the minibatch and performs the computations using the same set of parameters. Af-ter backward propagation, the gradients for each parameter across all GPUs are aggregated together, and a single step of synchronous SGD is performed. Due to the need to train on extremely large number of examples (on the scale of billions), we run our system with large batch sizes, ranging from 512 to 4096.</p>
<p>We use techniques similar to those proposed by Goyal et al. [16] to ensure fast convergence and maintain training and generalization accuracy when dealing with large batch sizes. We use a gradual warmup procedure that increases learning rate from small to a peak value in the first epoch according to the linear scaling rule. Afterwards the learning rate is decreased exponentially.</p>
<p>Producer-consumer minibatch construction. During training, the adjacency list and the feature matrix for billions of nodes are placed in CPU memory due to their large size. However, during the convolve step of PinSage, each GPU process needs access to the neighborhood and feature information of nodes in the neighbor-hood. Accessing the data in CPU memory from GPU is not efficient. To solve this problem, we use a re-indexing technique to create a sub-graph G′ = (V ′, E′) containing nodes and their neighborhood, which will be involved in the computation of the current minibatch. A small feature matrix containing only node features relevant to computation of the current minibatch is also extracted such that the order is consistent with the index of nodes in G′. The adjacency list of G′ and the small feature matrix are fed into GPUs at the start of each minibatch iteration, so that no communication between the GPU and CPU is needed during the convolve step, greatly improving GPU utilization.</p>
<p>The training procedure has alternating usage of CPUs and GPUs. The model computations are in GPUs, whereas extracting features, re-indexing, and negative sampling are computed on CPUs. In ad-dition to parallelizing GPU computation with multi-tower training, and CPU computation using OpenMP [25], we design a producer-consumer pattern to run GPU computation at the current iteration and CPU computation at the next iteration in parallel. This further reduces the training time by almost a half.</p>
<p>Sampling negative items. Negative sampling is used in our loss function (Equation 1) as an approximation of the normalization factor of edge likelihood [23]. To improve efficiency when training with large batch sizes, we sample a set of 500 negative items to be shared by all training examples in each minibatch. This drastically saves the number of embeddings that need to be computed during each training step, compared to running negative sampling for each node independently. Empirically, we do not observe a difference between the performance of the two sampling schemes.</p>
<p>In the simplest case, we could just uniformly sample negative examples from the entire set of items. However, ensuring that the inner product of the positive example (pair of items (q, i )) is larger than that of the q and each of the 500 negative items is too “easy” and does not provide fine enough “resolution” for the system to learn. In particular, our recommendation algorithm should be capable of finding 1,000 most relevant items to q among the catalog of over 2 billion items. In other words, our model should be able to distinguish/identify 1 item out of 2 million items. But with 500 random negative items, the model’s resolution is only 1 out of</p>
<p>500.Thus, if we sample 500 random negative items out of 2 billion items, the chance of any of these items being even slightly related to the query item is small. Therefore, with large probability the learning will not make good parameter updates and will not be able to differentiate slightly related items from the very related ones.</p>
<p>To solve the above problem, for each positive training example (i.e., item pair (q, i)), we add “hard” negative examples, i.e., items that are somewhat related to the query item q, but not as related as the positive item i. We call these “hard negative items”. They are generated by ranking items in a graph according to their Per-sonalized PageRank scores with respect to query item q [14]. Items ranked at 2000-5000 are randomly sampled as hard negative items. As illustrated in Figure 2, the hard negative examples are more similar to the query than random negative examples, and are thus challenging for the model to rank, forcing the model to learn to distinguish items at a finer granularity.<br /> 使用大型微型计算机进行多GPU培训。为了在一台机器上充分利用多个GPU进行培训，我们以多塔式方式进行前向和后向传播。对于多个GPU，我们首先将每个小批量（图1底部）划分为相等大小的部分。每个GPU获取一部分小批量并使用相同的参数集执行计算。在后向传播之后，所有GPU上的每个参数的梯度被聚合在一起，并且执行同步SGD的单个步骤。由于需要训练大量的例子（数十亿的规模），我们运行我们的系统，批量大，从512到4096。</p>
<p>我们使用类似于Goyal等人提出的技术。 [16]确保在处理大批量时快速收敛并保持培训和泛化准确性。我们使用渐进的预热程序，根据线性缩放规则，在第一个时期内将学习率从小值提高到峰值。之后学习率呈指数下降。</p>
<p>生产者 &#8211; 消费者小批量建设。在训练期间，由于数据量大，数十亿个节点的邻接列表和特征矩阵被放置在CPU内存中。但是，在PinSage的卷积步骤中，每个GPU进程都需要访问邻居和邻居中节点的特征信息。从GPU访问CPU内存中的数据效率不高。为了解决这个问题，我们使用重新索引技术来创建包含节点及其邻域的子图G’=（V’，E’），其将参与当前小批量的计算。还提取仅包含与当前小批量的计算相关的节点特征的小特征矩阵，使得该顺序与G’中的节点索引一致。 G’的邻接列表和小特征矩阵在每个小批量迭代开始时被馈送到GPU中，因此在卷积步骤期间不需要GPU和CPU之间的通信，从而大大提高了GPU利用率。</p>
<p>训练过程交替使用CPU和GPU。模型计算在GPU中，而在CPU上计算提取特征，重新索引和负抽样。除了使用多塔培训并行GPU计算和使用OpenMP进行CPU计算[25]之外，我们还设计了一个生产者 &#8211; 消费者模式，以便在当前迭代中运行GPU计算，并在下一次迭代中并行运行CPU计算。这进一步将训练时间减少了近一半。</p>
<p>采样负面项目。在我们的损失函数（等式1）中使用负采样作为边缘似然归一化因子的近似值[23]。为了提高大批量培训的效率，我们对每个小批量的所有培训示例共享500个负面项目。与每个节点独立运行负采样相比，这大大节省了每个训练步骤中需要计算的嵌入数量。根据经验，我们没有观察到两种抽样方案的性能差异。</p>
<p>在最简单的情况下，我们可以从整个项目集合中统一采样负面示例。但是，确保正例（项目对（q，i））的内积大于q的内积，并且500个负项中的每一个都太“容易”并且不能提供足够精细的“分辨率”。系统要学习。特别是，我们的推荐算法应该能够在超过20亿个项目的目录中找到1000个最相关的项目。换句话说，我们的模型应该能够区分/识别200万个项目中的1个项目。但是有500个随机负面项目，模型的分辨率只有1个</p>
<p>500.因此，如果我们从20亿个项目中抽取500个随机负面项目，那么这些项目中任何一个与查询项目稍微相关的可能性就很小。因此，很有可能学习不会进行良好的参数更新，并且无法区分略微相关的项目与非常相关的项目。</p>
<p>为了解决上述问题，对于每个积极的训练样例（即项目对（q，i）），我们添加“硬”否定示例，即与查询项目q有些相关的项目，但不是与积极的项目我我们将这些称为“硬性负面物品”。它们是通过根据查询项q [14]的Per-sonalized PageRank分数对图表中的项目进行排名而生成的。排名为2000-5000的项目被随机抽样为硬阴性项目。如图2所示，硬否定示例与查询比随机否定示例更相似，因此难以对模型进行排名，迫使模型学习以更精细的粒度区分项目。</p>
<p>Using hard negative items throughout the training procedure doubles the number of epochs needed for the training to con-verge. To help with convergence, we develop a curriculum training scheme [4]. In the first epoch of training, no hard negative items are used, so that the algorithm quickly finds an area in the parameter space where the loss is relatively small. We then add hard negative items in subsequent epochs, focusing the model to learn how to distinguish highly related pins from only slightly related ones. At epoch n of the training, we add n − 1 hard negative items to the set of negative items for each item.<br /> 在整个训练过程中使用硬阴性项目会使训练所需的时期数量增加一倍。 为了帮助融合，我们制定了课程培训计划[4]。 在第一个训练时期，没有使用硬负项，因此算法快速找到参数空间中损失相对较小的区域。 然后，我们在后续时期添加硬阴性项，重点关注模型，以了解如何区分高度相关的引脚和仅略微相关的引脚。 在训练的时期，我们将n &#8211; 1个硬阴性项添加到每个项目的负项目集中。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707092622891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> Figure 2: Random negative examples and hard negative ex-amples. Notice that the hard negative example is signifi-cantly more similar to the query, than the random negative example, though not as similar as the positive example.<br /> 图2：随机负面例子和硬性负面例子。 请注意，与负面示例相比，硬负面示例显着更类似于查询，尽管与正面示例不太相似。</p>
<h5><a id="34	Node_Embeddings_via_MapReduce_223"></a>3.4 Node Embeddings via MapReduce</h5>
<p>After the model is trained, it is still challenging to directly apply the trained model to generate embeddings for all items, including those that were not seen during training. Naively computing embeddings for nodes using Algorithm 2 leads to repeated computations caused by the overlap between K-hop neighborhoods of nodes. As illus-trated in Figure 1, many nodes are repeatedly computed at multiple layers when generating the embeddings for different target nodes. To ensure efficient inference, we develop a MapReduce approach that runs model inference without repeated computations.</p>
<p>We observe that inference of node embeddings very nicely lends itself to MapReduce computational model. Figure 3 details the data flow on the bipartite pin-to-board Pinterest graph, where we assume the input (i.e., “layer-0”) nodes are pins/items (and the layer-1 nodes are boards/contexts). The MapReduce pipeline has two key parts:</p>
<p>(1)One MapReduce job is used to project all pins to a low-dimensional latent space, where the aggregation operation will be performed (Algorithm 1, Line 1).</p>
<p>(2)Another MapReduce job is then used to join the resulting pin representations with the ids of the boards they occur in, and the board embedding is computed by pooling the features of its (sampled) neighbors.</p>
<p>Note that our approach avoids redundant computations and that the latent vector for each node is computed only once. After the em-beddings of the boards are obtained, we use two more MapReduce jobs to compute the second-layer embeddings of pins, in a similar fashion as above, and this process can be iterated as necessary (up to K convolutional layers).3</p>
<p>在训练模型之后，直接应用训练模型来生成所有项目的嵌入仍然具有挑战性，包括在训练期间未看到的项目。使用算法2对节点进行初始计算嵌入导致由节点的K跳邻域之间的重叠引起的重复计算。如图1所示，当为不同的目标节点生成嵌入时，在多个层重复计算许多节点。为了确保有效的推理，我们开发了一种MapReduce方法，该方法运行模型推理而无需重复计算。</p>
<p>我们观察到节点嵌入的推断非常好地适用于MapReduce计算模型。图3详述了二分针对板Pinterest图的数据流，其中我们假设输入（即“第0层”）节点是引脚/项（并且第1层节点是板/上下文）。 MapReduce管道有两个关键部分：</p>
<p>（1）一个MapReduce作业用于将所有引脚投影到低维潜在空间，其中将执行聚合操作（算法1，第1行）。</p>
<p>（2）然后使用另一个MapReduce作业将得到的引脚表示与它们出现的板的ID相连，并且通过汇集其（采样的）邻居的特征来计算板嵌入。</p>
<p>请注意，我们的方法避免了冗余计算，并且每个节点的潜在向量仅计算一次。在获得电路板的em-bedding之后，我们使用另外两个MapReduce作业以与上面类似的方式计算引脚的第二层嵌入，并且可以根据需要迭代该过程（直到K个卷积层）。</p>
<h6><a id="35	Efficient_nearestneighbor_lookups_249"></a>3.5 Efficient nearest-neighbor lookups</h6>
<p>The embeddings generated by PinSage can be used for a wide range of downstream recommendation tasks, and in many settings we can directly use these embeddings to make recommendations by performing nearest-neighbor lookups in the learned embedding space. That is, given a query item q, the we can recommend items whose embeddings are the K-nearest neighbors of the query item’s embedding. Approximate KNN can be obtained efficiently via lo-cality sensitive hashing [2]. After the hash function is computed, retrieval of items can be implemented with a two-level retrieval pro-cess based on the Weak AND operator [5]. Given that the PinSage model is trained offline and all node embeddings are computed via MapReduce and saved in a database, the efficient nearest-neighbor lookup operation enables the system to serve recommendations in an online fashion,<br /> PinSage生成的嵌入可用于各种下游推荐任务，在许多设置中，我们可以通过在学习的嵌入空间中执行最近邻查找来直接使用这些嵌入来提出建议。 也就是说，给定查询项q，我们可以推荐其嵌入是查询项嵌入的K最近邻居的项。 通过物理敏感散列可以有效地获得近似KNN [2]。 在计算散列函数之后，可以使用基于Weak AND运算符的两级检索过程来实现项目的检索[5]。 鉴于PinSage模型是离线训练的，并且所有节点嵌入都是通过MapReduce计算并保存在数据库中，有效的最近邻查找操作使系统能够以在线方式提供建议，</p>
<h3><a id="4_EXPERIMENTS_253"></a>4 EXPERIMENTS</h3>
<p>To demonstrate the efficiency of PinSage and the quality of the embeddings it generates, we conduct a comprehensive suite of experiments on the entire Pinterest object graph, including offline experiments, production A/B tests as well as user studies.<br /> 为了证明PinSage的效率及其产生的嵌入质量，我们对整个Pinterest对象图进行了一整套实验，包括离线实验，生产A / B测试以及用户研究。</p>
<h6><a id="41	Experimental_Setup_258"></a>4.1 Experimental Setup</h6>
<p>We evaluate the embeddings generated by PinSage in two tasks: recommending related pins and recommending pins in a user’s home/news feed. To recommend related pins, we select the K near-est neighbors to the query pin in the embedding space. We evaluate performance on this related-pin recommendation task using both offline ranking measures as well as a controlled user study. For the homefeed recommendation task, we select the pins that are closest in the embedding space to one of the most recently pinned items by the user. We evaluate performance of a fully-deployed production system on this task using A/B tests to measure the overall impact on user engagement.</p>
<p>Training details and data preparation. We define the set, L, of positive training examples (Equation (1)) using historical user engagement data. In particular, we use historical user engagement data to identify pairs of pins (q, i), where a user interacted with pin</p>
<p>iimmediately after she interacted with pin q. We use all other pins as negative items (and sample them as described in Section 3.3). Overall, we use 1.2 billion pairs of positive training examples (in addition to 500 negative examples per batch and 6 hard negative examples per pin). Thus in total we use 7.5 billion training examples.<br /> Since PinSage can efficiently generate embeddings for unseen data, we only train on a subset of the Pinterest graph and then generate embeddings for the entire graph using the MapReduce pipeline described in Section 3.4. In particular, for training we use a randomly sampled subgraph of the entire graph, containing 20% of all boards (and all the pins touched by those boards) and 70% of the labeled examples. During hyperparameter tuning, a remaining 10% of the labeled examples are used. And, when testing, we run inference on the entire graph to compute embeddings for all 2 billion pins, and the remaining 20% of the labeled examples are used to test the recommendation performance of our PinSage in the offline evaluations. Note that training on a subset of the full graph drastically decreased training time, with a negligible impact on final performance. In total, the full datasets for training and evaluation are approximately 18TB in size with the full output embeddings being 4TB.<br /> 我们评估PinSage在两个任务中生成的嵌入：推荐相关引脚并在用户的主页/新闻源中推荐引脚。为了推荐相关引脚，我们选择嵌入空间中查询引脚的K近邻。我们使用离线排名度量和受控用户研究来评估此相关引脚推荐任务的性能。对于主页提交推荐任务，我们选择嵌入空间中最接近用户最近固定项目之一的引脚。我们使用A / B测试来评估完全部署的生产系统在此任务上的性能，以衡量对用户参与的总体影响。</p>
<p>培训细节和数据准备。我们使用历史用户参与数据定义积极训练样例（等式（1））的集合L.特别地，我们使用历史用户参与数据来识别用户与引脚交互的引脚对（q，i）</p>
<p>她与pin q交互后立刻。我们使用所有其他引脚作为负项（并按照第3.3节中的描述对它们进行采样）。总的来说，我们使用了12亿对正面训练示例（除了每批500个负面示例和每个针脚6个硬负面示例）。因此，我们总共使用了75亿个培训示例。<br /> 由于PinSage可以有效地为看不见的数据生成嵌入，因此我们只训练Pinterest图的子集，然后使用3.4节中描述的MapReduce管道为整个图生成嵌入。特别是，对于训练，我们使用整个图形的随机采样子图，包含20％的所有板（以及这些板触及的所有引脚）和70％的标记示例。在超参数调整期间，使用剩余的10％的标记示例。并且，在测试时，我们对整个图形进行推断，以计算所有20亿个引脚的嵌入，其余20％的标记示例用于测试我们的PinSage在离线评估中的推荐性能。请注意，对完整图表子集的培训大大减少了培训时间，对最终性能的影响可以忽略不计。总的来说，用于训练和评估的完整数据集大小约为18TB，完整输出嵌入为4TB。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707092854376.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Features used for learning. Each pin at Pinterest is associated with an image and a set of textual annotations (title, description). To generate feature representation xq for each pin q, we concatenate visual embeddings (4,096 dimensions), textual annotation embed-dings (256 dimensions), and the log degree of the node/pin in the graph. The visual embeddings are the 6-th fully connected layer of a classification network using the VGG-16 architecture [28]. Tex-tual annotation embeddings are trained using a Word2Vec-based model [23], where the context of an annotation consists of other annotations that are associated with each pin.</p>
<p>Baselines for comparison. We evaluate the performance of Pin-Sage against the following state-of-the-art content-based, graph-based and deep learning baselines that generate embeddings of pins:</p>
<p>(1)Visual embeddings (Visual): Uses nearest neighbors of deep visual embeddings for recommendations. The visual features are described above.</p>
<p>(2)Annotation embeddings (Annotation): Recommends based on nearest neighbors in terms of annotation embeddings. The annotation embeddings are described above.<br /> (3)Combined embeddings (Combined): Recommends based on concatenating visual and annotation embeddings, and using a 2-layer multi-layer perceptron to compute embeddings that capture both visual and annotation features.</p>
<p>(4)Graph-based method (Pixie): This random-walk-based method [14] uses biased random walks to generate ranking scores by simulating random walks starting at query pin q. Items with top K scores are retrieved as recommendations. While this approach does not generate pin embeddings, it is currently the state-of-the-art at Pinterest for certain recommendation tasks [14] and thus an informative baseline.</p>
<p>The visual and annotation embeddings are state-of-the-art deep learning content-based systems currently deployed at Pinterest to generate representations of pins. Note that we do not compare against other deep learning baselines from the literature simply due to the scale of our problem. We also do not consider non-deep learning approaches for generating item/content embeddings, since other works have already proven state-of-the-art performance of deep learning approaches for generating such embeddings [9, 12, 24].</p>
<p>用于学习的功能。 Pinterest中的每个引脚都与一个图像和一组文本注释（标题，描述）相关联。为了生成每个引脚q的特征表示xq，我们连接了视觉嵌入（4,096维），文本注释嵌入（256维）以及图中节点/引脚的对数度。视觉嵌入是使用VGG-16架构的分类网络的第6个完全连接层[28]。使用基于Word2Vec的模型[23]训练文本注释嵌入，其中注释的上下文包含与每个引脚相关联的其他注释。</p>
<p>比较基线。我们针对以下基于内容的，基于图形的深度学习基线评估了Pin-Sage的性能，这些基线生成了引脚的嵌入：</p>
<p>（1）视觉嵌入（Visual）：使用深度视觉嵌入的最近邻居作为推荐。视觉特征如上所述。</p>
<p>（2）注释嵌入（注释）：根据注释嵌入推荐基于最近邻居。注释嵌入如上所述。<br /> （3）组合嵌入（组合）：推荐基于连接视觉和注释嵌入，并使用2层多层感知器计算捕获视觉和注释特征的嵌入。</p>
<p>（4）基于图的方法（Pixie）：这种基于随机游走的方法[14]通过模拟从查询引脚q开始的随机游走来使用偏向随机游走来生成排名分数。具有最高K分数的项目被检索作为推荐。虽然这种方法不会产生引脚嵌入，但它目前是Pinterest中针对某些推荐任务[14]的最新技术，因此也是一个信息基线。</p>
<p>视觉和注释嵌入是目前在Pinterest部署的最先进的基于深度学习内容的系统，用于生成引脚的表示。请注意，由于问题的严重性，我们不会与文献中的其他深度学习基线进行比较。我们也不考虑用于生成项目/内容嵌入的非深度学习方法，因为其他工作已经证明了用于生成这种嵌入的深度学习方法的最先进性能[9,12,24]。</p>
<p>We also conduct ablation studies and consider several variants of PinSage when evaluating performance:</p>
<ul>
<li>
<p>max-pooling uses the element-wise max as a symmetric aggre-gation function (i.e., γ = max) without hard negative samples;</p>
</li>
<li>
<p>mean-pooling uses the element-wise mean as a symmetric aggregation function (i.e., γ = mean);</p>
</li>
<li>
<p>mean-pooling-xent is the same as mean-pooling but uses the cross-entropy loss introduced in [18].</p>
</li>
<li>
<p>mean-pooling-hard is the same as mean-pooling, except that it incorporates hard negative samples as detailed in Section 3.3.</p>
</li>
<li>
<p>PinSage uses all optimizations presented in this paper, includ-ing the use of importance pooling in the convolution step.</p>
</li>
</ul>
<p>The max-pooling and cross-entropy settings are extensions of the best-performing GCN model from Hamilton et al. [18]—other vari-ants (e.g., based on Kipf et al. [21]) performed significantly worse in development tests and are omitted for brevity.4 For all the above variants, we used K = 2, hidden dimension size m = 2048, and set the embedding dimension d to be 1024.</p>
<p>Computation resources. Training of PinSage is implemented in TensorFlow [1] and run on a single machine with 32 cores and 16 Tesla K80 GPUs. To ensure fast fetching of item’s visual and annotation features, we store them in main memory, together with the graph, using Linux HugePages to increase the size of virtual memory pages from 4KB to 2MB. The total amount of memory used in training is 500GB. Our MapReduce inference pipeline is run on a Hadoop2 cluster with 378 d2.8xlarge Amazon AWS nodes.</p>
<p>我们还进行消融研究，并在评估性能时考虑PinSage的几种变体：</p>
<ul>
<li>
<p>max-pooling使用逐元素max作为对称聚合函数（即γ= max）而没有硬阴性样本;</p>
</li>
<li>
<p>均值池使用元素均值作为对称聚合函数（即，γ=均值）;</p>
</li>
<li>
<p>mean-pooling-xent与均值池相同，但使用[18]中引入的交叉熵损失。</p>
</li>
<li>
<p>mean-pooling-hard与mean-pooling相同，除了它包含第3.3节中详述的硬阴性样本。</p>
</li>
<li>
<p>PinSage使用本文中介绍的所有优化，包括在卷积步骤中使用重要性池。</p>
</li>
</ul>
<p>最大池和交叉熵设置是Hamilton等人的最佳性能GCN模型的扩展。 [18]其他变量（例如，基于Kipf等人[21]）在开发测试中表现更差，为简洁省略.4对于所有上述变体，我们使用K = 2，隐藏尺寸大小为m = 2048，并将嵌入维度d设置为1024。</p>
<p>计算资源。 PinSage的培训在TensorFlow [1]中实施，并在具有32个核心和16个Tesla K80 GPU的单台机器上运行。为了确保快速获取项目的视觉和注释功能，我们将它们与图形一起存储在主存储器中，使用Linux HugePages将虚拟内存页面的大小从4KB增加到2MB。培训中使用的内存总量为500GB。我们的MapReduce推理管道在具有378个d2.8xlarge Amazon AWS节点的Hadoop2集群上运行。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707092944123.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>表1：PinSage和基于内容的深度学习基线的命中率和MRR。 总体而言，PinSage在最佳基线上的命中率提高了150％，MRR提高了60％</p>
<h4><a id="42	Offline_Evaluation_333"></a>4.2 Offline Evaluation</h4>
<p>To evaluate performance on the related pin recommendation task, we define the notion of hit-rate. For each positive pair of pins (q, i) in the test set, we use q as a query pin and then compute its top<br /> Knearest neighbors NNq from a sample of 5 million test pins. We then define the hit-rate as the fraction of queries q where i was ranked among the top K of the test sample (i.e., where i ∈ NNq ). This metric directly measures the probability that recommendations made by the algorithm contain the items related to the query pin q. In our experiments K is set to be 500.</p>
<p>We also evaluate the methods using Mean Reciprocal Rank (MRR), which takes into account of the rank of the item j among recommended items for query item q:<br /> 为了评估相关引脚推荐任务的性能，我们定义了命中率的概念。 对于测试集中的每个正对引脚（q，i），我们使用q作为查询引脚，然后计算其顶部<br /> 来自500万个测试引脚样本的最近邻NNq。 然后，我们将命中率定义为查询q的分数，其中i在测试样本的前K中排名（即，其中i∈NNq）。 该度量直接测量算法所做推荐包含与查询引脚q相关的项目的概率。 在我们的实验中，K设定为500。</p>
<p>我们还使用均值倒数等级（MRR）来评估方法，其考虑了查询项目q的推荐项目中的项目j的等级：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093036909.png" alt="在这里插入图片描述"><br /> Due to the large pool of candidates (more than 2 billion), we use a scaled version of the MRR in Equation (2), where Ri, q is the rank of item i among recommended items for query q, and n is the total number of labeled item pairs. The scaling factor 100 ensures that, for example, the difference between rank at 1, 000 and rank at 2, 000 is still noticeable, instead of being very close to 0.</p>
<p>Table 1 compares the performance of the various approaches using the hit rate as well as the MRR.5 PinSage with our new importance-pooling aggregation and hard negative examples achieves the best performance at 67% hit-rate and 0.59 MRR, outperforming the top baseline by 40% absolute (150% relative) in terms of the hit rate and also 22% absolute (60% relative) in terms of MRR. We also observe that combining visual and textual information works much better than using either one alone (60% improvement of the combined approach over visual/annotation only).</p>
<p>Embedding similarity distribution. Another indication of the effectiveness of the learned embeddings is that the distances be-tween random pairs of item embeddings are widely distributed. If all items are at about the same distance (i.e., the distances are tightly clustered) then the embedding space does not have enough “resolu-tion” to distinguish between items of different relevance. Figure 4 plots the distribution of cosine similarities between pairs of items using annotation, visual, and PinSage embeddings. This distribution of cosine similarity between random pairs of items demonstrates the effectiveness of PinSage, which has the most spread out distribu-tion. In particular, the kurtosis of the cosine similarities of PinSage embeddings is 0.43, compared to 2.49 for annotation embeddings and 1.20 for visual embeddings.<br /> Another important advantage of having such a wide-spread in the embeddings is that it reduces the collision probability of the subsequent LSH algorithm, thus increasing the efficiency of serving the nearest neighbor pins during recommendation.<br /> 由于候选人数量大（超过20亿），我们在等式（2）中使用MRR的缩放版本，其中Ri，q是查询q的推荐项目中项目i的等级，n是总数标记的项目对的数量。缩放因子100确保例如，等级为1,000和等级为2,000的差异仍然是显着的，而不是非常接近于0。</p>
<p>表1比较了使用命中率的各种方法的性能以及MRR.5 PinSage与我们新的重要性汇总聚合和硬阴性示例在67％命中率和0.59 MRR下达到最佳性能，优于最高基线就命中率而言绝对值为40％（相对于150％），就MRR而言绝对值为22％（相对于60％）。我们还观察到，将视觉和文本信息结合起来比单独使用任何一个都要好得多（组合方法仅比视觉/注释提高60％）。</p>
<p>嵌入相似度分布。学习嵌入的有效性的另一个指示是随机对项目嵌入之间的距离被广泛分布。如果所有项目都处于大约相同的距离（即，距离紧密聚集），则嵌入空间没有足够的“分辨率”来区分不同相关的项目。图4绘制了使用注释，视觉和PinSage嵌入的项目对之间余弦相似度的分布。随机对项之间的余弦相似性的这种分布证明了PinSage的有效性，其具有最大的分布。特别是，PinSage嵌入的余弦相似度的峰度为0.43，而注释嵌入为2.49，视觉嵌入为1.20。<br /> 在嵌入中具有如此广泛的扩展的另一个重要优点是它降低了后续LSH算法的冲突概率，从而提高了在推荐期间服务最近邻居引脚的效率。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093124819.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 图4：视觉嵌入，注释嵌入和Pin-Sage嵌入的成对余弦相似度的概率密度。</p>
<h4><a id="43	User_Studies_359"></a>4.3 User Studies</h4>
<p>We also investigate the effectiveness of PinSage by performing head-to-head comparison between different learned representations. In the user study, a user is presented with an image of the query pin, together with two pins retrieved by two different recommendation algorithms. The user is then asked to choose which of the two candidate pins is more related to the query pin. Users are instructed to find various correlations between the recommended items and the query item, in aspects such as visual appearance, object category and personal identity. If both recommended items seem equally related, users have the option to choose “equal”. If no consensus is reached among 2/3 of users who rate the same question, we deem the result as inconclusive.</p>
<p>Table 2 shows the results of the head-to-head comparison be-tween PinSage and the 4 baselines. Among items for which the user has an opinion of which is more related, around 60% of the pre-ferred items are recommended by PinSage. Figure 5 gives examples of recommendations and illustrates strengths and weaknesses of the different methods. The image to the left represents the query item. Each row to the right corresponds to the top recommendations made by the visual embedding baseline, annotation embedding baseline, Pixie, and PinSage. Although visual embeddings gener-ally predict categories and visual similarity well, they occasionally make large mistakes in terms of image semantics. In this example, visual information confused plants with food, and tree logging with war photos, due to similar image style and appearance. The graph-based Pixie method, which uses the graph of pin-to-board relations, correctly understands that the category of query is “plants” and it recommends items in that general category. However, it does not find the most relevant items. Combining both visual/textual and graph information, PinSage is able to find relevant items that are both visually and topically similar to the query item.</p>
<p>我们还通过在不同的学习表示之间进行头对头比较来研究PinSage的有效性。在用户研究中，向用户呈现查询引脚的图像，以及由两个不同推荐算法检索的两个引脚。然后要求用户选择两个候选引脚中的哪一个与查询引脚更相关。指示用户在视觉外观，对象类别和个人身份等方面找到推荐项目和查询项目之间的各种相关性。如果两个推荐项似乎相同，则用户可以选择“相等”。如果对同一问题评分的2/3的用户未达成共识，我们认为结果不确定。</p>
<p>表2显示了PinSage与4个基线之间的头对头比较结果。在用户具有更多相关意见的项目中，PinSage推荐约60％的优先项目。图5给出了建议的示例，并说明了不同方法的优缺点。左侧的图像表示查询项。右侧的每一行对应于可视嵌入基线，注释嵌入基线，Pixie和PinSage所做的最高建议。虽然视觉嵌入通常可以很好地预测类别和视觉相似性，但它们偶尔会在图像语义方面犯大错误。在这个例子中，由于类似的图像样式和外观，视觉信息使植物与食物混淆，并且树木记录与战争照片。基于图形的Pixie方法使用引脚到板的关系图，正确地理解查询的类别是“植物”，并且它推荐该一般类别中的项目。但是，它没有找到最相关的项目。结合视觉/文本和图形信息，PinSage能够找到与查询项目在视觉上和主题上相似的相关项目。</p>
<p>In addition, we visualize the embedding space by randomly choosing 1000 items and compute the 2D t-SNE coordinates from the PinSage embedding, as shown in Figure 6.6 We observe that the proximity of the item embeddings corresponds well with the simi-larity of content, and that items of the same category are embedded into the same part of the space. Note that items that are visually different but have the same theme are also close to each other in the embedding space, as seen by the items depicting different fashion-related items on the bottom side of the plot.</p>
<p>此外，我们通过随机选择1000个项目来可视化嵌入空间，并从PinSage嵌入计算2D t-SNE坐标，如图6.6所示。我们观察到项目嵌入的接近度与内容的相似性很好地对应， 并且相同类别的项目嵌入到空间的相同部分中。 请注意，视觉上不同但具有相同主题的项目在嵌入空间中也彼此接近，如在绘图底部描绘不同时尚相关项目的项目所示。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093244394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4><a id="44	Production_AB_Test_375"></a>4.4 Production A/B Test</h4>
<p>Lastly, we also report on the production A/B test experiments, which compared the performance of PinSage to other deep learning content-based recommender systems at Pinterest on the task of homefeed recommendations. We evaluate the performance by ob-serving the lift in user engagement. The metric of interest is repin rate, which measures the percentage of homefeed recommendations that have been saved by the users. A user saving a pin to a board is a high-value action that signifies deep engagement of the user. It means that a given pin presented to a user at a given time was relevant enough for the user to save that pin to one of their boards so that they can retrieve it later.</p>
<p>We find that PinSage consistently recommends pins that are more likely to be re-pinned by the user than the alternative methods. Depending on the particular setting, we observe 10-30% improve-ments in repin rate over the Annotation and Visual embedding based recommendations.<br /> 最后，我们还报告了生产A / B测试实验，该实验比较了PinSage与Pinterest上其他基于深度学习内容的推荐系统在家庭饲料建议任务上的表现。 我们通过观察用户参与的提升来评估性能。 感兴趣的度量标准是repin rate，用于衡量用户保存的家庭馈送建议的百分比。 用户将引脚保存到电路板是一种高价值的操作，表示用户的深度参与。 这意味着在给定时间呈现给用户的给定引脚足够相关，用户可以将该引脚保存到其中一个板上，以便以后可以检索它。</p>
<p>我们发现PinSage始终推荐比其他方法更有可能被用户重新固定的引脚。 根据特定设置，我们观察到基于Annotation和Visual embedding建议的重复率提高了10-30％。</p>
<h5><a id="45	Training_and_Inference_Runtime_Analysis_383"></a>4.5 Training and Inference Runtime Analysis</h5>
<p>One advantage of GCNs is that they can be made inductive [19]: at the inference (i.e., embedding generation) step, we are able to compute embeddings for items that were not in the training set. This allows us to train on a subgraph to obtain model parameters, and then make embed nodes that have not been observed during training. Also note that it is easy to compute embeddings of new nodes that get added into the graph over time. This means that recommendations can be made on the full (and constantly grow-ing) graph. Experiments on development data demonstrated that training on a subgraph containing 300 million items could achieve the best performance in terms of hit-rate (i.e., further increases in the training set size did not seem to help), reducing the runtime by a factor of 6 compared to training on the full graph.<br /> Table 3 shows the the effect of batch size of the minibatch SGD on the runtime of PinSage training procedure, using the mean-pooling-hard variant. For varying batch sizes, the table shows: (1) the computation time, in milliseconds, for each minibatch, when varying batch size; (2) the number of iterations needed for the model to converge; and (3) the total estimated time for the training proce-dure. Experiments show that a batch size of 2048 makes training most efficient.</p>
<p>When training the PinSage variant with importance pooling, another trade-off comes from choosing the size of neighborhood T . Table 3 shows the runtime and performance of PinSage when</p>
<p>T= 10, 20 and 50. We observe a diminishing return as T increases, and find that a two-layer GCN with neighborhood size 50 can best capture the neighborhood information of nodes, while still being computationally efficient.<br /> After training completes, due to the highly efficient MapReduce inference pipeline, the whole inference procedure to generate em-beddings for 3 billion items can finish in less than 24 hours.</p>
<p>GCN的一个优点是可以使它们具有归纳性[19]：在推理（即嵌入生成）步骤中，我们能够计算不在训练集中的项目的嵌入。这允许我们在子图上训练以获得模型参数，然后制作在训练期间未观察到的嵌入节点。另请注意，计算新节点的嵌入很容易随着时间的推移而添加到图形中。这意味着可以在完整（并且不断增长）的图表上进行推荐。对开发数据的实验表明，对包含3亿个项目的子图的训练可以在命中率方面达到最佳性能（即，训练集大小的进一步增加似乎没有帮助），将运行时间缩短了6倍与完整图表上的培训相比。<br /> 表3显示了使用mean-pooling-hard变体，miniatch SGD的批量大小对PinSage训练过程的运行时间的影响。对于不同的批量大小，该表显示：（1）当批量大小变化时，每个小批量的计算时间（以毫秒为单位）; （2）模型收敛所需的迭代次数; （3）培训程序的总预计时间。实验表明，2048的批量大小使培训效率最高。</p>
<p>在训练具有重要性汇集的PinSage变体时，另一个权衡取决于选择邻域T的大小。表3显示了PinSage的运行时和性能</p>
<p>T = 10,20和50.随着T的增加，我们观察到收益递减，并发现邻域大小为50的双层GCN可以最好地捕获节点的邻域信息，同时仍然具有计算效率。<br /> 培训完成后，由于高效的MapReduce推理管道，生成30亿件物品的整体推理程序可以在不到24小时内完成。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093352552.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 图5：不同算法推荐的Pinterest引脚示例。 左边的图像是查询引脚。 使用Visual em-beddings，Annotation embeddings，基于图形的Pixie和PinSage计算右侧的推荐项目。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093445155.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4><a id="Figure_6_tSNE_plot_of_item_embeddings_in_2_dimensions_405"></a>Figure 6: t-SNE plot of item embeddings in 2 dimensions.</h4>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070709351932.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h6><a id="Table_3_Runtime_comparisons_for_different_batch_sizes_409"></a>Table 3: Runtime comparisons for different batch sizes.</h6>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093557957.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4><a id="Table_4_Performance_tradeoffs_for_importance_pooling_412"></a>Table 4: Performance tradeoffs for importance pooling.</h4>
<h4><a id="5	CONCLUSION_415"></a>5 CONCLUSION</h4>
<p>We proposed PinSage, a random-walk graph convolutional network (GCN). PinSage is a highly-scalable GCN algorithm capable of learn-ing embeddings for nodes in web-scale graphs containing billions of objects. In addition to new techniques that ensure scalability, we introduced the use of importance pooling and curriculum training that drastically improved embedding performance. We deployed PinSage at Pinterest and comprehensively evaluated the quality of the learned embeddings on a number of recommendation tasks, with offline metrics, user studies and A/B tests all demonstrating a substantial improvement in recommendation performance. Our work demonstrates the impact that graph convolutional methods can have in a production recommender system, and we believe that PinSage can be further extended in the future to tackle other graph representation learning problems at large scale, including knowledge graph reasoning and graph clustering.<br /> 我们提出了PinSage，一种随机游走图卷积网络（GCN）。 PinSage是一种高度可扩展的GCN算法，能够在包含数十亿个对象的Web级图形中学习节点的嵌入。 除了确保可扩展性的新技术之外，我们还引入了重要性池和课程训练，大大提高了嵌入性能。 我们在Pinterest部署了PinSage，并在一系列推荐任务中全面评估了学习嵌入的质量，离线指标，用户研究和A / B测试都证明了推荐性能的实质性改进。 我们的工作展示了图卷积方法在生产推荐系统中可能产生的影响，我们相信PinSage可以在未来进一步扩展，以解决大规模的其他图表表示学习问题，包括知识图推理和图聚类。</p>
<h2><a id="Acknowledgments_419"></a>Acknowledgments</h2>
<p>The authors acknowledge Raymond Hsu, Andrei Curelea and Ali Altaf for performing various A/B tests in production system, Jerry</p>
<p>Zitao Liu for providing data used by Pixie[14], and Vitaliy Kulikov for help in nearest neighbor query of the item embeddings.</p>
<h2><a id="REFERENCES_429"></a>REFERENCES</h2>
<p>[1]M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al. 2016. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 (2016).</p>
<p>[2]A. Andoni and P. Indyk. 2006. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. In FOCS.<br /> [3]T. Bansal, D. Belanger, and A. McCallum. 2016. Ask the GRU: Multi-task learning for deep text recommendations. In RecSys. ACM.<br /> [4]Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009. Curriculum learning. In ICML.<br /> [5]A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. 2003. Efficient query evaluation using a two-level retrieval process. In CIKM.<br /> [6]M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. 2017. Geometric deep learning: Going beyond euclidean data. IEEE Signal Processing Magazine 34, 4 (2017).</p>
<p>[7]J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. 2014. Spectral networks and locally connected networks on graphs. In ICLR.<br /> [8]J. Chen, T. Ma, and C. Xiao. 2018. FastGCN: Fast Learning with Graph Convolu-tional Networks via Importance Sampling. ICLR (2018).<br /> [9]P. Covington, J. Adams, and E. Sargin. 2016. Deep neural networks for youtube recommendations. In RecSys. ACM.<br /> [10]H. Dai, B. Dai, and L. Song. 2016. Discriminative Embeddings of Latent Variable Models for Structured Data. In ICML.<br /> [11]M. Defferrard, X. Bresson, and P. Vandergheynst. 2016. Convolutional neural networks on graphs with fast localized spectral filtering. In NIPS.<br /> [12]A. Van den Oord, S. Dieleman, and B. Schrauwen. 2013. Deep content-based music recommendation. In NIPS.<br /> [13]D. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. 2015. Convolutional networks on graphs for learning molecular fingerprints. In NIPS.</p>
<p>[14]C. Eksombatchai, P. Jindal, J. Z. Liu, Y. Liu, R. Sharma, C. Sugnet, M. Ulrich, and</p>
<p>J.Leskovec. 2018. Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time. WWW (2018).<br /> [15]M. Gori, G. Monfardini, and F. Scarselli. 2005. A new model for learning in graph domains. In IEEE International Joint Conference on Neural Networks.</p>
<p>[16]P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch,</p>
<p>Y.Jia, and K. He. 2017. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. arXiv preprint arXiv:1706.02677 (2017).<br /> [17]A. Grover and J. Leskovec. 2016. node2vec: Scalable feature learning for networks. In KDD.<br /> [18]W. L. Hamilton, R. Ying, and J. Leskovec. 2017. Inductive Representation Learning on Large Graphs. In NIPS.<br /> [19]W. L. Hamilton, R. Ying, and J. Leskovec. 2017. Representation Learning on Graphs: Methods and Applications. IEEE Data Engineering Bulletin (2017).</p>
<p>[20]S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley. 2016. Molecular graph convolutions: moving beyond fingerprints. CAMD 30, 8.<br /> [21]T. N. Kipf and M. Welling. 2017. Semi-supervised classification with graph convolutional networks. In ICLR.<br /> [22]Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. 2015. Gated graph sequence neural networks. In ICLR.<br /> [23]T. Mikolov, I Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS.<br /> [24]F. Monti, M. M. Bronstein, and X. Bresson. 2017. Geometric matrix completion with recurrent multi-graph neural networks. In NIPS.<br /> [25]OpenMP Architecture Review Board. 2015. OpenMP Application Program Inter-face Version 4.5. (2015).<br /> [26]B. Perozzi, R. Al-Rfou, and S. Skiena. 2014. DeepWalk: Online learning of social representations. In KDD.<br /> [27]F. Scarselli, M. Gori, A.C. Tsoi, M. Hagenbuchner, and G. Monfardini. 2009. The graph neural network model. IEEE Transactions on Neural Networks 20, 1 (2009), 61–80.</p>
<p>[28]K. Simonyan and A. Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).<br /> [29]R. van den Berg, T. N. Kipf, and M. Welling. 2017. Graph Convolutional Matrix Completion. arXiv preprint arXiv:1706.02263 (2017).<br /> [30]J. You, R. Ying, X. Ren, W. L. Hamilton, and J. Leskovec. 2018. GraphRNN: Generating Realistic Graphs using Deep Auto-regressive Models. ICML (2018).<br /> [31]M. Zitnik, M. Agrawal, and J. Leskovec. 2018. Modeling polypharmacy side effects with graph convolutional networks. Bioinformatics (2018).</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>DCN:Deep &#038; Cross Network for Ad Click Predictions简介</title>
		<link>https://uzzz.org/article/1604.html</link>
				<pubDate>Sat, 16 Mar 2019 08:16:39 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[论文]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1604.html</guid>
				<description><![CDATA[Deep &#38; Cross Network for Ad Click Predictions 摘要 作者起草了DCN，该网络可以保持DNN的优点（隐式地生成特征之间的交互），同时又利用交叉网络来对特征进行显式的交叉计算。这也不要求手工的特征工程，同时只是在DNN的基础上加了一些可容忍的复杂度。实验证明DCN已经在CTR预估与分类问题上超过了sota。 ]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<h2><a id="Deep__Cross_Network_for_Ad_Click_Predictions_0"></a>Deep &amp; Cross Network for Ad Click Predictions</h2>
<h3><a id="_2"></a>摘要</h3>
<p>作者起草了DCN，该网络可以保持DNN的优点（隐式地生成特征之间的交互），同时又利用交叉网络来对特征进行显式的交叉计算。这也不要求手工的特征工程，同时只是在DNN的基础上加了一些可容忍的复杂度。实验证明DCN已经在CTR预估与分类问题上超过了sota。</p>
<h3><a id="_6"></a>介绍</h3>
<p>对于web伸缩型的推荐系统，因为其产生的数据较为稀疏，对于线性模型来说已经不太好处理了。因此交叉特征变得很重要，但是这经常要求我们手动特征工程，为了减少这方面的工作，交叉网络应运而生。同时联合DNN，发挥两者的共同优势。</p>
<h3><a id="_10"></a>嵌入和堆叠层</h3>
<p>对于离散数据，一般处理时会被编码成one-hot向量，对于实际应用中维度会非常高，因此使用<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161332569.png" alt="在这里插入图片描述"></p>
<p>来将这些离散特征转换成实数值的稠密向量，最后将嵌入向量和连续特征向量堆叠在一起形成一个向量。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161342759.png" alt="在这里插入图片描述"></p>
<h3><a id="_19"></a>交叉网络</h3>
<p>对于每层的计算，使用下述公式：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019031616135250.png" alt="在这里插入图片描述"></p>
<p>一层交叉层的可视化如下图所示：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161400699.png" alt="在这里插入图片描述"></p>
<p>该网络可以使交叉特征的次数随着层数的增加而不断变大，对于l层其最高多项式次数为l+1。</p>
<p>计算的时间与空间复杂度都是线性，因此DCN的效率与DNN是一个量级的。</p>
<h3><a id="_33"></a>深度网络</h3>
<p>深度网络就是一个全连接的前馈神经网络，每个深度层具有如下公式：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161416355.png" alt="在这里插入图片描述"></p>
<h3><a id="_39"></a>链接层</h3>
<p>将两个网络的输出联合，送进标准的logits层。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161424323.png" alt="在这里插入图片描述"></p>
<p>正则化的log loss函数：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161434957.png" alt="在这里插入图片描述"></p>
<h3><a id="_49"></a>实验</h3>
<ol>
<li>
<p>比较DCN，DC，DNN，FM，LR模型的最好的logloss</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161443600.png" alt="在这里插入图片描述"></p>
</li>
<li>
<p>比较实现对应的logloss，DNN和DCN需要的参数数量</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019031616145798.png" alt="在这里插入图片描述"></p>
</li>
<li>
<p>在固定参数下实现最好的logloss所需要的内存</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161512312.png" alt="在这里插入图片描述"></p>
</li>
<li>
<p>在层数与结点一致的情况下，比较DNN与DCN的logloss(X<span class="katex--inline"><span class="katex"><span class="katex-mathml"></p>
<math>
         <semantics><br />
          <mrow><br />
           <mn><br />
            1<br />
           </mn><br />
           <msup><br />
            <mn><br />
             0<br />
            </mn><br />
            <mrow><br />
             <mo><br />
              −<br />
             </mo><br />
             <mn><br />
              2<br />
             </mn><br />
            </mrow><br />
           </msup><br />
          </mrow><br />
          <annotation encoding="application/x-tex"><br />
           10^{-2}<br />
          </annotation><br />
         </semantics><br />
        </math>
<p></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.814108em; vertical-align: 0em;"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span>)差距，负值代表DCN表现好于DNN</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161528807.png" alt="在这里插入图片描述"></p>
</li>
<li>
<p>最后展示了不同设置的变化趋势。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190316161542428.png" alt="在这里插入图片描述"></p>
</li>
</ol>
<p>可以参考我的<a href="https://github.com/loserChen/TensorFlow-In-Practice" rel="nofollow" data-token="6687502f53a5f6339d52280282678380">github</a>来看看源代码，如有错误，欢迎交流。</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>《Learning Deep Structured Semantic Models for Web Search using Clickthrough Data 》论文总结</title>
		<link>https://uzzz.org/article/1578.html</link>
				<pubDate>Tue, 12 Mar 2019 14:14:10 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[统计搜索]]></category>
		<category><![CDATA[论文]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1578.html</guid>
				<description><![CDATA[1.背景 DSSM是Deep Structured Semantic Model的缩写，即我们通常说的基于深度网络的语义模型，其核心思想是将query和doc映射到到共同维度的语义空间中，通过最大化query和doc语义向量之间的余弦相似度，从而训练得到隐含语义模型，达到检索的目的。DSSM有很广泛的应用，比如：搜索引擎检索，广告相关性，问答系统，机器翻译等]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<h2><strong>1.背景</strong></h2>
<p>DSSM是Deep Structured Semantic Model的缩写，即我们通常说的基于深度网络的语义模型，其核心思想是将query和doc映射到到共同维度的语义空间中，通过最大化query和doc语义向量之间的余弦相似度，从而训练得到隐含语义模型，达到检索的目的。DSSM有很广泛的应用，比如：搜索引擎检索，广告相关性，问答系统，机器翻译等。</p>
<h2><a name="t3"></a>2. DSSM</h2>
<p><strong>2.1简介</strong></p>
<p>DSSM [1]（Deep Structured Semantic Models）的原理很简单，通过搜索引擎里 Query 和 Title 的海量的点击曝光日志，用 DNN 把 Query 和 Title 表达为低纬语义向量，并通过 cosine 距离来计算两个语义向量的距离，最终训练出语义相似度模型。该模型既可以用来预测两个句子的语义相似度，又可以获得某句子的低纬语义向量表达。</p>
<p>DSSM 从下往上可以分为三层结构：输入层、表示层、匹配层</p>
<p><img alt="" class="has" src="https://blog-10039692.file.myqcloud.com/1501555296606_1048_1501555297548.png"></p>
<p>典型的DNN结构是将原始的文本特征映射为在语义空间上表示的特征。DNN在搜索引擎排序中主要是有下面2个作用：</p>
<ol>
<li>将query中term的高维向量映射为低维语义向量</li>
<li>根据语义向量计算query与doc之间的相关性分数</li>
</ol>
<p>通常， x用来表示输入的term向量， y表示输出向量， l_{i}，i=1,&#8230;,N-1 表示隐藏层， Wi表示第 i层的参数矩</p>
<p>阵， bi表示 第 i个偏置项。</p>
<p><img alt="" class="has" height="131" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019031221392816.png" width="425"></p>
<p>我们使用 tanh作为输出层和隐藏层的激活函数，有下列公式。</p>
<p><img alt="" class="has" height="72" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190312214010171.png" width="186"></p>
<p>在搜索排序中，我们使用 Q来表示一个query， D来表示一个doc，那么他们的相关性分数可以用下面的公式衡量：</p>
<p><img alt="" class="has" height="86" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190312214109619.png" width="436"></p>
<p>其中， yQ与 yD是query与doc的语义向量。在搜索引擎中，给定一个query，会返回一些按照相关性分数排序的文档。</p>
<p>通常情况下，输入的term向量使用最原始的bag of words特征，通过one-hot进行编码。但是在实际场景中，词典的大小将会非常大，如果直接将该数据输入给DNN，神经网络是无法进行训练和预测的。因此，在DSSM中引入了word hashing的方法，并且作为DNN中的第一层。</p>
<p><strong>2.2 word hashing</strong></p>
<p>word hashing方法是用来减少输入向量的维度，该方法基于字母的 <img alt="n" class="has" src="http://www.zhihu.com/equation?tex=n"> -gram。给定一个单词（good），我们首先增加词的开始和结束部分（#good#），然后将该词转换为字母 <img alt="n" class="has" src="http://www.zhihu.com/equation?tex=n"> -gram的形式（假设为trigrams：#go，goo，ood，od#）。最后该词使用字母&nbsp; <img alt="n" class="has" src="http://www.zhihu.com/equation?tex=n"> -gram的向量来表示。</p>
<p>这种方法的问题在于有可能造成冲突，因为两个不同的词可能有相同的<img alt="n" class="has" src="http://www.zhihu.com/equation?tex=n"> -gram向量来表示。下图显示了word hashing在2个词典中的统计。与原始的ont-hot向量表示的词典大小相比，word hashing明显降低了向量表示的维度。</p>
<p><img alt="" class="has" height="203" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190312214630343.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zODUyNjMwNg==,size_16,color_FFFFFF,t_70" width="637"></p>
<p><strong>2.3 DSSM的学习</strong></p>
<p>点击日志里通常包含了用户搜索的query和用户点击的doc，可以假定如果用户在当前query下对doc进行了点击，则该query与doc是相关的。通过该规则，可以通过点击日志构造训练集与测试集。</p>
<p>首先，通过softmax 函数可以把query 与样本 doc 的语义相似性转化为一个后验概率：</p>
<p><img alt="" class="has" height="83" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190312214822961.png" width="450"></p>
<p>其中 gamma是一个softmax函数的平滑因子， D表示被排序的候选文档集合，在实际中，对于正样本，每一个（query， 点击doc）对，使用 (Q, D^{+}) 表示；对于负样本，随机选择4个曝光但未点击的doc。</p>
<p>在训练阶段，通过极大似然估计来最小化损失函数：</p>
<p><img alt="" class="has" height="88" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190312215302794.png" width="315"></p>
<p>其中 <img alt="" class="has" height="30" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190312215352600.png" width="20">表示神经网络的参数。模型通过随机梯度下降（SGD）来进行优化，最终可以得到各网络层的参数 <img alt="" class="has" height="25" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190312215414301.png" width="50">。</p>
<p>&nbsp;</p>
<h2><strong>3.总结</strong></h2>
<p>DSSM的提出主要有下面的优点：</p>
<ul>
<li>解决了LSA、LDA、Autoencoder等方法存在的一个最大的问题：字典爆炸（导致计算复杂度非常高），因为在英文单词中，词的数量可能是没有限制的，但是字母 <img alt="n" class="has" src="http://www.zhihu.com/equation?tex=n"> -gram的数量通常是有限的</li>
<li>基于词的特征表示比较难处理新词，字母的 <img alt="n" class="has" src="http://www.zhihu.com/equation?tex=n"> -gram可以有效表示，鲁棒性较强</li>
<li>使用有监督方法，优化语义embedding的映射问题</li>
<li>省去了人工的特征工程</li>
<li>传统的输入层是用 Embedding 的方式（如 Word2Vec 的词向量）或者主题模型的方式（如 LDA 的主题向量）来直接做词的映射，再把各个词的向量累加或者拼接起来，由于 Word2Vec 和 LDA 都是无监督的训练，这样会给整个模型引入误差，DSSM 采用统一的有监督训练，不需要在中间过程做无监督模型的映射，因此精准度会比较高。</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>word hashing可能造成冲突</li>
<li>DSSM采用了词袋模型，损失了上下文信息</li>
<li>在排序中，搜索引擎的排序由多种因素决定，由于用户点击时doc的排名越靠前，点击的概率就越大，如果仅仅用点击来判断是否为正负样本，噪声比较大，难以收敛(DSSM 是弱监督模型)</li>
<li>DSSM 是端到端的模型，虽然省去了人工特征转化、特征工程和特征组合，但端到端的模型有个问题就是效果不可控。</li>
</ul>
<p>对于中文而言，处理方式与英文有很多不一样的地方。中文往往需要进行分词，但是我们可以仿照英文的处理方式，将中文的最小粒度看作是单字（在某些文献里看到过用偏旁部首，笔画，拼音等方法）。因此，通过这种word hashing方式，可以将向量空间大大降低。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>DehazeNet: An End-to-End System for Single Image Haze Removal</title>
		<link>https://uzzz.org/article/2994.html</link>
				<pubDate>Fri, 04 May 2018 11:15:37 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[计算机]]></category>
		<category><![CDATA[论文]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2994.html</guid>
				<description><![CDATA[项目主页：http://caibolun.github.io/DehazeNet/ GitHub代码 ：https://github.com/caibolun/DehazeNet BReLU+Caffe ：https://github.com/zlinker/mycaffe 其他复现：（1）https://github.com/zlinker/DehazeN]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
<p style="margin-left:0px;">项目主页：http://caibolun.github.io/DehazeNet/</p>
<p style="margin-left:0px;">GitHub代码 ：https://github.com/caibolun/DehazeNet</p>
<p style="margin-left:0px;">BReLU+Caffe ：https://github.com/zlinker/mycaffe</p>
<p style="margin-left:0px;">其他复现：（1）https://github.com/zlinker/DehazeNet&nbsp; &nbsp; （2）https://github.com/allenyangyl/dehaze</p>
<p style="margin-left:0px;">总结：</p>
<blockquote>
<p style="margin-left:0px;">提出一种名为DehazeNet的可训练的端到端系统，用于传输值估计。 DehazeNet将模糊图像作为输入，并输出其中间透射图，随后用于通过大气散射模型恢复无雾图像。 DehazeNet采用基于卷积神经网络的深层架构，<strong>其层专门设计用于体现图像去雾中已建立的假设/先验</strong>。具体而言，Maxout单位的图层用于特征提取，这可以生成几乎所有与雾相关的特征。我们还在DehazeNet中提出了一种新的非线性激活函数，称为<strong>双边整流线性单元</strong>，它能够提高恢复的无雾图像的质量。我们在提议的<strong>DehazeNet的组件与现有方法中使用的组件之间建立连接</strong>。基准图像的实验表明，DehazeNet比现有方法具有更高的性能，同时保持高效和易用。</p>
</blockquote>
<h2>摘要</h2>
<blockquote>
<p>背景：Single image haze removal is a challenging ill-posed problem.</p>
<p>现存方法：Existing methods use various constraints/priors to get plausible dehazing solutions. The key to achieve haze removal is to estimate a medium transmission map for an input hazy image.</p>
<p>提出的方法：In this paper, we propose a trainable end-to-end system called DehazeNet, for medium transmission estimation. DehazeNet takes a hazy image as input, and outputs its medium transmission map that is subsequently used to recover a haze-free image via atmospheric scattering model. DehazeNet adopts convolutional neural network-based deep architecture, whose layers are specially designed to <strong>embody the established assumptions/priors in image dehazing</strong>. Specifically<strong>, the layers of Maxout units are used for feature extraction, which can generate almost all haze- relevant features</strong>. We also propose a novel nonlinear activation function in DehazeNet, called <strong>bilateral rectified linear unit</strong>, which is able to improve the quality of recovered haze-free image. We establish connections between the components of the proposed DehazeNet and those used in existing methods. Experiments on benchmark images show that DehazeNet achieves superior performance over existing methods, yet keeps efficient and easy to use.&nbsp;</p>
</blockquote>
<h2>贡献：</h2>
<ul>
<li>DehazeNet是一个端到端系统。 它直接学习和估计模糊图像patches与其传输图之间的映射关系。 这是通过其深层架构的特殊设计来实现的，以体现已建立的图像去雾原理。</li>
<li>提出了一种新的非线性激活函数，称为双边整流线性单元1（BReLU）。 BReLU扩展了整流线性单元（ReLU）并证明了其在获得精确图像恢复方面的重要性。 从技术上讲，BReLU使用双边约束来减少搜索空间并改善收敛。</li>
<li>在DehazeNet的组件与现有的去雾方法中使用的假设/先验之间建立联系，并解释DehazeNet通过自动从头到尾自动学习所有这些组件来改进这些方法。&nbsp;</li>
</ul>
<p style="text-align:center;"><img alt="" class="has" height="625" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190422220212394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0p1bGlhbG92ZTEwMjEyMw==,size_16,color_FFFFFF,t_70" width="1000"></p>
<h2><strong>图像去雾的核心</strong></h2>
<p>现有的<strong>图像去雾</strong>（Image Dehazing）技术离不开一个简单的自然模型——<strong>大气散射模型（Atmospheric Scattering Model）</strong>。大气散射模型描述了，在雾霾和光照的共同作用下的成像机制：</p>
<p style="text-align:center;"><img alt="" class="has" src="http://img.mp.sohu.com/upload/20170808/a56c40761c12470ab82253e51f24fd65_th.png" style="margin-left:auto;"></p>
<p>阳光在物体表面形成反射光&nbsp;J(x)，反射光在穿过雾霾的过程发生散射，只有部分能量&nbsp;J(x)t(x) 能到达摄像头。与此同时，阳光也在悬浮颗粒表面散射形成大气光&nbsp;α&nbsp;被摄像头接收。因此，摄像头中的成像&nbsp;I(x) 可由两部分组成，透射的物体亮度&nbsp;J(x)t(x) 和散射的大气光照&nbsp;α(1－t(x))：</p>
<p style="text-align:center;"><img alt="" class="has" src="http://img.mp.itc.cn/upload/20170808/9e2049510f33480c9071895e8daa1c07.jpg" style="margin-left:auto;"></p>
<p>其中，t(x) 是<strong>媒介透射率</strong>（medium transmission），顾名思义表示<strong>能顺利透过雾霾到达摄像头的比率</strong>。因此，透射率跟物体与摄像头距离&nbsp;d(x) 成反比，离摄像头越远的物体受雾霾影响更大。当距离&nbsp;d(x) 趋于无穷大时，透射率&nbsp;t(x) 趋于零，I(x) 趋近于&nbsp;α，α=maxy∈{x|t(x)≤t0}I(y)。综上所述，去雾的核心是如何更精确地估计媒介透射率&nbsp;t(x)。</p>
<p style="text-align:center;"><img alt="" class="has" src="http://img.mp.sohu.com/upload/20170808/38e7afbc7b574efa93938fe9a353ff92_th.png" style="margin-left:auto;"></p>
<h2><strong>基于人工特征</strong></h2>
<p>手工特征是传统机器视觉的基础，讲究的是熟能生巧，依赖的是实践出真知。通过“<strong>观察→经验→设计</strong>”构建各式各样的特征来满足各式各样的任务需求。图像去雾技术也是沿着手工特征逐步地发展起来。</p>
<p><strong>（1）暗通道先验[2]（Dark Channel Prior，DCP）</strong></p>
<p>说起去雾特征，不得不提起的暗通道先验（DCP）。大道之行在于简，DCP作为CVPR 2009的最佳论文，以简洁有效的先验假设解决了雾霾浓度估计问题。</p>
<p>观察发现，<strong>清晰图像块的RGB颜色空间中有一个通道很暗（数值很低甚至接近于零）。因此基于暗通道先验，雾的浓度可由最暗通道的数值近似表示：</strong></p>
<p style="text-align:center;"><img alt="" class="has" src="http://img.mp.itc.cn/upload/20170808/ed775fc52cca4d84960219a5a8059c0e.jpg" style="margin-left:auto;"></p>
<p><strong>（2）最大对比度[3]（Maximum Contrast，MC）</strong></p>
<p>根据大气散射模型，雾霾会降低物体成像的对比度：Σx‖ΔI(x)‖=tΣx‖ΔJ(x)‖≤Σx‖ΔJ(x)‖。因此，基于这个推论可利用局部对比度来近似估计雾霾的浓度。同时，也可以通过最大化局部对比度来还原图像的颜色和能见度。</p>
<p style="text-align:center;"><img alt="" class="has" src="http://img.mp.itc.cn/upload/20170808/d228189cb49740758d225a3c7ccc9177.jpg" style="margin-left:auto;"></p>
<p><strong>（3）颜色衰减先验[4]（Color Attenuation Prior，CAP）</strong></p>
<p>颜色衰减先验（CAP）是一种与暗通道先验（DCP）相似的先验特征。观察发现雾霾会同时导致图像饱和度的降低和亮度的增加，整体上表现为颜色的衰减。根据颜色衰减先验，亮度和饱和度的差值被应用于估计雾霾的浓度：<img alt="" class="has" src="http://www.sohu.com/a/162957608_717210" style="margin-left:auto;"></p>
<p style="text-align:center;"><img alt="" class="has" src="http://img.mp.itc.cn/upload/20170808/3e36ed40d5374b8bb385979654ccf86f_th.jpg" style="margin-left:auto;"></p>
<h2><strong>基于深度智能</strong></h2>
<p>人的视觉系统并不需依赖这些显式的特征变换，便可以很好地估计雾的浓度和场景的深度。<strong>DehazeNet</strong>是一个特殊设计的深度卷积网络，利用深度学习去智能地学习雾霾特征，解决手工特征设计的难点和痛点。</p>
<p style="text-align:center;"><img alt="" class="has" height="393" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190422220340674.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0p1bGlhbG92ZTEwMjEyMw==,size_16,color_FFFFFF,t_70" width="800"></p>
<p><strong>（1）特征提取（Feature Extraction）</strong></p>
<p>特征提取有别于传统卷积神经网络，DehazeNet采用“卷积+<strong><span style="color:#f33b45;">Maxout</span></strong>[5]”的结构作为网络第一层：</p>
<p style="text-align:center;"><img alt="" class="has" src="http://img.mp.itc.cn/upload/20170808/97f4d366337e4104884a90dabb8e75e7.jpg" style="margin-left:auto;"></p>
<p>并且可以证明，“卷积+Maxout”等价于传统的手工去雾特征：</p>
<p>当W1是反向（Opposite）滤波器,通道的最大等价于通道的最小值，等价于暗通道先验（DCP）；当W1是环形（Round）滤波器, 等价于对比度提取，等价于最大对比度（MC）；当W1同时包含反向（Opposite）滤波器和全通（All-pass）滤波器，等价于RGB到HSV颜色空间转换，等价于颜色衰减先验（CAP）。</p>
<p>此外，从机器学习角度看，Maxout是一种样条函数，具有更强的非线性拟合能力，如下图（d）。</p>
<p style="text-align:center;"><img alt="" class="has" src="http://img.mp.sohu.com/upload/20170808/17f9c759756e45e9a7c0c283ed1af3db_th.png" style="margin-left:auto;"></p>
<p><strong>（2）多尺度映射（Multi-scale Mapping）与局部极值（Local Extremum）</strong></p>
<p><strong>多尺度特征会提高不同分辨率下特征提取的鲁棒性</strong>。传统去雾方法中也会采用不同尺度的滤波器（均值、中值、最小值）来增强特征在不同尺度下的鲁棒性。借鉴于GoogLeNet中的inception结构，采用3组不同尺度（3×3，5×5，7×7）的滤波器实现DehazeNet的尺度鲁棒性：</p>
<p style="text-align:center;"><img alt="" class="has" height="64" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190422221448139.png" width="600"></p>
<p><strong>局部极值（MAX Pooling）是深度卷积神经网络的经典操作</strong>。局部极值约束了透射率的局部一致性，可以有效抑制透射率的估计噪声。此外，局部极值也对应于暗通道先验（DCP）的局部最小值和最大对比度（MC）的局部最大值。</p>
<p><strong>（3）非线性回归（Non</strong><strong>-linear Regression）</strong></p>
<p>大气透射率是一个概率（0到1），不可能无穷大，也不可能无穷小。受到Sigmoid和ReLU激励函数的启发，提出双边纠正线性单元（Bilateral Rectified Linear Unit，BReLU），在双边约束的同时，保证局部的线性。</p>
<p style="text-align:center;"><img alt="" class="has" src="http://img.mp.sohu.com/upload/20170808/a0e923384254446eaec60650e41e12d7_th.png" style="margin-left:auto;"></p>
<p>BReLU的非线性回归对应于传统去雾方法中的边缘抑制操作（如DCP和CAP）。双边约束引入先验信息缩小参数搜索空间，使得网络更加容易训练；局部线性避免Sigmoid函数梯度不一致带来的收敛困难。</p>
<p><strong>DehazeNet基于手工特征，又超出传统方法，从人工到智能</strong>。因此，DehazeNet取得了更好的去雾结果，更多的对比实验和代码资源：</p>
<h2 style="margin-left:0px;">参考文献</h2>
<p>[1] Cai B, Xu X, Jia K, et al. DehazeNet: An End-to-End System for Single Image Haze Removal [J]. IEEE Transactions on Image Processing, 2016, 25(11): 5187-5198.</p>
<p>[2] He K, Sun J, Tang X. Single image haze removal using dark channel prior[J]. IEEE transactions on pattern analysis and machine intelligence, 2011, 33(12): 2341-2353.</p>
<p>[3] Tan R T. Visibility in bad weather from a single image[C]//Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 2008: 1-8.</p>
<p>[4] Zhu Q, Mai J, Shao L. A fast single image haze removal algorithm using color attenuation prior[J]. IEEE Transactions on Image Processing, 2015, 24(11): 3522-3533.</p>
<p>[5] Goodfellow I J, Warde-Farley D, Mirza M, et al. Maxout networks[J]. ICML (3), 2013, 28: 1319-1327.</p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>DeepCoNN</title>
		<link>https://uzzz.org/article/1717.html</link>
				<pubDate>Thu, 15 Mar 2018 01:17:52 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[论文]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1717.html</guid>
				<description><![CDATA[DeepCoNN Lei Zheng, Vahid Noroozi, and Philip S. Yu. 2017. Joint Deep Modeling of Users and Items Using Reviews for Recommendation. In Proceedings of the Tenth ACM International Co]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-light">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<p>DeepCoNN <br /> Lei Zheng, Vahid Noroozi, and Philip S. Yu. 2017. Joint Deep Modeling of Users and Items Using Reviews for Recommendation. In Proceedings of the Tenth ACM International Conference on Web Search and Data Mining (WSDM ’17). ACM, New York, NY, USA, 425–434. <br /> <a href="h%C2%8Aps://doi.org/10.1145/3018661.3018665" rel="nofollow" data-token="ef9bce448909f7868730f1d571b81a77"><em>paper download link</em></a></p>
<p>多对多的关系 <br /> 用户u-评论文本text-物品i <br /> 验证集：(1) 调参 (2) 选参 <br /> “10% is treated as the validation set to tune the hyper-parameters”验证集的作用在于调整超参数。 <br /> “All the hyper-parameters of the baselines and DeepCoNN are <em>selected</em> based on the performance on the <strong>validation set</strong>.”</p>
<p>“pre-trained word embeddings” 有什么作用，怎么实现？ <br /> GloVe stands for “Global Vectors for Word Representation”. It’s a somewhat popular embedding technique based on factorizing a matrix of word <br /> co-occurence statistics.</p>
<p><a href="http://blog.csdn.net/lmm6895071/article/details/75688936" rel="nofollow" data-token="23a70d88bb1e93068f8b45573b0e8686">推荐系统总结</a></p>
<p><a href="http://blog.csdn.net/juanjuan1314/article/details/70045762" rel="nofollow" data-token="cda1fb0323a2102a63a71899d789ab4c">CDL</a></p>
<p>本Markdown编辑器使用<a href="https://github.com/benweet/stackedit" rel="nofollow" data-token="0815874d01397361bf8dfd53e0ac1bd7">StackEdit</a>修改而来，用它写博客，将会带来全新的体验哦：</p>
<ul>
<li><strong>Markdown和扩展Markdown简洁的语法</strong></li>
<li><strong>代码块高亮</strong></li>
<li><strong>图片链接和图片上传</strong></li>
<li><strong><em>LaTex</em>数学公式</strong></li>
<li><strong>UML序列图和流程图</strong></li>
<li><strong>离线写博客</strong></li>
<li><strong>导入导出Markdown文件</strong></li>
<li><strong>丰富的快捷键</strong></li>
</ul>
<hr>
<h2 id="快捷键">快捷键</h2>
<ul>
<li>加粗 <code>Ctrl + B</code> </li>
<li>斜体 <code>Ctrl + I</code> </li>
<li>引用 <code>Ctrl + Q</code></li>
<li>插入链接 <code>Ctrl + L</code></li>
<li>插入代码 <code>Ctrl + K</code></li>
<li>插入图片 <code>Ctrl + G</code></li>
<li>提升标题 <code>Ctrl + H</code></li>
<li>有序列表 <code>Ctrl + O</code></li>
<li>无序列表 <code>Ctrl + U</code></li>
<li>横线 <code>Ctrl + R</code></li>
<li>撤销 <code>Ctrl + Z</code></li>
<li>重做 <code>Ctrl + Y</code></li>
</ul>
<h2 id="markdown及扩展">Markdown及扩展</h2>
<blockquote>
<p>Markdown 是一种轻量级标记语言，它允许人们使用易读易写的纯文本格式编写文档，然后转换成格式丰富的HTML页面。 —— <a href="https://zh.wikipedia.org/wiki/Markdown" rel="nofollow noopener noreferrer" target="_blank" data-token="3cae1ce17720abbfc25d886df4b8fb5f"> [ 维基百科 ]</a></p>
</blockquote>
<p>使用简单的符号标识不同的标题，将某些文字标记为<strong>粗体</strong>或者<em>斜体</em>，创建一个<a href="http://www.csdn.net" rel="nofollow" data-token="f7a4a76fce9ca485bf7119ec802761c0">链接</a>等，详细语法参考帮助？。</p>
<p>本编辑器支持 <strong>Markdown Extra</strong> , 　扩展了很多好用的功能。具体请参考<a href="https://github.com/jmcmanus/pagedown-extra" rel="nofollow" title="Pagedown Extra" data-token="943b08a5287d9db16adf7ab902849450">Github</a>. </p>
<h3 id="表格">表格</h3>
<p><strong>Markdown　Extra</strong>　表格语法：</p>
<table>
<thead>
<tr>
<th>项目</th>
<th>价格</th>
</tr>
</thead>
<tbody>
<tr>
<td>Computer</td>
<td>$1600</td>
</tr>
<tr>
<td>Phone</td>
<td>$12</td>
</tr>
<tr>
<td>Pipe</td>
<td>$1</td>
</tr>
</tbody>
</table>
<p>可以使用冒号来定义对齐方式：</p>
<table>
<thead>
<tr>
<th align="left">项目</th>
<th align="right">价格</th>
<th align="center">数量</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Computer</td>
<td align="right">1600 元</td>
<td align="center">5</td>
</tr>
<tr>
<td align="left">Phone</td>
<td align="right">12 元</td>
<td align="center">12</td>
</tr>
<tr>
<td align="left">Pipe</td>
<td align="right">1 元</td>
<td align="center">234</td>
</tr>
</tbody>
</table>
<h3 id="定义列表">定义列表</h3>
<dl>
<dt>
    <strong>Markdown　Extra</strong>　定义列表语法：
   </dt>
<dt>
    项目１
   </dt>
<dt>
    项目２
   </dt>
<dd>
    定义 A
   </dd>
<dd>
    定义 B
   </dd>
<dt>
    项目３
   </dt>
<dd>
    定义 C
   </dd>
<dd>
<p>定义 D</p>
<blockquote>
<p>定义D内容</p>
</blockquote>
</dd>
</dl>
<h3 id="代码块">代码块</h3>
<p>代码块语法遵循标准markdown代码，例如：</p>
<pre class="prettyprint"><code class="language-python hljs "><span class="hljs-decorator">@requires_authorization</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">somefunc</span><span class="hljs-params">(param1=<span class="hljs-string">''</span>, param2=<span class="hljs-number">0</span>)</span>:</span>
    <span class="hljs-string">'''A docstring'''</span>
    <span class="hljs-keyword">if</span> param1 &gt; param2: <span class="hljs-comment"># interesting</span>
        <span class="hljs-keyword">print</span> <span class="hljs-string">'Greater'</span>
    <span class="hljs-keyword">return</span> (param2 - param1 + <span class="hljs-number">1</span>) <span class="hljs-keyword">or</span> <span class="hljs-keyword">None</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">SomeClass</span>:</span>
    <span class="hljs-keyword">pass</span>
<span class="hljs-prompt">&gt;&gt;&gt; </span>message = <span class="hljs-string">'''interpreter <span class="hljs-prompt">... </span>prompt'''</span></code></pre>
<h3 id="脚注">脚注</h3>
<p>生成一个脚注<a href="#fn:footnote" rel="nofollow" id="fnref:footnote" title="See footnote" class="footnote" data-token="a310949006532b0be16eee02e6cd4565">1</a>.</p>
<h3 id="目录">目录</h3>
<p>用 <code>[TOC]</code>来生成目录：</p>
</p>
<div class="toc">
<ul>
<li>
<ul>
<li><a href="#快捷键" rel="nofollow" data-token="935a463c953218b86f28c66c8dfeb274">快捷键</a></li>
<li><a href="#markdown及扩展" rel="nofollow" data-token="4df4498a1fa946f9b9d145fb0677a2eb">Markdown及扩展</a>
<ul>
<li><a href="#表格" rel="nofollow" data-token="3f9e56bdcb7e065a4f6a78feec6af48c">表格</a></li>
<li><a href="#定义列表" rel="nofollow" data-token="51723e0db253f97fbc7d2ad95fbfe1f5">定义列表</a></li>
<li><a href="#代码块" rel="nofollow" data-token="1bb3a4e519889dae18c2fcc64db8344b">代码块</a></li>
<li><a href="#脚注" rel="nofollow" data-token="6d429fa68ed1b5332403fcfb3a9b0228">脚注</a></li>
<li><a href="#目录" rel="nofollow" data-token="2ad947a353a507a82620db37f77cc366">目录</a></li>
<li><a href="#数学公式" rel="nofollow" data-token="29c6cc337b254078443b1b1ab8ede01e">数学公式</a></li>
<li><a href="#uml-图" rel="nofollow" data-token="484a9695c6daa139455c60f22eee530f">UML 图:</a></li>
</ul>
</li>
<li><a href="#离线写博客" rel="nofollow" data-token="f6502c3a3bc0386d6d13febde92fdbb1">离线写博客</a></li>
<li><a href="#浏览器兼容" rel="nofollow" data-token="e7d9d7f136380752abb601d97df3a039">浏览器兼容</a></li>
</ul>
</li>
</ul></div>
</p>
<h3 id="数学公式">数学公式</h3>
<p>使用MathJax渲染<em>LaTex</em> 数学公式，详见<a href="http://math.stackexchange.com/" rel="nofollow" data-token="5e01cbfba8d506e32c625549f4be96a1">math.stackexchange.com</a>.</p>
<ul>
<li>行内公式，数学公式为：<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-3-Frame" tabindex="0" data-mathml="

<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi mathvariant=&quot;normal&quot;>&amp;#x0393;</mi><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo stretchy=&quot;false&quot;>)</mo><mo>=</mo><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo>&amp;#x2212;</mo><mn>1</mn><mo stretchy=&quot;false&quot;>)</mo><mo>!</mo><mspace width=&quot;1em&quot; /><mi mathvariant=&quot;normal&quot;>&amp;#x2200;</mi><mi>n</mi><mo>&amp;#x2208;</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi mathvariant=&quot;double-struck&quot;>N</mi></mrow></math>
<p>&#8221; role=&#8221;presentation&#8221; style=&#8221;position: relative;&#8221;><br />
     <nobr aria-hidden="true"><br />
      <span class="math" id="MathJax-Span-1" style="width: 13.003em; display: inline-block;"><span style="display: inline-block; position: relative; width: 10.836em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(1.558em 1010.84em 2.892em -999.997em); top: -2.442em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3" style="font-family: MathJax_Main;">Γ</span><span class="mo" id="MathJax-Span-4" style="font-family: MathJax_Main;">(</span><span class="mi" id="MathJax-Span-5" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-6" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-7" style="font-family: MathJax_Main; padding-left: 0.281em;">=</span><span class="mo" id="MathJax-Span-8" style="font-family: MathJax_Main; padding-left: 0.281em;">(</span><span class="mi" id="MathJax-Span-9" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-10" style="font-family: MathJax_Main; padding-left: 0.225em;">−</span><span class="mn" id="MathJax-Span-11" style="font-family: MathJax_Main; padding-left: 0.225em;">1</span><span class="mo" id="MathJax-Span-12" style="font-family: MathJax_Main;">)</span><span class="mo" id="MathJax-Span-13" style="font-family: MathJax_Main;">!</span><span class="mspace" id="MathJax-Span-14" style="height: 0em; vertical-align: 0em; width: 1.003em; display: inline-block; overflow: hidden;"></span><span class="mi" id="MathJax-Span-15" style="font-family: MathJax_Main;">∀</span><span class="mi" id="MathJax-Span-16" style="font-family: MathJax_Math-italic;">n</span><span class="mo" id="MathJax-Span-17" style="font-family: MathJax_Main; padding-left: 0.281em;">∈</span><span class="texatom" id="MathJax-Span-18" style="padding-left: 0.281em;"><span class="mrow" id="MathJax-Span-19"><span class="mi" id="MathJax-Span-20" style="font-family: MathJax_AMS;">N</span></span></span></span><span style="display: inline-block; width: 0px; height: 2.447em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.33em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 1.337em;"></span></span><br />
     </nobr><span class="MJX_Assistive_MathML" role="presentation"></p>
<math xmlns="http://www.w3.org/1998/Math/MathML">
       <mi mathvariant="normal"><br />
        Γ<br />
       </mi><br />
       <mo stretchy="false"><br />
        (<br />
       </mo><br />
       <mi><br />
        n<br />
       </mi><br />
       <mo stretchy="false"><br />
        )<br />
       </mo><br />
       <mo><br />
        =<br />
       </mo><br />
       <mo stretchy="false"><br />
        (<br />
       </mo><br />
       <mi><br />
        n<br />
       </mi><br />
       <mo><br />
        −<br />
       </mo><br />
       <mn><br />
        1<br />
       </mn><br />
       <mo stretchy="false"><br />
        )<br />
       </mo><br />
       <mo><br />
        !<br />
       </mo><br />
       <mspace width="1em"></mspace><br />
       <mi mathvariant="normal"><br />
        ∀<br />
       </mi><br />
       <mi><br />
        n<br />
       </mi><br />
       <mo><br />
        ∈<br />
       </mo><br />
       <mrow class="MJX-TeXAtom-ORD"><br />
        <mi mathvariant="double-struck"><br />
         N<br />
        </mi><br />
       </mrow><br />
      </math>
<p></span></span>。</li>
<li>块级公式：</li>
</ul>
<p><span class="MathJax_Preview" style="color: inherit; display: none;"></span></p>
<div class="MathJax_Display" style="text-align: center;">
   <span class="MathJax" id="MathJax-Element-4-Frame" tabindex="0" data-mathml="

<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>x</mi><mo>=</mo><mstyle displaystyle=&quot;true&quot; scriptlevel=&quot;0&quot;><mfrac><mrow><mo>&amp;#x2212;</mo><mi>b</mi><mo>&amp;#x00B1;</mo><msqrt><msup><mi>b</mi><mn>2</mn></msup><mo>&amp;#x2212;</mo><mn>4</mn><mi>a</mi><mi>c</mi></msqrt></mrow><mrow><mn>2</mn><mi>a</mi></mrow></mfrac></mstyle></math>
<p>&#8221; role=&#8221;presentation&#8221; style=&#8221;text-align: center; position: relative;&#8221;><br />
    <nobr aria-hidden="true"><br />
     <span class="math" id="MathJax-Span-21" style="width: 10.888em; display: inline-block;"><span style="display: inline-block; position: relative; width: 9.065em; height: 0px; font-size: 120%;"><span style="position: absolute; clip: rect(0.471em 1009.07em 2.971em -999.997em); top: -2.133em; left: 0em;"><span class="mrow" id="MathJax-Span-22"><span class="mi" id="MathJax-Span-23" style="font-family: MathJax_Math-italic;">x</span><span class="mo" id="MathJax-Span-24" style="font-family: MathJax_Main; padding-left: 0.263em;">=</span><span class="mstyle" id="MathJax-Span-25" style="padding-left: 0.263em;"><span class="mrow" id="MathJax-Span-26"><span class="mfrac" id="MathJax-Span-27"><span style="display: inline-block; position: relative; width: 6.93em; height: 0px; margin-right: 0.107em; margin-left: 0.107em;"><span style="position: absolute; clip: rect(3.023em 1006.77em 4.326em -999.997em); top: -4.685em; left: 50%; margin-left: -3.383em;"><span class="mrow" id="MathJax-Span-28"><span class="mo" id="MathJax-Span-29" style="font-family: MathJax_Main;">−</span><span class="mi" id="MathJax-Span-30" style="font-family: MathJax_Math-italic;">b</span><span class="mo" id="MathJax-Span-31" style="font-family: MathJax_Main; padding-left: 0.211em;">±</span><span class="msqrt" id="MathJax-Span-32" style="padding-left: 0.211em;"><span style="display: inline-block; position: relative; width: 4.378em; height: 0px;"><span style="position: absolute; clip: rect(3.076em 1003.49em 4.273em -999.997em); top: -4.008em; left: 0.836em;"><span class="mrow" id="MathJax-Span-33"><span class="msubsup" id="MathJax-Span-34"><span style="display: inline-block; position: relative; width: 0.836em; height: 0px;"><span style="position: absolute; clip: rect(3.18em 1000.42em 4.169em -999.997em); top: -4.008em; left: 0em;"><span class="mi" id="MathJax-Span-35" style="font-family: MathJax_Math-italic;">b</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; top: -4.32em; left: 0.419em;"><span class="mn" id="MathJax-Span-36" style="font-size: 70.7%; font-family: MathJax_Main;">2</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span><span class="mo" id="MathJax-Span-37" style="font-family: MathJax_Main; padding-left: 0.211em;">−</span><span class="mn" id="MathJax-Span-38" style="font-family: MathJax_Main; padding-left: 0.211em;">4</span><span class="mi" id="MathJax-Span-39" style="font-family: MathJax_Math-italic;">a</span><span class="mi" id="MathJax-Span-40" style="font-family: MathJax_Math-italic;">c</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.596em 1003.54em 3.961em -999.997em); top: -4.581em; left: 0.836em;"><span style="display: inline-block; position: relative; width: 3.544em; height: 0px;"><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: -0.102em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; font-family: MathJax_Main; top: -4.008em; left: 2.867em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.367em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 0.888em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.357em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 1.878em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="font-family: MathJax_Main; position: absolute; top: -4.008em; left: 2.346em;">−<span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.076em 1000.84em 4.378em -999.997em); top: -4.06em; left: 0em;"><span style="font-family: MathJax_Main;">√</span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(3.18em 1000.99em 4.169em -999.997em); top: -3.331em; left: 50%; margin-left: -0.518em;"><span class="mrow" id="MathJax-Span-41"><span class="mn" id="MathJax-Span-42" style="font-family: MathJax_Main;">2</span><span class="mi" id="MathJax-Span-43" style="font-family: MathJax_Math-italic;">a</span></span><span style="display: inline-block; width: 0px; height: 4.013em;"></span></span><span style="position: absolute; clip: rect(0.836em 1006.93em 1.201em -999.997em); top: -1.247em; left: 0em;"><span style="display: inline-block; overflow: hidden; vertical-align: 0em; border-top-width: 1.3px; border-top-style: solid; width: 6.93em; height: 0px;"></span><span style="display: inline-block; width: 0px; height: 1.044em;"></span></span></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.138em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.872em; border-left-width: 0px; border-left-style: solid; width: 0px; height: 2.816em;"></span></span><br />
    </nobr><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"></p>
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
      <mi><br />
       x<br />
      </mi><br />
      <mo><br />
       =<br />
      </mo><br />
      <mstyle displaystyle="true" scriptlevel="0"><br />
       <mfrac><br />
        <mrow><br />
         <mo><br />
          −<br />
         </mo><br />
         <mi><br />
          b<br />
         </mi><br />
         <mo><br />
          ±<br />
         </mo><br />
         <msqrt><br />
          <msup><br />
           <mi><br />
            b<br />
           </mi><br />
           <mn><br />
            2<br />
           </mn><br />
          </msup><br />
          <mo><br />
           −<br />
          </mo><br />
          <mn><br />
           4<br />
          </mn><br />
          <mi><br />
           a<br />
          </mi><br />
          <mi><br />
           c<br />
          </mi><br />
         </msqrt><br />
        </mrow><br />
        <mrow><br />
         <mn><br />
          2<br />
         </mn><br />
         <mi><br />
          a<br />
         </mi><br />
        </mrow><br />
       </mfrac><br />
      </mstyle><br />
     </math>
<p></span></span>
  </div>
</p>
<p>更多LaTex语法请参考 <a href="http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference" rel="nofollow" data-token="5994967b434141d58f356eb853e2f52d">这儿</a>.</p>
<h3 id="uml-图">UML 图:</h3>
<p>可以渲染序列图：</p>
<div class="sequence-diagram">
   <svg height="265" version="1.1" width="460" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style="overflow: hidden; position: relative; left: -0.75px;"><br />
    <desc style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
     Created with Raphaël 2.1.2<br />
    </desc><br />
    <defs style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);">
     <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
     <marker id="raphael-marker-endblock55-obj63" markerheight="5" markerwidth="5" orient="auto" refx="2.5" refy="2.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      <use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#raphael-marker-block" transform="rotate(180 2.5 2.5) scale(1,1)" stroke-width="1.0000" fill="#000" stroke="none" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></use><br />
     </marker><br />
     <marker id="raphael-marker-endblock55-obj69" markerheight="5" markerwidth="5" orient="auto" refx="2.5" refy="2.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      <use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#raphael-marker-block" transform="rotate(180 2.5 2.5) scale(1,1)" stroke-width="1.0000" fill="#000" stroke="none" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></use><br />
     </marker><br />
    </defs><br />
    <rect x="10" y="20" width="50" height="39" rx="0" ry="0" fill="none" stroke="#000000" stroke-width="2" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></rect><br />
    <rect x="20" y="30" width="30" height="19" rx="0" ry="0" fill="#ffffff" stroke="none" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></rect><br />
    <text x="35" y="39.5" text-anchor="middle" font-family="Andale Mono, monospace" font-size="16px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: middle; font-family: 'Andale Mono', monospace; font-size: 16px;"><br />
     <tspan dy="5.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      张三<br />
     </tspan><br />
    </text><br />
    <rect x="10" y="206" width="50" height="39" rx="0" ry="0" fill="none" stroke="#000000" stroke-width="2" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></rect><br />
    <rect x="20" y="216" width="30" height="19" rx="0" ry="0" fill="#ffffff" stroke="none" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></rect><br />
    <text x="35" y="225.5" text-anchor="middle" font-family="Andale Mono, monospace" font-size="16px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: middle; font-family: 'Andale Mono', monospace; font-size: 16px;"><br />
     <tspan dy="5.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      张三<br />
     </tspan><br />
    </text>
    <path fill="none" stroke="#000000" d="M35,59L35,206" stroke-width="2" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
    <rect x="195" y="20" width="50" height="39" rx="0" ry="0" fill="none" stroke="#000000" stroke-width="2" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></rect><br />
    <rect x="205" y="30" width="30" height="19" rx="0" ry="0" fill="#ffffff" stroke="none" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></rect><br />
    <text x="220" y="39.5" text-anchor="middle" font-family="Andale Mono, monospace" font-size="16px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: middle; font-family: 'Andale Mono', monospace; font-size: 16px;"><br />
     <tspan dy="5.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      李四<br />
     </tspan><br />
    </text><br />
    <rect x="195" y="206" width="50" height="39" rx="0" ry="0" fill="none" stroke="#000000" stroke-width="2" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></rect><br />
    <rect x="205" y="216" width="30" height="19" rx="0" ry="0" fill="#ffffff" stroke="none" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></rect><br />
    <text x="220" y="225.5" text-anchor="middle" font-family="Andale Mono, monospace" font-size="16px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: middle; font-family: 'Andale Mono', monospace; font-size: 16px;"><br />
     <tspan dy="5.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      李四<br />
     </tspan><br />
    </text>
    <path fill="none" stroke="#000000" d="M220,59L220,206" stroke-width="2" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
    <rect x="45.921875" y="74.5" width="163.125" height="19" rx="0" ry="0" fill="#ffffff" stroke="none" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></rect><br />
    <text x="127.5" y="84" text-anchor="middle" font-family="Andale Mono, monospace" font-size="16px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: middle; font-family: 'Andale Mono', monospace; font-size: 16px;"><br />
     <tspan dy="5.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      嘿，小四儿, 写博客了没?<br />
     </tspan><br />
    </text>
    <path fill="none" stroke="#000000" d="M35,98C35,98,185.4685080051422,98,215.00383037724532,98" stroke-width="2" marker-end="url(#raphael-marker-endblock55-obj63)" stroke-dasharray="0" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
    <rect x="240" y="118" width="145" height="29" rx="0" ry="0" fill="none" stroke="#000000" stroke-width="2" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></rect><br />
    <rect x="245" y="123" width="135" height="19" rx="0" ry="0" fill="#ffffff" stroke="none" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></rect><br />
    <text x="312.5" y="132.5" text-anchor="middle" font-family="Andale Mono, monospace" font-size="16px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: middle; font-family: 'Andale Mono', monospace; font-size: 16px;"><br />
     <tspan dy="5.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      李四愣了一下，说：<br />
     </tspan><br />
    </text><br />
    <rect x="45" y="162.5" width="165" height="19" rx="0" ry="0" fill="#ffffff" stroke="none" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></rect><br />
    <text x="127.5" y="172" text-anchor="middle" font-family="Andale Mono, monospace" font-size="16px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: middle; font-family: 'Andale Mono', monospace; font-size: 16px;"><br />
     <tspan dy="5.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      忙得吐血，哪有时间写。<br />
     </tspan><br />
    </text>
    <path fill="none" stroke="#000000" d="M220,186C220,186,69.53149199485779,186,39.99616962275468,186" stroke-width="2" marker-end="url(#raphael-marker-endblock55-obj69)" stroke-dasharray="6,2" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
   </svg>
  </div>
<p>或者流程图：</p>
<div class="flow-chart">
   <svg height="372.75" version="1.1" width="143.5" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" style="overflow: hidden; position: relative; left: -0.5px;"><br />
    <desc style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
     Created with Raphaël 2.1.2<br />
    </desc><br />
    <defs style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
     <marker id="raphael-marker-endblock33-obj78" markerheight="3" markerwidth="3" orient="auto" refx="1.5" refy="1.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      <use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#raphael-marker-block" transform="rotate(180 1.5 1.5) scale(0.6,0.6)" stroke-width="1.6667" fill="black" stroke="none" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></use><br />
     </marker><br />
     <marker id="raphael-marker-endblock33-obj79" markerheight="3" markerwidth="3" orient="auto" refx="1.5" refy="1.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      <use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#raphael-marker-block" transform="rotate(180 1.5 1.5) scale(0.6,0.6)" stroke-width="1.6667" fill="black" stroke="none" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></use><br />
     </marker><br />
     <marker id="raphael-marker-endblock33-obj80" markerheight="3" markerwidth="3" orient="auto" refx="1.5" refy="1.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      <use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#raphael-marker-block" transform="rotate(180 1.5 1.5) scale(0.6,0.6)" stroke-width="1.6667" fill="black" stroke="none" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></use><br />
     </marker><br />
     <marker id="raphael-marker-endblock33-obj82" markerheight="3" markerwidth="3" orient="auto" refx="1.5" refy="1.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      <use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#raphael-marker-block" transform="rotate(180 1.5 1.5) scale(0.6,0.6)" stroke-width="1.6667" fill="black" stroke="none" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></use><br />
     </marker><br />
    </defs><br />
    <rect x="0" y="0" width="50" height="39" rx="20" ry="20" fill="#ffffff" stroke="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);" stroke-width="2" class="flowchart" id="st" transform="matrix(1,0,0,1,35.25,13.75)"></rect><br />
    <text x="10" y="19.5" text-anchor="start" font-family="sans-serif" font-size="14px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: sans-serif; font-size: 14px; font-weight: normal;" id="stt" class="flowchartt" font-weight="normal" transform="matrix(1,0,0,1,35.25,13.75)"><br />
     <tspan dy="5.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      开始<br />
     </tspan><br />
    </text><br />
    <rect x="0" y="0" width="80" height="39" rx="0" ry="0" fill="#ffffff" stroke="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);" stroke-width="2" class="flowchart" id="op" transform="matrix(1,0,0,1,20.25,116.5)"></rect><br />
    <text x="10" y="19.5" text-anchor="start" font-family="sans-serif" font-size="14px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: sans-serif; font-size: 14px; font-weight: normal;" id="opt" class="flowchartt" font-weight="normal" transform="matrix(1,0,0,1,20.25,116.5)"><br />
     <tspan dy="5.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      我的操作<br />
     </tspan><br />
    </text>
    <path fill="#ffffff" stroke="#000000" d="M28.125,14.625L0,29.25L56.25,58.5L112.5,29.25L56.25,0L0,29.25" stroke-width="2" font-family="sans-serif" font-weight="normal" id="cond" class="flowchart" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); font-family: sans-serif; font-weight: normal;" transform="matrix(1,0,0,1,4,209.5)"></path>
    <text x="33.125" y="29.25" text-anchor="start" font-family="sans-serif" font-size="14px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: sans-serif; font-size: 14px; font-weight: normal;" id="condt" class="flowchartt" font-weight="normal" transform="matrix(1,0,0,1,4,209.5)"><br />
     <tspan dy="5.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      确认？<br />
     </tspan><br />
     <tspan dy="18" x="33.125" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></tspan><br />
    </text><br />
    <rect x="0" y="0" width="50" height="39" rx="20" ry="20" fill="#ffffff" stroke="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);" stroke-width="2" class="flowchart" id="e" transform="matrix(1,0,0,1,35.25,331.75)"></rect><br />
    <text x="10" y="19.5" text-anchor="start" font-family="sans-serif" font-size="14px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: sans-serif; font-size: 14px; font-weight: normal;" id="et" class="flowchartt" font-weight="normal" transform="matrix(1,0,0,1,35.25,331.75)"><br />
     <tspan dy="5.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      结束<br />
     </tspan><br />
    </text>
    <path fill="none" stroke="#000000" d="M60.25,52.75C60.25,52.75,60.25,100.8800451606512,60.25,113.50008141615399" stroke-width="2" marker-end="url(#raphael-marker-endblock33-obj78)" font-family="sans-serif" font-weight="normal" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); font-family: sans-serif; font-weight: normal;"></path>
    <path fill="none" stroke="#000000" d="M60.25,155.5C60.25,155.5,60.25,195.15409994125366,60.25,206.50043908460066" stroke-width="2" marker-end="url(#raphael-marker-endblock33-obj79)" font-family="sans-serif" font-weight="normal" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); font-family: sans-serif; font-weight: normal;"></path>
    <path fill="none" stroke="#000000" d="M60.25,268C60.25,268,60.25,316.1300451606512,60.25,328.750081416154" stroke-width="2" marker-end="url(#raphael-marker-endblock33-obj80)" font-family="sans-serif" font-weight="normal" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); font-family: sans-serif; font-weight: normal;"></path>
    <text x="65.25" y="278" text-anchor="start" font-family="sans-serif" font-size="14px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: sans-serif; font-size: 14px; font-weight: normal;" font-weight="normal"><br />
     <tspan dy="5.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      yes<br />
     </tspan><br />
    </text>
    <path fill="none" stroke="#000000" d="M116.5,238.75C116.5,238.75,141.5,238.75,141.5,238.75C141.5,238.75,141.5,91.5,141.5,91.5C141.5,91.5,60.25,91.5,60.25,91.5C60.25,91.5,60.25,106.87344455718994,60.25,113.50924777425826" stroke-width="2" marker-end="url(#raphael-marker-endblock33-obj82)" font-family="sans-serif" font-weight="normal" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); font-family: sans-serif; font-weight: normal;"></path>
    <text x="121.5" y="228.75" text-anchor="start" font-family="sans-serif" font-size="14px" stroke="none" fill="#000000" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0); text-anchor: start; font-family: sans-serif; font-size: 14px; font-weight: normal;" font-weight="normal"><br />
     <tspan dy="5.5" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"><br />
      no<br />
     </tspan><br />
    </text><br />
   </svg>
  </div>
<ul>
<li>关于 <strong>序列图</strong> 语法，参考 <a href="http://bramp.github.io/js-sequence-diagrams/" rel="nofollow" data-token="eac17beea2dbdffaeccf909356a7eb9e">这儿</a>,</li>
<li>关于 <strong>流程图</strong> 语法，参考 <a href="http://adrai.github.io/flowchart.js/" rel="nofollow" data-token="977a4d98abbec7628576e159d2f3f610">这儿</a>.</li>
</ul>
<h2 id="离线写博客">离线写博客</h2>
<p>即使用户在没有网络的情况下，也可以通过本编辑器离线写博客（直接在曾经使用过的浏览器中输入<a href="http://write.blog.csdn.net/mdeditor" rel="nofollow" data-token="9bc0e3753f7ad7ebda7e1eccf9ec7a9c">write.blog.csdn.net/mdeditor</a>即可。<strong>Markdown编辑器</strong>使用浏览器离线存储将内容保存在本地。</p>
<p>用户写博客的过程中，内容实时保存在浏览器缓存中，在用户关闭浏览器或者其它异常情况下，内容不会丢失。用户再次打开浏览器时，会显示上次用户正在编辑的没有发表的内容。</p>
<p>博客发表后，本地缓存将被删除。　</p>
<p>用户可以选择 <i class="icon-disk"></i> 把正在写的博客保存到服务器草稿箱，即使换浏览器或者清除缓存，内容也不会丢失。</p>
<blockquote>
<p><strong>注意：</strong>虽然浏览器存储大部分时候都比较可靠，但为了您的数据安全，在联网后，<strong>请务必及时发表或者保存到服务器草稿箱</strong>。</p>
</blockquote>
<h2 id="浏览器兼容">浏览器兼容</h2>
<ol>
<li>目前，本编辑器对Chrome浏览器支持最为完整。建议大家使用较新版本的Chrome。</li>
<li>IE９以下不支持</li>
<li>IE９，１０，１１存在以下问题 <br /> 
<ol>
<li>不支持离线功能</li>
<li>IE9不支持文件导入导出</li>
<li>IE10不支持拖拽文件导入</li>
</ol>
</li>
</ol>
<hr>
<div class="footnotes">
<hr>
<ol>
<li id="fn:footnote">这里是 <strong>脚注</strong> 的 <em>内容</em>. <a href="#fnref:footnote" rel="nofollow" title="Return to article" class="reversefootnote" data-token="f29056f78b903bb800334f3bcad606e5"><img src="https://s.w.org/images/core/emoji/12.0.0-1/72x72/21a9.png" alt="↩" class="wp-smiley" style="height: 1em; max-height: 1em;" /></a></li>
</ol></div>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>深度学习之图像修复</title>
		<link>https://uzzz.org/article/2968.html</link>
				<pubDate>Sun, 19 Mar 2017 10:25:44 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[论文]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2968.html</guid>
				<description><![CDATA[图像修复问题就是还原图像中缺失的部分。基于图像中已有信息，去还原图像中的缺失部分。 从直观上看，这个问题能否解决是看情况的，还原的关键在于剩余信息的使用，剩余信息中如果存在有缺失部分信息的patch，那么剩下的问题就是从剩余信息中判断缺失部分与哪一部分相似。而这，就是现在比较流行的PatchMatch的基本思想。 CNN出现以来，有若干比较重要的进展： 被证]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<p>图像修复问题就是还原图像中缺失的部分。基于图像中已有信息，去还原图像中的缺失部分。</p>
<p><img src="https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blog2016-september-later/image_inpainting/1.png" alt="" title=""></p>
<p>从直观上看，这个问题能否解决是看情况的，还原的关键在于剩余信息的使用，剩余信息中如果存在有缺失部分信息的patch，那么剩下的问题就是从剩余信息中判断缺失部分与哪一部分相似。而这，就是现在比较流行的PatchMatch的基本思想。</p>
<p>CNN出现以来，有若干比较重要的进展：</p>
<ul>
<li>被证明有能力在CNN的高层捕捉到图像的抽象信息。</li>
<li><a href="http://blog.csdn.net/stdcoutzyx/article/details/54025243" rel="nofollow" data-token="c9c77842f2e17ef3bed4c39da541582a">Perceptual Loss</a>的出现证明了一个训练好的CNN网络的feature map可以很好的作为图像生成中的损失函数的辅助工具。</li>
<li><a href="http://blog.csdn.net/stdcoutzyx/article/details/53151038" rel="nofollow" data-token="d62a7bbc22190dc912641374e65739d6">GAN</a>可以利用监督学习来强化生成网络的效果。其效果的原因虽然还不具可解释性，但是可以理解为可以以一种不直接的方式使生成网络学习到规律。</li>
</ul>
<p>基于上述三个进展，参考文献[1]提出了一种基于CNN的图像复原方法。</p>
<h1 id="cnn网络结构">CNN网络结构</h1>
<p><img src="https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blog2016-september-later/image_inpainting/2.png" alt="" title=""></p>
<p>该算法需要使用两个网络，一个是内容生成网络，另一个是纹理生成网络。内容生成网络直接用于生成图像，推断缺失部分可能的内容。纹理生成网络用于增强内容网络的产出的纹理，具体则为将生成的补全图像和原始无缺失图像输入进纹理生成网络，在某一层feature_map上计算损失，记为Loss NN。</p>
<p>内容生成网络需要使用自己的数据进行训练，而纹理生成网络则使用已经训练好的VGG Net。这样，生成图像可以分为如下几个步骤：</p>
<p>定义缺失了某个部分的图像为x0</p>
<ul>
<li>x0输入进内容生成网络得到生成图片x</li>
<li>x作为最后生成图像的初始值</li>
<li>保持纹理生成网络的参数不变，使用Loss NN对x进行梯度下降，得到最后的结果。</li>
</ul>
<p>关于内容生成网络的训练和Loss NN的定义，下面会一一解释</p>
<h2 id="内容生成网络">内容生成网络</h2>
<p><img src="https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blog2016-september-later/image_inpainting/3.png" alt="" title=""></p>
<p>生成网络结构如上，其损失函数使用了L2损失和对抗损失的组合。所谓的对抗损失是来源于<a href="http://blog.csdn.net/stdcoutzyx/article/details/53151038" rel="nofollow" data-token="d62a7bbc22190dc912641374e65739d6">对抗神经网络</a>.</p>
<p><img src="https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blog2016-september-later/image_inpainting/8.png" alt="" title=""></p>
<p><img src="https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blog2016-september-later/image_inpainting/9.png" alt="" title=""></p>
<p><img src="https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blog2016-september-later/image_inpainting/10.png" alt="" title=""></p>
<p>在该生成网络中，为了是训练稳定，做了两个改变：</p>
<ul>
<li>将所有的ReLU/leaky-ReLU都替换为ELU层</li>
<li>使用fully-connected layer替代chnnel-wise的全连接网络。</li>
</ul>
<h2 id="纹理生成网络">纹理生成网络</h2>
<p>纹理生成网络的Loss NN如下：</p>
<p><img src="https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blog2016-september-later/image_inpainting/11.png" alt="" title=""></p>
<p>它分为三个部分，即Pixel-wise的欧式距离，基于已训练好纹理网络的feature layer的perceptual loss，和用于平滑的TV Loss。</p>
<p>α和β都是5e<sup>-6</sup>，</p>
<p>Pixel-wise的欧氏距离如下：</p>
<p><img src="https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blog2016-september-later/image_inpainting/12.png" alt="" title=""></p>
<p>TV Loss如下：</p>
<p><img src="https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blog2016-september-later/image_inpainting/15.png" alt="" title=""></p>
<p>Perceptual Loss的计算比较复杂，这里利用了PatchMatch的信息，即为缺失部分找到最近似的Patch，为了达到这一点，将缺失部分分为很多个固定大小的patch作为query，也将已有的部分分为同样固定大小的patch，生成dataset PATCHES，在匹配query和PATCHES中最近patch的时候，需要在纹理生成网络中的某个layer的激活值上计算距离而不是计算像素距离。</p>
<p>但是，寻找最近邻Patch这个操作似乎是不可计算导数的，如何破解这一点呢？同<a href="http://blog.csdn.net/stdcoutzyx/article/details/54173846" rel="nofollow" data-token="b1d87766642fc6214d33b4ab99e25a8d">MRF+CNN</a>类似，在这里，先将PATCHES中的各个patch的的feature_map抽取出来，将其组合成为一个新的卷积层，然后得到query的feature map后输入到这个卷积层中，最相似的patch将获得最大的激活值，所以将其再输入到一个max-pooling层中，得到这个最大值。这样，就可以反向传播了。</p>
<h2 id="高清图像上的应用">高清图像上的应用</h2>
<p>本算法直接应用到高清图像上时效果并不好，所以，为了更好的初始化，使用了Stack迭代算法。即先将高清图像down-scale到若干级别[1,2,3,…,S]，其中S级别为原图本身，然后在级别1上使用图像均值初始化缺失部分，得到修复后的结果，再用这个结果，初始化下一级别的输入。以此类推。</p>
<p><img src="https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blog2016-september-later/image_inpainting/7.png" alt="" title=""></p>
<h1 id="效果">效果</h1>
<p><img src="https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blog2016-september-later/image_inpainting/5.png" alt="" title=""></p>
<p>上图从上往下一次为，有缺失的原图，PatchMatch算法，Context Decoder算法（GAN+L2)和本算法。</p>
<h2 id="内容生成网络的作用">内容生成网络的作用</h2>
<p><img src="https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blog2016-september-later/image_inpainting/4.png" alt="" title=""></p>
<p>起到了内容限制的作用，上图比较了有内容生成网络和没有内容生成网络的区别，有的可以在内容上更加符合原图。</p>
<h2 id="应用">应用</h2>
<p>图像的语义编辑，从左到右依次为原图，扣掉某部分的原图，PatchMatch结果，和本算法结果。</p>
<p><img src="https://raw.githubusercontent.com/stdcoutzyx/Blogs/master/blog2016-september-later/image_inpainting/6.png" alt="" title=""></p>
<p>可知，该方法虽然不可以复原真实的图像，但却可以补全成一张完整的图像。这样，当拍照中有不想干的物体或人进入到摄像头中时，依然可以将照片修复成一张完整的照片。</p>
<h1 id="总结">总结</h1>
<p>CNN的大发展，图像越来越能够变得语义化了。有了以上的图像复原的基础，尽可以进行发挥自己的想象，譬如：在图像上加一个东西，但是光照和颜色等缺明显不搭，可以用纹理网络进行修复。</p>
<p>该方法的缺点也是很明显：</p>
<ul>
<li>性能和内存问题</li>
<li>只用了图片内的patch，而没有用到整个数据集中的数据。</li>
</ul>
<h2 id="参考文献">参考文献</h2>
<p>[1]. Yang C, Lu X, Lin Z, et al. High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis[J]. arXiv preprint arXiv:1611.09969, 2016.</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-526ced5128.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>暗黑破坏神 2 私服 sf 114.215.178.67</title>
		<link>https://uzzz.org/article/3196.html</link>
				<pubDate>Wed, 09 Mar 2016 06:22:55 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[论文]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/3196.html</guid>
				<description><![CDATA[注册表 REGEDIT4 [HKEY_CURRENT_USER\Software\Blizzard Entertainment\Diablo II] &#8220;BNETIP&#8221;=&#8221;114.215.178.67&#8221; 1.11b 原版 QQ群：487460007]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
<p><span style="color:rgb(51,51,51);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:14px;line-height:24px;">注册表</span></p>
<p><span style="color:rgb(51,51,51);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:14px;line-height:24px;">REGEDIT4</span></p>
<p>  <span style="color:rgb(51,51,51);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:14px;line-height:24px;">[HKEY_CURRENT_USER\Software\Blizzard Entertainment\Diablo II]</span><br />
  <br style="color:rgb(51,51,51);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:14px;line-height:24px;"></p>
<p><span style="color:rgb(51,51,51);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:14px;line-height:24px;">&#8220;BNETIP&#8221;=&#8221;114.215.178.67&#8221;</span></p>
<p></p>
<p><span style="color:rgb(51,51,51);font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;font-size:14px;line-height:24px;">1.11b 原版</span></p>
<p><span style="font-size:18px;color:#3333ff;">QQ群：487460007</span></p>
</p></div>
</div>
]]></content:encoded>
										</item>
	</channel>
</rss>
