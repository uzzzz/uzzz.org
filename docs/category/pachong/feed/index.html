<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>爬虫 &#8211; 有组织在!</title>
	<atom:link href="https://uzzz.org/category/pachong/feed" rel="self" type="application/rss+xml" />
	<link>https://uzzz.org/</link>
	<description></description>
	<lastBuildDate>Tue, 22 Jan 2019 01:36:24 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.4</generator>

<image>
	<url>https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png</url>
	<title>爬虫 &#8211; 有组织在!</title>
	<link>https://uzzz.org/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>awesome-spider</title>
		<link>https://uzzz.org/article/2114.html</link>
				<pubDate>Tue, 22 Jan 2019 01:36:24 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[爬虫]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2114.html</guid>
				<description><![CDATA[awesome-spider 收集各种爬虫 （默认爬虫语言为 python）, 欢迎大家 提 pr 或 issue, 收集脚本见此项目&#160;github-search A 暗网爬虫(Go) 爱丝APP图片爬虫 B Bilibili 用户 Bilibili 视频 Bilibili 小视频 Bing美图爬虫 B站760万视频信息爬虫 博客园(node.js) 百度百科(node.js) 北邮人水木清华招聘 百度云网盘 琉璃神社爬虫 Boss 直聘 C cnblog caoliu 1024 D 豆瓣读书 豆瓣爬虫集 豆瓣害羞组 豆瓣图书广度爬取 DNS记录和子域名 DHT网络磁力种子爬虫 抖音 E E绅士 G Girl-atlas girl13 github trending Github 仓库及用户分析爬虫 H HDOJ爬虫 I Instagram INC500 世界5000强爬虫 J 京东 京东搜索+评论 京东商品+评论 机票 煎蛋妹纸 煎蛋妹纸selenium版本 今日头条，网易，腾讯等新闻 计算机书籍控图书 K 看知乎 课程格子校花榜 konachan L 链家 链家成交在售在租房源 拉勾 炉石传说 leetcode 领英销售导航器爬虫 LinkedInSalesNavigator M 马蜂窝 用户足迹 MyCar 漫画喵 一键下载漫画~ MM131性感美女写真图全爬取 美女写真套图爬虫&#160;（一）（二）（三） 妹子图 猫眼网电影评分 N 新闻监控 你好污啊 O ofo共享单车爬虫 P Pixiv PornHub packtpub 91porn Q QQ空间 QQ 群 清华大学网络学堂爬虫 去哪儿 前程无忧Python招聘岗位信息爬取分析 R 人人影视 RSS 爬虫 rosi 妹子图 reddit 壁纸 reddit S soundcloud Stackoverflow 100万问答爬虫 Shadowsocks 账号爬虫 spider163 网易云音乐爬虫 时光网电影数据和海报爬虫 T tumblr 下载tumblr喜欢内容 TuShare 天猫双12爬虫 Taobao mm Tmall 女性文胸尺码爬虫 淘宝直播弹幕爬虫(node) 天涯论坛文章 V Youtube字幕下载 视频信息爬虫 电影网站 W 乌云公开漏洞 微信公众号 “代理”方式抓取微信公众号文章 网易新闻 网易精彩评论 微博主题搜索分析 网易云音乐 新.网易热评 唯品会商品 X 雪球股票信息(java) 新浪微博 新浪微博分布式爬虫 心灵毒鸡汤 Y 英美剧 TV (node.js) Z ZOL 手机壁纸爬虫 知乎(python) 知乎(php) 知网 知乎妹子 自如实时房源提醒 # 80s 影视资源爬虫 &#8211; JianSo_Movie 其他 各种爬虫 DHT 爬虫 SimDHT p2pspider]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
<h1>awesome-spider</h1>
<p>收集各种爬虫 （默认爬虫语言为 python）, 欢迎大家 提 pr 或 issue, 收集脚本见此项目&nbsp;<a href="https://github.com/facert/github_search" rel="nofollow" data-token="403e2fe2d3500a9d66668286eabf533c">github-search</a></p>
<h3>A</h3>
<ul>
<li><a href="https://github.com/s-rah/onionscan" rel="nofollow" data-token="81875177737c98c70a429b7d89cfc554">暗网爬虫(Go)</a></li>
<li><a href="https://github.com/x-spiders/aiss-spider" rel="nofollow" data-token="44fb0ca4cfa793351d6d77cade72c144">爱丝APP图片爬虫</a></li>
</ul>
<h3>B</h3>
<ul>
<li><a href="https://github.com/airingursb/bilibili-user" rel="nofollow" data-token="f77f159913f4d2356ab54feb3f2c7683">Bilibili 用户</a></li>
<li><a href="https://github.com/airingursb/bilibili-video" rel="nofollow" data-token="dd15b52c7e8fd4b9dfb1cb482462600f">Bilibili 视频</a></li>
<li><a href="https://github.com/AngelKitty/bilibili-smallvideo" rel="nofollow" data-token="65339cf894cb78e166a5b30ce2dc11df">Bilibili 小视频</a></li>
<li><a href="https://github.com/zhangzp9970/GnomeBingLockScreen" rel="nofollow" data-token="ac03d92d02710d551c13c3906fe25b9f">Bing美图爬虫</a></li>
<li><a href="https://github.com/chenjiandongx/bili-spider" rel="nofollow" data-token="e688545473e9a926cf9ebaafffc77b89">B站760万视频信息爬虫</a></li>
<li><a href="https://github.com/chokcoco/cnblogSpider" rel="nofollow" data-token="a6ac01411aca16de155d2dcabb8aa6cf">博客园(node.js)</a></li>
<li><a href="https://github.com/nswbmw/micro-scraper" rel="nofollow" data-token="a1d51804bd3da1150a4b30bd24a38aea">百度百科(node.js)</a></li>
<li><a href="https://github.com/Marcus-T/Crawler_Job" rel="nofollow" data-token="05bf80c17e301e774e8f10179c337354">北邮人水木清华招聘</a></li>
<li><a href="https://github.com/gudegg/yunSpider" rel="nofollow" data-token="3e587e44f49b78292380e5aaa12edcfc">百度云网盘</a></li>
<li><a href="https://github.com/Chion82/hello-old-driver" rel="nofollow" data-token="3d4d3aa4495bcf79b8acf9a0a09dc9ab">琉璃神社爬虫</a></li>
<li><a href="https://github.com/xianyunyh/spider_job" rel="nofollow" data-token="af836bb1029a1caf7d73e2d4da4c5d21">Boss 直聘</a></li>
</ul>
<h3>C</h3>
<ul>
<li><a href="https://github.com/jackgitgz/CnblogsSpider" rel="nofollow" data-token="8d00e6393fb043f46019040aeb03f23e">cnblog</a></li>
<li><a href="https://github.com/LintBin/1024crawer" rel="nofollow" data-token="5601aee81b1b49bde364b68118ca9242">caoliu 1024</a></li>
</ul>
<h3>D</h3>
<ul>
<li><a href="https://github.com/lanbing510/DouBanSpider" rel="nofollow" data-token="a3835370c857e348db3e890546f7268d">豆瓣读书</a></li>
<li><a href="https://github.com/dontcontactme/doubanspiders" rel="nofollow" data-token="91f22bef3aa57b6d12fa632422e13686">豆瓣爬虫集</a></li>
<li><a href="https://github.com/rockdai/haixiu" rel="nofollow" data-token="dac6a904c99167ad9613e7ba2045442e">豆瓣害羞组</a></li>
<li><a href="https://github.com/CasterWx/java-Crawler/tree/master/src/%E7%88%AC%E8%99%AB/%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90/%E8%B1%86%E7%93%A3%E5%9B%BE%E4%B9%A6%E6%B7%B1%E5%BA%A6%E7%88%AC%E5%8F%96" rel="nofollow" data-token="b7884b9b705c0b4e8c1e0d266f5f5918">豆瓣图书广度爬取</a></li>
<li><a href="https://github.com/TheRook/subbrute" rel="nofollow" data-token="36f63d548a9183343602b558444312d9">DNS记录和子域名</a></li>
<li><a href="https://github.com/chenjiandongx/magnet-dht" rel="nofollow" data-token="19cf7c8387fac5e4a84e3625c579e25c">DHT网络磁力种子爬虫</a></li>
<li><a href="https://github.com/Python3WebSpider/DouYin" rel="nofollow" data-token="2bfc572cf73814fcbe7b9cde1db52a32">抖音</a></li>
</ul>
<h3>E</h3>
<ul>
<li><a href="https://github.com/shuiqukeyou/E-HentaiCrawler" rel="nofollow" data-token="55ee68b67f74fb145f28160c1b4e446a">E绅士</a></li>
</ul>
<h3>G</h3>
<ul>
<li><a href="https://github.com/pein0119/girl-atlas-crawler" rel="nofollow" data-token="96ee9e7de14fc89c0a12484be37a1224">Girl-atlas</a></li>
<li><a href="https://github.com/xuelangcxy/girlCrawler" rel="nofollow" data-token="3a74ede338b355edb7e9dfe847571072">girl13</a></li>
<li><a href="https://github.com/bonfy/github-trending" rel="nofollow" data-token="1f9c9186c62fc8b92c65c00e56218160">github trending</a></li>
<li><a href="https://github.com/chenjiandongx/Github" rel="nofollow" data-token="c392f969d7b507a6e7c60f40d3fe7213">Github 仓库及用户分析爬虫</a></li>
</ul>
<h3>H</h3>
<ul>
<li><a href="https://github.com/stevenshuang/spider/tree/master/hdoj" rel="nofollow" data-token="c46e18e9e80817f7b239a913429114eb">HDOJ爬虫</a></li>
</ul>
<h3>I</h3>
<ul>
<li><a href="https://github.com/xTEddie/Scrapstagram" rel="nofollow" data-token="3815f383d11fdcfce2b9a62a0eecac50">Instagram</a></li>
<li><a href="https://github.com/XetRAHF/Scrapping-INC500" rel="nofollow" data-token="f3e94541d7c317858e3dd65466451cbc">INC500 世界5000强爬虫</a></li>
</ul>
<h3>J</h3>
<ul>
<li><a href="https://github.com/taizilongxu/scrapy_jingdong" rel="nofollow" data-token="b2e1ce15a3f1934c010a130c597e8d8c">京东</a></li>
<li><a href="https://github.com/Chyroc/JDong" rel="nofollow" data-token="3e9ce0b47bcd852ecb8836c5098260c2">京东搜索+评论</a></li>
<li><a href="https://github.com/samrayleung/jd_spider" rel="nofollow" data-token="cd7b7a894f7e78ad9e53a41868931173">京东商品+评论</a></li>
<li><a href="https://github.com/fankcoder/findtrip" rel="nofollow" data-token="910394b6f5f6865ef35f56a3be75d3fa">机票</a></li>
<li><a href="https://github.com/kulovecc/jandan_spider" rel="nofollow" data-token="e1f2895bce6f1ae1233fb214300422ca">煎蛋妹纸</a></li>
<li><a href="https://github.com/Tony-Chiong/jandan_spider" rel="nofollow" data-token="3f8a67c33ce31b60e73c7fdcb89f1415">煎蛋妹纸selenium版本</a></li>
<li><a href="https://github.com/lzjqsdd/NewsSpider" rel="nofollow" data-token="c537767382ddf389588d886b6938749b">今日头条，网易，腾讯等新闻</a></li>
<li><a href="https://github.com/CasterWx/java-Crawler/tree/master/src/%E7%88%AC%E8%99%AB/%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90/Book" rel="nofollow" data-token="1f46235678813e7ad5303e5821b9debe">计算机书籍控图书</a></li>
</ul>
<h3>K</h3>
<ul>
<li><a href="https://github.com/atonasting/zhihuspider" rel="nofollow" data-token="12164ac4aeca2c9502cdd5ea770d52dd">看知乎</a></li>
<li><a href="https://github.com/xinqiu/kechenggezi-Spider" rel="nofollow" data-token="2dc38c37da9def88a098318100fd2d9f">课程格子校花榜</a></li>
<li><a href="https://github.com/wudaown/konachanDL" rel="nofollow" data-token="52c6929f273cfd99053fa75eae070708">konachan</a></li>
</ul>
<h3>L</h3>
<ul>
<li><a href="https://github.com/lanbing510/LianJiaSpider" rel="nofollow" data-token="65f10a6b8ca8758adb57b5084b17be8e">链家</a></li>
<li><a href="https://github.com/XuefengHuang/lianjia-scrawler" rel="nofollow" data-token="74b739b27df454321237b09193cb4803">链家成交在售在租房源</a></li>
<li><a href="https://github.com/GuozhuHe/webspider" rel="nofollow" data-token="bf62e787f7f5363124e9b6d4e7064556">拉勾</a></li>
<li><a href="https://github.com/youfou/hsdata" rel="nofollow" data-token="72bc92453c96bf1d0a8900281ee39845">炉石传说</a></li>
<li><a href="https://github.com/bonfy/leetcode" rel="nofollow" data-token="257908508372388aa4710f7fd8e1aa34">leetcode</a></li>
<li><a href="https://github.com/XetRAHF/Spider_LinkedInSalesNavigatorURL" rel="nofollow" data-token="8bb7d23a1ef31d571220ea12dddbd1ea">领英销售导航器爬虫 LinkedInSalesNavigator</a></li>
</ul>
<h3>M</h3>
<ul>
<li><a href="https://github.com/eternal-flame-AD/mafengwo" rel="nofollow" data-token="aac2214ed05d869a54934e7c89197ce2">马蜂窝 用户足迹</a></li>
<li><a href="https://github.com/Thoxvi/MyCar_python" rel="nofollow" data-token="f87823f4b0cf834858f80e11e98990d9">MyCar</a></li>
<li><a href="https://github.com/miaoerduo/cartoon-cat" rel="nofollow" data-token="61f767465cc5fcd13413dba2314718d2">漫画喵 一键下载漫画~</a></li>
<li><a href="https://github.com/qwertyuiop6/mm131" rel="nofollow" data-token="9b76357fd105bd8e8813284bc9c5625f">MM131性感美女写真图全爬取</a></li>
<li>美女写真套图爬虫&nbsp;<a href="https://github.com/chenjiandongx/mmjpg" rel="nofollow" data-token="590c5557e7e3eb2e0af4280ae5dd5c09">（一）</a><a href="https://github.com/chenjiandongx/mzitu" rel="nofollow" data-token="72711c798d87d16f56b8ad9bfb4a1620">（二）</a><a href="https://github.com/chenjiandongx/photo-gevent" rel="nofollow" data-token="7b0f9c5ec41e56c4bab7228693d08f19">（三）</a></li>
<li><a href="https://github.com/3inchtime/mmjpg_spider" rel="nofollow" data-token="3c49b0dfb4fd650ee606a08616af0efa">妹子图</a></li>
<li><a href="https://github.com/CasterWx/python-maoyan-spider" rel="nofollow" data-token="fa84c875ad0a740ad5c611164107a7be">猫眼网电影评分</a></li>
</ul>
<h3>N</h3>
<ul>
<li><a href="https://github.com/NolanZhao/news_feed" rel="nofollow" data-token="a07867792722250259cfe8c689f77d3e">新闻监控</a></li>
<li><a href="https://github.com/sy-records/speech_spiders/tree/master/nihaowu" rel="nofollow" data-token="8e5c32b1f0ad2e301c0ac1295be8076e">你好污啊</a></li>
</ul>
<h3>O</h3>
<ul>
<li><a href="https://github.com/SilverBooker/ofoSpider" rel="nofollow" data-token="db1d56812bc0126fc806ec3aff6165ee">ofo共享单车爬虫</a></li>
</ul>
<h3>P</h3>
<ul>
<li><a href="https://github.com/littleVege/pixiv_crawl" rel="nofollow" data-token="de58baa5be05c915c01ab235587854ba">Pixiv</a></li>
<li><a href="https://github.com/xiyouMc/WebHubBot" rel="nofollow" data-token="5fc5a5be4d7161ad795fbee96060e53b">PornHub</a></li>
<li><a href="https://github.com/niqdev/packtpub-crawler" rel="nofollow" data-token="08392030bd41b87469c50ef01134e909">packtpub</a></li>
<li><a href="https://github.com/eqblog/91_porn_spider" rel="nofollow" data-token="d6577d52cffc41b26da0d0b61c643fa0">91porn</a></li>
</ul>
<h3>Q</h3>
<ul>
<li><a href="https://github.com/LiuXingMing/QQSpider" rel="nofollow" data-token="8b8fe7216d0a10502718724880724292">QQ空间</a></li>
<li><a href="https://github.com/caspartse/QQ-Groups-Spider" rel="nofollow" data-token="d05f33b5092f1f3b844bca95b78d37dd">QQ 群</a></li>
<li><a href="https://github.com/kehao95/thu_learn" rel="nofollow" data-token="0101c9089e3b45a611f0eb2853738d76">清华大学网络学堂爬虫</a></li>
<li><a href="https://github.com/lining0806/QunarSpider" rel="nofollow" data-token="f224110a273e8ad6be51caa07904cbf8">去哪儿</a></li>
<li><a href="https://github.com/chenjiandongx/51job" rel="nofollow" data-token="ca352062d095bc4ae0a124f2c264207a">前程无忧Python招聘岗位信息爬取分析</a></li>
</ul>
<h3>R</h3>
<ul>
<li><a href="https://github.com/gnehsoah/yyets-spider" rel="nofollow" data-token="8ef80ededb5ef58041916a49d52ca7cc">人人影视</a></li>
<li><a href="https://github.com/shanelau/rssSpider" rel="nofollow" data-token="5f0c0b455178bb1a06ab54f96cc27928">RSS 爬虫</a></li>
<li><a href="https://github.com/evilcos/crawlers" rel="nofollow" data-token="822c8a31c935fef8982500ef1f1ed2d4">rosi 妹子图</a></li>
<li><a href="https://github.com/tsarjak/WallpapersFromReddit" rel="nofollow" data-token="c25dd8e8f09a44d2861d460d3296c145">reddit 壁纸</a></li>
<li><a href="https://github.com/dannyvai/reddit_crawlers" rel="nofollow" data-token="723ff6829242cf6522fb8aea801e5180">reddit</a></li>
</ul>
<h3>S</h3>
<ul>
<li><a href="https://github.com/Cortexelus/dadabots" rel="nofollow" data-token="8ca3f0d1afb473a97c8163323a024893">soundcloud</a></li>
<li><a href="https://github.com/chenjiandongx/stackoverflow" rel="nofollow" data-token="5acbfc7387873ea7d76079e61d7e7c6b">Stackoverflow 100万问答爬虫</a></li>
<li><a href="https://github.com/chenjiandongx/soksaccounts" rel="nofollow" data-token="7ecec9fbdec4d8b82bbcfc7406200e16">Shadowsocks 账号爬虫</a></li>
<li><a href="https://github.com/chengyumeng/spider163" rel="nofollow" data-token="f520b58075169535207fedf5efe9ac52">spider163 网易云音乐爬虫</a></li>
<li><a href="https://github.com/Danielyan86/Movie-scrapy" rel="nofollow" data-token="b7bbe8336cfa8cc34552de70558b8eec">时光网电影数据和海报爬虫</a></li>
</ul>
<h3>T</h3>
<ul>
<li><a href="https://github.com/facert/tumblr_spider" rel="nofollow" data-token="f32d6206a858f68f9ce147ba6c4089bb">tumblr</a></li>
<li><a href="https://github.com/cyang812/get_tumblr_likes" rel="nofollow" data-token="9c8674634d1c9c9f59bf48f6479d14d5">下载tumblr喜欢内容</a></li>
<li><a href="https://github.com/waditu/tushare" rel="nofollow" data-token="74128189c2bb00c5d16c4657ad958b9d">TuShare</a></li>
<li><a href="https://github.com/LiuXingMing/Tmall1212" rel="nofollow" data-token="16421bfac10b379325ca567f41c4d196">天猫双12爬虫</a></li>
<li><a href="https://github.com/carlonelong/TaobaoMMCrawler" rel="nofollow" data-token="14d752372209e8394349cbfa720cb5b4">Taobao mm</a></li>
<li><a href="https://github.com/chenjiandongx/cup-size" rel="nofollow" data-token="fa5121083aa915671abeb8d748655a8b">Tmall 女性文胸尺码爬虫</a></li>
<li><a href="https://github.com/xiaozhongliu/taobao-live-crawler" rel="nofollow" data-token="8e2bbe477934f1b1ca341b58fd5c0695">淘宝直播弹幕爬虫(node)</a></li>
<li><a href="https://github.com/CasterWx/java-Crawler/tree/master/src/%E7%88%AC%E8%99%AB/%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90/%E7%88%AC%E5%8F%96%E8%AE%BA%E5%9D%9B%E6%96%87%E7%AB%A0" rel="nofollow" data-token="f7c3b54d25e78232c5557382a0cdf823">天涯论坛文章</a></li>
</ul>
<h3>V</h3>
<ul>
<li><a href="https://github.com/qwertyuiop6/get_youtube_subtitle" rel="nofollow" data-token="2092c509159850ad0101213bbf026289">Youtube字幕下载</a></li>
<li><a href="https://github.com/billvsme/videoSpider" rel="nofollow" data-token="09cdfc2cb15e3179da7df025ab3a2e61">视频信息爬虫</a></li>
<li><a href="https://github.com/chenqing/spider" rel="nofollow" data-token="047c5330655418831028fd51ca20da51">电影网站</a></li>
</ul>
<h3>W</h3>
<ul>
<li><a href="https://github.com/hanc00l/wooyun_public" rel="nofollow" data-token="173865a446328242cc883436238c129e">乌云公开漏洞</a></li>
<li><a href="https://github.com/bowenpay/wechat-spider" rel="nofollow" data-token="90d088a4d00fabd40a64a4c61fab5546">微信公众号</a></li>
<li><a href="https://github.com/lijinma/wechat_spider" rel="nofollow" data-token="659a2badc36704cb43730c5bd202dfaa">“代理”方式抓取微信公众号文章</a></li>
<li><a href="https://github.com/armysheng/tech163newsSpider" rel="nofollow" data-token="58eea48de2122166e38559ef8f5beb58">网易新闻</a></li>
<li><a href="https://github.com/dongweiming/commentbox" rel="nofollow" data-token="a70077848abed44bec18d6ec858ce03e">网易精彩评论</a></li>
<li><a href="https://github.com/luzhijun/weiboSA" rel="nofollow" data-token="28b5b9fd36820dd21e40192e60f76842">微博主题搜索分析</a></li>
<li><a href="https://github.com/RitterHou/music-163" rel="nofollow" data-token="6d78646c2128aad488d604bc93e820d4">网易云音乐</a></li>
<li><a href="https://github.com/CasterWx/java-Crawler/tree/master/src/%E7%88%AC%E8%99%AB/%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90/%E7%BD%91%E6%98%93%E4%BA%91%E7%83%AD%E8%AF%84%E7%88%AC%E5%8F%96" rel="nofollow" data-token="d3f5fa45161d0ceaf38a27c3c854b732">新.网易热评</a></li>
<li><a href="https://github.com/CasterWx/java-Crawler/tree/master/src/%E7%88%AC%E8%99%AB/%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90/vip" rel="nofollow" data-token="5e9e6cc9aabcd9097219499c25da6ae3">唯品会商品</a></li>
</ul>
<h3>X</h3>
<ul>
<li><a href="https://github.com/decaywood/XueQiuSuperSpider" rel="nofollow" data-token="1e05ad2b66ce830786e874e6cb722674">雪球股票信息(java)</a></li>
<li><a href="https://github.com/LiuXingMing/SinaSpider" rel="nofollow" data-token="a06288cc9d75297d823cb5c0addcc001">新浪微博</a></li>
<li><a href="https://github.com/ResolveWang/weibospider" rel="nofollow" data-token="039e275811d84f6c2479079d4dfb6709">新浪微博分布式爬虫</a></li>
<li><a href="https://github.com/sy-records/speech_spiders/tree/master/chicken-soup" rel="nofollow" data-token="7e764eb74d2cd86b4faae71b15d90338">心灵毒鸡汤</a></li>
</ul>
<h3>Y</h3>
<ul>
<li><a href="https://github.com/pockry/tv-crawler" rel="nofollow" data-token="6042937d917da2dfe1401382b788fb47">英美剧 TV (node.js)</a></li>
</ul>
<h3>Z</h3>
<ul>
<li><a href="https://github.com/chenjiandongx/wallpaper" rel="nofollow" data-token="5f646130936f23942381d985433713b2">ZOL 手机壁纸爬虫</a></li>
<li><a href="https://github.com/LiuRoy/zhihu_spider" rel="nofollow" data-token="d4f58276cc0afc044caa326ffcc5fb24">知乎(python)</a></li>
<li><a href="https://github.com/owner888/phpspider" rel="nofollow" data-token="4e737c0046b44df0805f03f08a0ad061">知乎(php)</a></li>
<li><a href="https://github.com/yanzhou/CnkiSpider" rel="nofollow" data-token="b740aeb2ec332c71043e65c3c2ca3b81">知网</a></li>
<li><a href="https://github.com/yjm12321/zhihu-girl" rel="nofollow" data-token="e96345576185a67ff907eca8e0ef82a6">知乎妹子</a></li>
<li><a href="https://github.com/facert/ziroom_realtime_spider" rel="nofollow" data-token="521f0f22cc54f6c6ca4c9810a02208ad">自如实时房源提醒</a></li>
</ul>
<h3>#</h3>
<ul>
<li><a href="https://github.com/but0n/JianSo_Movie" rel="nofollow" data-token="9fac113240cb388153d476def9624f35">80s 影视资源爬虫 &#8211; JianSo_Movie</a></li>
</ul>
<h3>其他</h3>
<ul>
<li><a href="https://github.com/Nyloner/Nyspider" rel="nofollow" data-token="4533bff44db4c55b3d832ea070b52245">各种爬虫</a></li>
<li><a href="https://github.com/blueskyz/DHTCrawler" rel="nofollow" data-token="ea7cb1bb92f599722f9fe69a4109dcf6">DHT 爬虫</a></li>
<li><a href="https://github.com/dontcontactme/simDHT" rel="nofollow" data-token="7a64838e031abefede104e42ea3bfc8f">SimDHT</a></li>
<li><a href="https://github.com/dontcontactme/p2pspider" rel="nofollow" data-token="8a1a199f4046119cce3100401ac28c46">p2pspider</a></li>
</ul></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>爬虫系列（四）&#8211;全站爬取</title>
		<link>https://uzzz.org/article/3305.html</link>
				<pubDate>Mon, 17 Sep 2018 06:49:39 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[Python]]></category>
		<category><![CDATA[爬虫]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/3305.html</guid>
				<description><![CDATA[爬虫系列（四）&#8211;全站爬取 全站爬取需要的数据基于一个这样的假设：某网站的页面上存在该网站其他页面的连接，通过这些连接跳转的新的页面进行数据的爬取。在开始这个之前，要先明白栈和队列。本篇中介绍的是单线程的实现方式，大规模的爬取需要多线程，分布式爬取。 1.实现步骤 （1）准备几个起始链接加入待队列Q中，例如Q=[&#8220;http://www.xxx.com/aaa/&#8221;,&#8221;http://www.xxx.com/bbb/&#8221;,&#8221;http://www.xxx.com/ccc/&#8221;] （2）并将这几个链接加入一个入队集合S中,S={&#8220;http://www.xxx.com/aaa/&#8221;,&#8221;http://www.xxx.com/bbb/&#8221;,&#8221;http://www.xxx.com/ccc/&#8221;}这个集合作用是保证一个网页只爬取一次。 （3）从Q中取出一个链接url（出队，取出队首元素，并从队列中删除该元素），如果Q中没有链接，结束爬取。如果有链接url=&#8221;http://www.xxx.com/aaa/&#8221;，进行（4）步骤。 （4）对url爬取，保存需要的数据（写到文件中，建议使用json格式保存，一行一个页面），找出该页面上的所有链接urls （5）把符合我们要求的连接（例如以http://www.xxx.com 开头的链接）找出来，判断每一个连接urli是否在S中。如果不在S中，把urli加入S，urli入队 （6）继续执行（3）步骤 注意1：这个是广度优先爬取，如果把队换成栈，会变成深度优先爬取。如果没有特殊的需求，一般都是使用广度优先爬取。 注意2：对于一个小网站来说，这样操作没有什么问题，但是有些网站页面很多，Q和S中存储的连接太多直接撑爆内存，这时可以实现一个硬盘队列（栈）和硬盘集合，本系列文章不实现这些功能。 注意3：有些网站的连接到站内的url形如&#8221;/aaa?a=1&#38;b=2&#8243;,需要改写成&#8221;http://域名/aaa?a=1&#38;b=2&#8243; 注意4：有些网站会根据短时间内一个ip访问大量页面制定反爬虫策略，可以爬取一个页面后，休眠一段时间接着爬取 2.代码实现(代码仅对于代码中要爬取的网站有效，其他网站需要重新配置规则) import time import os import json from urllib import request from lxml import etree header_dict = { "Accept":"application/json, text/javascript, */*; q=0.01", "Accept-Language":"zh-CN,zh;q=0.9", "User-Agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36", } def get_http(load_url,header=None): res="" try: req = request.Request(url=load_url,headers=header)#创建请求对象 coonect = request.urlopen(req)#打开该请求 byte_res = coonect.read()#读取所有数据，很暴力 try: res=byte_res.decode(encoding='utf-8') except: try: res=byte_res.decode(encoding='gbk') except: res="" except Exception as e: print(e) return res #创建数据文件夹 if not os.path.exists("./sina_data"): os.mkdir("sina_data") #输出的文件 raw_file=open("./sina_data/raw.txt","w",encoding="utf-8") json_file=open("./sina_data/json.txt","w",encoding="utf-8") #数据存储模板 obj={"title":"","url":"","content":""} saved_url={"http://www.sina.com.cn/"}#记录已经爬取过的页面 url_list=["http://www.sina.com.cn/"]#url队列 while len(url_list)&#62;0: url=url_list.pop() #获取html html_text=get_http(url,header_dict) if html_text=="": continue time.sleep(0.15) try: tree=etree.HTML(html_text) dir(tree) #url有这样特征的可能是文章 if url.find("html")&#62;=0 and url.find("list")&#60;0: #找出文章标题，根据多个页面找出标题规则，发现不满足该规则的标题要添加进去 t_xpath=[ "//h1[@id='main_title']/text()", "//h1/text()", "//th[@class='f24']//font/text()", "/html/head/title/text()" ] title=[] for tx in t_xpath: if len(title)==0: title=tree.xpath(tx) else: break #找出文章正文，根据多个页面找出正文规则，发现不满足该规则的正文要添加进去 c_xpath=[ "//div[@id='artibody']//p/text()", "//td[@class='l17']//p/text()", "//div[@class='content']//p/text()", "//div[@class='article']//p/text()", "//div[@id='article']//p/text()", "//div[@class='article-body main-body']//p/text()", "//div[@class='textbox']//p/text()" ] content=[] for cx in c_xpath: if len(content)==0: content=tree.xpath(cx) else: break if len(title)*len(content)==0: #没有标题或正文保存原始网页，这个可以不写，有些页面根本不是文章 raw_file.write(html_text.replace("\n", "").replace("\r", "")) raw_file.write("\n") print("没有标题或正文"+url) else: #既有标题，也有正文保存数据 obj["url"]=url obj["title"]=title[0] obj["content"]=" ".join(content) jstr=json.dumps(obj) json_file.write(jstr) json_file.write("\n") #找到所有url urls=tree.xpath("//a/@href") for u in urls: flag=False #过滤url end_filter=[".apk",".iso",".jpg",".jpeg",".bmp",".cdr",".php",".exe",".dmg",".apk"] for f in end_filter: if "".endswith(f): flag=True break if]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
<h1>爬虫系列（四）&#8211;全站爬取</h1>
<p style="text-indent:50px;"><em>全站爬取需要的数据基于一个这样的假设：某网站的页面上存在该网站其他页面的连接，通过这些连接跳转的新的页面进行数据的爬取。在开始这个之前，要先明白栈和队列。本篇中介绍的是单线程的实现方式，大规模的爬取需要多线程，分布式爬取。</em></p>
<hr>
<h2>1.实现步骤</h2>
<p>（1）准备几个起始链接加入待队列Q中，例如Q=[&#8220;http://www.xxx.com/aaa/&#8221;,&#8221;http://www.xxx.com/bbb/&#8221;,&#8221;http://www.xxx.com/ccc/&#8221;]</p>
<p>（2）并将这几个链接加入一个入队集合S中,S={&#8220;http://www.xxx.com/aaa/&#8221;,&#8221;http://www.xxx.com/bbb/&#8221;,&#8221;http://www.xxx.com/ccc/&#8221;}这个集合作用是保证一个网页只爬取一次。</p>
<p>（3）从Q中取出一个链接url（出队，取出队首元素，并从队列中删除该元素），如果Q中没有链接，结束爬取。如果有链接url=&#8221;http://www.xxx.com/aaa/&#8221;，进行（4）步骤。</p>
<p>（4）对url爬取，保存需要的数据（写到文件中，建议使用json格式保存，一行一个页面），找出该页面上的所有链接urls</p>
<p>（5）把符合我们要求的连接（例如以http://www.xxx.com 开头的链接）找出来，判断每一个连接urli是否在S中。如果不在S中，把urli加入S，urli入队</p>
<p>（6）继续执行（3）步骤</p>
<p><span style="color:#f33b45;">注意1：这个是广度优先爬取，如果把队换成栈，会变成深度优先爬取。如果没有特殊的需求，一般都是使用广度优先爬取。</span></p>
<p><span style="color:#f33b45;">注意2：对于一个小网站来说，这样操作没有什么问题，但是有些网站页面很多，Q和S中存储的连接太多直接撑爆内存，这时可以实现一个硬盘队列（栈）和硬盘集合，本系列文章不实现这些功能。</span></p>
<p><span style="color:#f33b45;">注意3：有些网站的连接到站内的url形如&#8221;/aaa?a=1&amp;b=2&#8243;,需要改写成&#8221;http://域名/aaa?a=1&amp;b=2&#8243;</span></p>
<p><span style="color:#f33b45;">注意4：有些网站会根据短时间内一个ip访问大量页面制定反爬虫策略，可以爬取一个页面后，休眠一段时间接着爬取</span></p>
<hr>
<h2>2.代码实现(代码仅对于代码中要爬取的网站有效，其他网站需要重新配置规则)</h2>
<pre class="has">
<code class="language-python">import time
import os
import json
from urllib import request
from lxml import etree

header_dict = {
    "Accept":"application/json, text/javascript, */*; q=0.01",
    "Accept-Language":"zh-CN,zh;q=0.9",
    "User-Agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36",
    }
def get_http(load_url,header=None):
    res=""
    try:
        req = request.Request(url=load_url,headers=header)#创建请求对象
        coonect = request.urlopen(req)#打开该请求
        byte_res = coonect.read()#读取所有数据，很暴力
        try:
            res=byte_res.decode(encoding='utf-8')
        except:
            try:
                res=byte_res.decode(encoding='gbk')
            except:
                res=""
    except Exception as e:
        print(e)
    return res

#创建数据文件夹
if not os.path.exists("./sina_data"):
    os.mkdir("sina_data")
#输出的文件
raw_file=open("./sina_data/raw.txt","w",encoding="utf-8")
json_file=open("./sina_data/json.txt","w",encoding="utf-8")

#数据存储模板
obj={"title":"","url":"","content":""}

saved_url={"http://www.sina.com.cn/"}#记录已经爬取过的页面
url_list=["http://www.sina.com.cn/"]#url队列

while len(url_list)&gt;0:
    url=url_list.pop()
    #获取html
    html_text=get_http(url,header_dict)
    if html_text=="":
        continue
    time.sleep(0.15)
    try:
        tree=etree.HTML(html_text)
        dir(tree)
        #url有这样特征的可能是文章
        if url.find("html")&gt;=0 and url.find("list")&lt;0:
            #找出文章标题，根据多个页面找出标题规则，发现不满足该规则的标题要添加进去
            t_xpath=[
                "//h1[@id='main_title']/text()",
                "//h1/text()",
                "//th[@class='f24']//font/text()",
                "/html/head/title/text()"
                ]
            title=[]
            for tx in t_xpath:
                if len(title)==0:
                    title=tree.xpath(tx)
                else:
                    break
            
            #找出文章正文，根据多个页面找出正文规则，发现不满足该规则的正文要添加进去
            c_xpath=[
                "//div[@id='artibody']//p/text()",
                "//td[@class='l17']//p/text()",
                "//div[@class='content']//p/text()",
                "//div[@class='article']//p/text()",
                "//div[@id='article']//p/text()",
                "//div[@class='article-body main-body']//p/text()",
                "//div[@class='textbox']//p/text()"
                ]
            content=[]
            for cx in c_xpath:
                if len(content)==0:
                    content=tree.xpath(cx)
                else:
                    break
            
            if len(title)*len(content)==0:
                #没有标题或正文保存原始网页，这个可以不写，有些页面根本不是文章
                raw_file.write(html_text.replace("\n", "").replace("\r", ""))
                raw_file.write("\n")
                print("没有标题或正文"+url)
            else:
                #既有标题，也有正文保存数据
                obj["url"]=url
                obj["title"]=title[0]
                obj["content"]=" ".join(content)
                jstr=json.dumps(obj)
                json_file.write(jstr)
                json_file.write("\n")
                
        #找到所有url
        urls=tree.xpath("//a/@href")
        for u in urls:
            
            flag=False
            
            #过滤url
            end_filter=[".apk",".iso",".jpg",".jpeg",".bmp",".cdr",".php",".exe",".dmg",".apk"]
            for f in end_filter:
                if "".endswith(f):
                    flag=True
                    break
            if flag:
                continue
            
            find_filter=[
                "vip.","guba.","lottery.","kaoshi.","club.baby","jiancai.",".cn/ku/","astro.","match.","games.","zhongce","list",
                "photo.","yangfanbook","zx.jiaju","nc.shtml","english.","download","chexian","auto","video","comfinanceweb.shtml",
                "//sax.","login","/bc.","aipai.","vip.book","talk.t","slide.","club.baby","biz.finance","blog","comment5","www.leju",
                "http://m."
                ]
            for f in find_filter:
                if u.find(f)&gt;=0:
                    flag=True
                    break
            if flag:
                continue
              
            if u.startswith("http") and u.find(".sina.")&gt;=0:
                if u in saved_url:
                    continue
                #加入待爬取队列
                saved_url.add(url)
                url_list.append(u)
    except Exception as e:
        print("error")
raw_file.close()
json_file.close()
        
        </code></pre>
<hr>
<p style="text-indent:50px;"><em>下一篇文章，使用爬虫爬取某商城网站评论数据。这类数据是在页面内动态加载的，之前这种方式已经不适用了。除了评论外，下拉加载的页面也是这样的。想了解这类数据爬取方法，那就快看下一篇吧。今天是2018年9月17日，下一篇可能还不存在，不过肯定会写的。</em></p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>python爬虫之反爬虫情况下的煎蛋网图片爬取初步探索</title>
		<link>https://uzzz.org/article/2226.html</link>
				<pubDate>Wed, 06 Dec 2017 11:04:02 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[爬虫]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2226.html</guid>
				<description><![CDATA[本次爬虫网址：http://jandan.net/ooxx 前言： &#160; 前段时间一直在折腾基于qqbot的QQ机器人，昨天用itchat在微信上也写了一个机器人，相比webqq，微信的web端功能比较丰富，图片、文件等都可以传输。今天闲来无事准备给写个爬虫丰富微信机器人的功能，就想到了爬煎蛋网上面的图片。 &#160; 说做就做，打开浏览器一看，渲染前的源码里是没有图片地址的。这个很正常，首先想到的就是异步请求去获取例如json格式的图片地址，然后渲染在页面上。于是用Chrome的全局搜索功能尝试搜了一下某一张图片的地址，结果居然是没有搜到。早就耳闻煎蛋被爬虫弄得苦不堪言，看来也开始采取一些措施了。于是去GitHub上搜了一下jiandan关键词，按时间排序，发现靠前的几个项目要不就没意识到煎蛋的反爬从而还是在用原来的方式直接处理源码，要不就是在用selenium（web自动化框架，可参考我的这篇文章：点击打开链接）进行爬虫。看来这个简单的这个反爬机制是最近一个月才用上的，很不凑巧被我撞上了。用selenium确实是一种万能、省力的方式，但实在太耗费性能。我在我阿里云的服务器上放了一个selenium+chromeheadless的微博爬虫，每次爬虫运行的时候服务器都非常卡。加上对煎蛋的反爬机制挺好奇的，于是我就准备通过分析js来找出图片的请求地址。 正文： &#160; 首先查看js渲染前的html源码，发现放图片的位置是这样的 本该放地址的地方赫然放着blank.gif，并且在onload属性上绑定了一个jandan_load_img函数。这个jandan_load_img就成为本次爬虫的突破所在了。继续ctrl+shift+F全局搜索，找到这个函数 function jandan_load_img(b) { var d = $(b); var f = d.next("span.img-hash"); var e = f.text(); f.remove(); var c = f_K1Ft7i9UekcAhptpgQlLRFFKpzH6gOr0(e, "n8DpQLgoyVr2evbxYcQyFzxk9NRmsSKQ"); var a = $('&#60;a href="' + c.replace(/(\/\/\w+\.sinaimg\.cn\/)(\w+)(\/.+\.(gif&#124;jpg&#124;jpeg))/, "$1large$3") + '" target="_blank" class="view_img_link"&#62;[查看原图]&#60;/a&#62;'); d.before(a); d.before("&#60;br&#62;"); d.removeAttr("onload"); d.attr("src", location.protocol + c.replace(/(\/\/\w+\.sinaimg\.cn\/)(\w+)(\/.+\.gif)/, "$1thumb180$3")); if (/\.gif$/.test(c)) { d.attr("org_src", location.protocol + c); b.onload = function() { add_img_loading_mask(this, load_sina_gif) } } } &#160; 果然就是这个函数在处理图片相关的标签，写在一个单独的js文件里。容易看到，第7、8行将a标签插入到img之前，查看源码看到a标签就是是查看原图的链接，也就是我们接下来爬取的时候用到的地址了。第6行f_后跟着一长串字母的这个函数(简称f函数)返回的就是图片地址。第7行中replace函数的作用是当图片为gif时替换中间的一个字符串为large。 &#160; 那么接下来的任务就是分析f函数到底是怎么获取图片的地址的。首先看参数，第一个参数e为img-hash标签的text，第二个参数则是一个常量。这个常量我实测是会变化的，所以需要我们去请求这个js文件然后用正则去匹配到该常量。js文件的地址则写在了html源码里，文件名应该也是会变化的，也是用正则去匹配到。拿到常量之后接下来仍然使用chrome全局搜索（注：最好是打上断点跳过去，同一个js文件里的第605行和第943行有两个f函数可能会造成干扰，参见本文评论区），找到f函数，我发现此函数只是在做一些md5、base_64加密等操作，并不算复杂，可以将js代码转为python运行。当然也可以选择直接运行js，不过应该也是比较耗费性能的。我在转化成python代码的过程中，在base64解码上耗费了较长时间，也说明了自己在字符编码方面的知识比较薄弱。 &#160; 下面是我对转换的一点解释，把一些无意义的ifelse等代码精简掉，再把代码分成五块之后，f函数长这样： var f_K1Ft7i9UekcAhptpgQlLRFFKpzH6gOr0 = function(m, r, d) { var q = 4; r = md5(r); var o = md5(r.substr(0, 16)); var l = m.substr(0, q); var c = o + md5(o + l); var k; m = m.substr(q); k = base64_decode(m); var h = new Array(256); for (var g = 0; g &#60; 256; g++) { h[g] = g } var b = new Array(); for (var g = 0; g &#60; 256; g++) { b[g] = c.charCodeAt(g % c.length) } for (var f = g = 0; g &#60; 256; g++) { f = (f + h[g] + b[g]) % 256; tmp = h[g]; h[g] = h[f];]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
<p>本次爬虫网址：http://jandan.net/ooxx</p>
</p>
<h1 style="font-family:'microsoft yahei';font-weight:300;line-height:1.1;color:rgb(63,63,63);"> <span style="font-size:48px;">前言：</span></h1>
</p>
<p>&nbsp; 前段时间一直在折腾基于qqbot的QQ机器人，昨天用itchat在微信上也写了一个机器人，相比webqq，微信的web端功能比较丰富，图片、文件等都可以传输。今天闲来无事准备给写个爬虫丰富微信机器人的功能，就想到了爬煎蛋网上面的图片。</p>
<p>&nbsp; 说做就做，打开浏览器一看，渲染前的源码里是没有图片地址的。这个很正常，首先想到的就是异步请求去获取例如json格式的图片地址，然后渲染在页面上。于是用Chrome的全局搜索功能尝试搜了一下某一张图片的地址，结果居然是没有搜到。早就耳闻煎蛋被爬虫弄得苦不堪言，看来也开始采取一些措施了。于是去GitHub上搜了一下jiandan关键词，按时间排序，发现靠前的几个项目要不就没意识到煎蛋的反爬从而还是在用原来的方式直接处理源码，要不就是在用selenium（web自动化框架，可参考我的这篇文章：<a href="http://blog.csdn.net/van_brilliant/article/details/78239476" rel="nofollow" data-token="f9539be4df780d4cabe79f7cfe98a785">点击打开链接</a>）进行爬虫。看来这个简单的这个反爬机制是最近一个月才用上的，很不凑巧被我撞上了。用selenium确实是一种万能、省力的方式，但实在太耗费性能。我在我阿里云的服务器上放了一个selenium+chromeheadless的微博爬虫，每次爬虫运行的时候服务器都非常卡。加上对煎蛋的反爬机制挺好奇的，于是我就准备通过分析js来找出图片的请求地址。</p>
</p>
<h1 style="color:rgb(63,63,63);line-height:1.1;text-align:justify;font-family:'microsoft yahei';font-weight:300;"> <span style="font-size:48px;">正文：</span></h1>
</p>
<p>&nbsp; 首先查看js渲染前的html源码，发现放图片的位置是这样的<img src="" alt=""><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20171206174439565?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdmFuX2JyaWxsaWFudA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
<p>本该放地址的地方赫然放着blank.gif，并且在onload属性上绑定了一个jandan_load_img函数。这个jandan_load_img就成为本次爬虫的突破所在了。继续ctrl+shift+F全局搜索，找到这个函数</p>
<p><img src="" alt=""></p>
<pre><code class="language-javascript">function jandan_load_img(b) {
    var d = $(b);
    var f = d.next("span.img-hash");
    var e = f.text();
    f.remove();
    var c = f_K1Ft7i9UekcAhptpgQlLRFFKpzH6gOr0(e, "n8DpQLgoyVr2evbxYcQyFzxk9NRmsSKQ");
    var a = $('&lt;a href="' + c.replace(/(\/\/\w+\.sinaimg\.cn\/)(\w+)(\/.+\.(gif|jpg|jpeg))/, "$1large$3") + '" target="_blank" class="view_img_link"&gt;[查看原图]&lt;/a&gt;');
    d.before(a);
    d.before("&lt;br&gt;");
    d.removeAttr("onload");
    d.attr("src", location.protocol + c.replace(/(\/\/\w+\.sinaimg\.cn\/)(\w+)(\/.+\.gif)/, "$1thumb180$3"));
    if (/\.gif$/.test(c)) {
        d.attr("org_src", location.protocol + c);
        b.onload = function() {
            add_img_loading_mask(this, load_sina_gif)
        }
    }
}</code></pre>
<p>&nbsp; 果然就是这个函数在处理图片相关的标签，写在一个单独的js文件里。容易看到，第7、8行将a标签插入到img之前，查看源码看到<span style="color:rgb(79,79,79);text-align:justify;">a标签</span>就是是查看原图的链接，也就是我们接下来爬取的时候用到的地址了。第6行f_后跟着一长串字母的这个函数(简称f函数)返回的就是图片地址。第7行中replace函数的作用是当图片为gif时替换中间的一个字符串为large。</p>
<p>&nbsp; 那么接下来的任务就是分析f函数到底是怎么获取图片的地址的。首先看参数，第一个参数e为img-hash标签的text，第二个参数则是一个常量。这个常量我实测是会变化的，所以需要我们去请求这个js文件然后用正则去匹配到该常量。js文件的地址则写在了html源码里，文件名应该也是会变化的，也是用正则去匹配到。拿到常量之后接下来仍然使用chrome全局搜索<strong>（注：最好是打上断点跳过去，同一个js文件里的第605行和第943行<span style="color:rgb(79,79,79);text-align:justify;">有两个<span style="color:rgb(79,79,79);text-align:justify;">f函数</span>可能会造成干扰，参见本文评论区</span>）</strong>，找到f函数，我发现此函数只是在做一些md5、base_64加密等操作，并不算复杂，可以将js代码转为python运行。当然也可以选择直接运行js，不过应该也是比较耗费性能的。我在转化成python代码的过程中，在base64解码上耗费了较长时间，也说明了自己在字符编码方面的知识比较薄弱。</p>
<p>&nbsp; 下面是我对转换的一点解释，把一些无意义的ifelse等代码精简掉，再把代码分成五块之后，f函数长这样：</p>
<pre><code class="language-javascript">var f_K1Ft7i9UekcAhptpgQlLRFFKpzH6gOr0 = function(m, r, d) {

    var q = 4;
    r = md5(r);
    var o = md5(r.substr(0, 16));
    var l = m.substr(0, q);	
    var c = o + md5(o + l);

	
	
    var k;
    m = m.substr(q);
    k = base64_decode(m);
    
	
	
    var h = new Array(256);
    for (var g = 0; g &lt; 256; g++) {
        h[g] = g
    }
    var b = new Array();
    for (var g = 0; g &lt; 256; g++) {
        b[g] = c.charCodeAt(g % c.length)
    }
	
	
	
    for (var f = g = 0; g &lt; 256; g++) {
        f = (f + h[g] + b[g]) % 256;
        tmp = h[g];
        h[g] = h[f];
        h[f] = tmp;
    }
	
	
	
    var t = "";
    k = k.split("");
    for (var p = f = g = 0; g &lt; k.length; g++) {
        p = (p + 1) % 256;
        f = (f + h[p]) % 256;
        tmp = h[p];
        h[p] = h[f];
        h[f] = tmp;
        t += chr(ord(k[g]) ^ (h[(h[p] + h[f]) % 256]));
    }
    t = t.substr(26);
			
    return t
};</code></pre>
<p></p>
<p>&nbsp; &nbsp; 转换得到的python代码也相对应地分成五块之后如下：</p>
<pre><code class="language-python">def parse(imgHash, constant): 

    q = 4
    constant = md5(constant)
    o = md5(constant[0:16])
    l = imgHash[0:q]
    c = o + md5(o + l)



    imgHash = imgHash[q:]
    k = decode_base64(imgHash)



    h =list(range(256))
    b = list(range(256))
    for g in range(0,256):
        b[g] = ord(c[g % len(c)])


        
    f=0
    for g in range(0,256):
        f = (f+h[g]+b[g]) % 256
        tmp = h[g]
        h[g] = h[f]
        h[f] = tmp
        


    result = ""
    p=0
    f=0
    for g in range(0,len(k)):   
        p = (p + 1) % 256;
        f = (f + h[p]) % 256
        tmp = h[p]
        h[p] = h[f]
        h[f] = tmp
        result += chr(k[g] ^ (h[(h[p] + h[f]) % 256]))
    result = result[26:]
    
    return result</code></pre>
<p>&nbsp; 这样对比之下应该就比较清晰了，基本上就是逐行翻译。另外base64需要重写一下。</p>
<p>&nbsp; 最后就是一些普通的http请求操作了，以及使用itchat进行图片传输。所有代码已经上传到github上，后续有时间我打算添加上ip代理等新功能。没有系统学习过python所以代码可能不太规范，希望大家多多交流。项目地址：<a href="https://github.com/van1997/JiandanSpider" rel="nofollow" data-token="82e89ec28c930f3c4717dacb41d160a1">点击打开链接</a></p>
<p>另附本次爬虫的思维导图：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20171206171420462?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdmFuX2JyaWxsaWFudA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></p>
<p></p>
<p>温馨提醒：虽然煎蛋肯定还有其他反爬措施，大家在爬虫过程中请务必遵守基本的互联网秩序，具体是啥相信大家都懂滴。</p>
<p><span style="font-size:48px;"><br /></span></p>
<p><span style="font-size:48px;">参考资料：</span></p>
<p>python base64解码incorect padding错误：<a href="https://stackoverflow.com/questions/2941995/python-ignore-incorrect-padding-error-when-base64-decoding" rel="nofollow" data-token="a97a7945c49e2dc6f4a7c574414984a2">点击打开链接</a></p>
<p></p>
<p>写于2017年12月6日晚</p>
<p>编辑于2018年2月6日晚</p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>python爬虫代理IP池(proxy pool)</title>
		<link>https://uzzz.org/article/2412.html</link>
				<pubDate>Thu, 14 Sep 2017 02:51:18 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[Python]]></category>
		<category><![CDATA[爬虫]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2412.html</guid>
				<description><![CDATA[1.今天我们来讲下一个非常有用的东西，代理ip池，结果就是一个任务每隔一定时间去到 目标ip代理提供网站（www.bugng.com）去爬取可用数据存到mysql数据库，并且检测数据库已有数据是否可用，不可用就删除。 2. 编写 提取代理ip到数据库 的爬虫 2.1准备mysql表 CREATE TABLE `t_ips` ( `id` int(10) NOT NULL AUTO_INCREMENT COMMENT '主键', `ip` varchar(15) COLLATE utf8_unicode_ci DEFAULT NULL COMMENT 'ip', `port` int(10) NOT NULL COMMENT 'port', `type` int(10) NOT NULL DEFAULT '0' COMMENT '0:http 1:https', PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=421 DEFAULT CHARSET=utf8 COLLATE=utf8_unicode_ci COMMENT='ip表'; 2.2创建爬虫工程，编写items.py(对应数据库的字段) import scrapy class IpsItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() ip = scrapy.Field() port = scrapy.Field() httpType = scrapy.Field() 2.3编写settings.py # -*- coding: utf-8 -*- ####################自已的配置################ MAX_PAGE = 2 ##抓取的代理ip网址 的 页数 #0 : http 1:https TYPE = 0 ### 代理ip类型 URL = 'http://www.bugng.com/gnpt?page=' ### 代理ip网址 TIMER_STOP_TIME = 20 ### 定时器暂停执行时间 ###################################### BOT_NAME = 'ips' SPIDER_MODULES = ['ips.spiders'] NEWSPIDER_MODULE = 'ips.spiders' USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36' ITEM_PIPELINES = { 'ips.pipelines.IpsPipeline': 300, } # 禁止重试 RETRY_ENABLED = False # Crawl responsibly by identifying yourself (and your website) on the user-agent #USER_AGENT = 'csdn (+http://www.yourdomain.com)' # Obey robots.txt rules ROBOTSTXT_OBEY = False # 减小下载超时: DOWNLOAD_TIMEOUT = 2 # 禁止cookies: COOKIES_ENABLED = False # 延迟下载 防止被ban]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<p>1.今天我们来讲下一个非常有用的东西，代理ip池，结果就是一个任务每隔一定时间去到 目标ip代理提供网站（www.bugng.com）去爬取可用数据存到mysql数据库，并且检测数据库已有数据是否可用，不可用就删除。</p>
<p><strong>2. 编写 提取代理ip到数据库 的爬虫</strong></p>
<p>2.1准备mysql表</p>
<pre class="prettyprint"><code class=" hljs sql"><span class="hljs-operator"><span class="hljs-keyword">CREATE</span> <span class="hljs-keyword">TABLE</span> <span class="hljs-string">`t_ips`</span> ( <span class="hljs-string">`id`</span> <span class="hljs-keyword">int</span>(<span class="hljs-number">10</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span> AUTO_INCREMENT COMMENT <span class="hljs-string">'主键'</span>, <span class="hljs-string">`ip`</span> <span class="hljs-keyword">varchar</span>(<span class="hljs-number">15</span>) <span class="hljs-keyword">COLLATE</span> utf8_unicode_ci <span class="hljs-keyword">DEFAULT</span> <span class="hljs-keyword">NULL</span> COMMENT <span class="hljs-string">'ip'</span>, <span class="hljs-string">`port`</span> <span class="hljs-keyword">int</span>(<span class="hljs-number">10</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span> COMMENT <span class="hljs-string">'port'</span>, <span class="hljs-string">`type`</span> <span class="hljs-keyword">int</span>(<span class="hljs-number">10</span>) <span class="hljs-keyword">NOT</span> <span class="hljs-keyword">NULL</span> <span class="hljs-keyword">DEFAULT</span> <span class="hljs-string">'0'</span> COMMENT <span class="hljs-string">'0:http 1:https'</span>, <span class="hljs-keyword">PRIMARY</span> <span class="hljs-keyword">KEY</span> (<span class="hljs-string">`id`</span>) ) ENGINE=InnoDB AUTO_INCREMENT=<span class="hljs-number">421</span> <span class="hljs-keyword">DEFAULT</span> CHARSET=utf8 <span class="hljs-keyword">COLLATE</span>=utf8_unicode_ci COMMENT=<span class="hljs-string">'ip表'</span>;</span></code></pre>
<p>2.2创建爬虫工程，编写items.py(对应数据库的字段)</p>
<pre class="prettyprint"><code class=" hljs haskell"><span class="hljs-import"><span class="hljs-keyword">import</span> scrapy</span>
<span class="hljs-class"> <span class="hljs-keyword">class</span> <span class="hljs-type">IpsItem</span><span class="hljs-container">(<span class="hljs-title">scrapy</span>.<span class="hljs-type">Item</span>)</span>: # define the fields for your item here like: # name = scrapy.<span class="hljs-type">Field</span><span class="hljs-container">()</span> ip = scrapy.<span class="hljs-type">Field</span><span class="hljs-container">()</span> port = scrapy.<span class="hljs-type">Field</span><span class="hljs-container">()</span> httpType = scrapy.<span class="hljs-type">Field</span><span class="hljs-container">()</span></span></code></pre>
<p>2.3编写settings.py</p>
<pre class="prettyprint"><code class=" hljs vala"><span class="hljs-preprocessor"># -*- coding: utf-8 -*-</span>


<span class="hljs-preprocessor">####################自已的配置################</span>
MAX_PAGE = <span class="hljs-number">2</span>   ##抓取的代理ip网址 的 页数
<span class="hljs-preprocessor">#0 : http 1:https</span>
TYPE = <span class="hljs-number">0</span>   ### 代理ip类型
URL = <span class="hljs-string">'http://www.bugng.com/gnpt?page='</span>   ### 代理ip网址

TIMER_STOP_TIME = <span class="hljs-number">20</span>  ### 定时器暂停执行时间
<span class="hljs-preprocessor">######################################</span>

BOT_NAME = <span class="hljs-string">'ips'</span>

SPIDER_MODULES = [<span class="hljs-string">'ips.spiders'</span>]
NEWSPIDER_MODULE = <span class="hljs-string">'ips.spiders'</span>


USER_AGENT = <span class="hljs-string">'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'</span>
ITEM_PIPELINES = {
    <span class="hljs-string">'ips.pipelines.IpsPipeline'</span>: <span class="hljs-number">300</span>,
}
<span class="hljs-preprocessor"># 禁止重试</span>
RETRY_ENABLED = False
<span class="hljs-preprocessor"># Crawl responsibly by identifying yourself (and your website) on the user-agent</span>
<span class="hljs-preprocessor">#USER_AGENT = 'csdn (+http://www.yourdomain.com)'</span>

<span class="hljs-preprocessor"># Obey robots.txt rules</span>
ROBOTSTXT_OBEY = False
<span class="hljs-preprocessor"># 减小下载超时:</span>
DOWNLOAD_TIMEOUT = <span class="hljs-number">2</span>
<span class="hljs-preprocessor"># 禁止cookies:</span>
COOKIES_ENABLED = False
<span class="hljs-preprocessor"># 延迟下载 防止被ban</span>
DOWNLOAD_DELAY=<span class="hljs-number">2</span></code></pre>
<p>2.4编写spider <br /> 这里用到了bs4,需要自行安装</p>
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span>
<span class="hljs-keyword">import</span> scrapy
<span class="hljs-keyword">import</span> logging
<span class="hljs-keyword">from</span> bs4 <span class="hljs-keyword">import</span> BeautifulSoup
<span class="hljs-keyword">from</span> ips.items <span class="hljs-keyword">import</span> IpsItem
<span class="hljs-keyword">from</span> ips.settings <span class="hljs-keyword">import</span> *

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">XicispiderSpider</span><span class="hljs-params">(scrapy.Spider)</span>:</span>
    name = <span class="hljs-string">'xiciSpider'</span>
    allowed_domains = [<span class="hljs-string">'xicidaili.com'</span>]
    start_urls = [<span class="hljs-string">'http://xicidaili.com/'</span>]

    <span class="hljs-comment">### 开始 放入url</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">start_requests</span><span class="hljs-params">(self)</span>:</span>
        req = []
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,MAX_PAGE):
            <span class="hljs-comment">### 代理ip网址的第几页的 url</span>
            req.append(scrapy.Request(URL + str(i-<span class="hljs-number">1</span>)))
        <span class="hljs-keyword">return</span> req

    <span class="hljs-comment">## 每一页url的 解析回调函数，利用bs4解析</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span><span class="hljs-params">(self, response)</span>:</span>
        print(<span class="hljs-string">'@@@@@@@@@ 开始解析 '</span>+response.url)
        <span class="hljs-keyword">try</span>:
            soup = BeautifulSoup(str(response.body, encoding = <span class="hljs-string">"utf-8"</span>),<span class="hljs-string">'html.parser'</span>)
            trs = soup.find(<span class="hljs-string">'table'</span>,{<span class="hljs-string">'class'</span>:<span class="hljs-string">'table'</span>}).find_all(<span class="hljs-string">'tr'</span>)
            <span class="hljs-keyword">for</span> tr <span class="hljs-keyword">in</span> trs[<span class="hljs-number">1</span>:]:
                tds = tr.find_all(<span class="hljs-string">'td'</span>)
                cur = <span class="hljs-number">0</span>
                item = IpsItem()
                item[<span class="hljs-string">'httpType'</span>] = TYPE
                <span class="hljs-keyword">for</span> td <span class="hljs-keyword">in</span> tds:
                    <span class="hljs-keyword">if</span> cur == <span class="hljs-number">0</span>:
                        item[<span class="hljs-string">'ip'</span>] = td.text
                    <span class="hljs-keyword">if</span> cur == <span class="hljs-number">1</span>:
                        item[<span class="hljs-string">'port'</span>] = td.text
                    cur = cur +<span class="hljs-number">1</span>
                <span class="hljs-keyword">yield</span> item  <span class="hljs-comment">#### 给pipline处理</span>
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            logging.log(logging.WARN, <span class="hljs-string">'@@@@@@@@@ start parser '</span> + str(e))</code></pre>
<p>2.5编写pipline <br /> 这里需要安装 : pip install mysqlclient <br /> 这里插入数据库之前做两个校验： <br /> 1.数据是否存在 <br /> 2.数据是否可用</p>
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span>

<span class="hljs-keyword">import</span> MySQLdb
<span class="hljs-keyword">import</span> MySQLdb.cursors
<span class="hljs-keyword">from</span> twisted.enterprise <span class="hljs-keyword">import</span> adbapi
<span class="hljs-keyword">import</span> logging
<span class="hljs-keyword">import</span> requests

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">IpsPipeline</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        dbargs = dict(
            host=<span class="hljs-string">'你的数据库ip'</span>,
            db=<span class="hljs-string">'数据库名称'</span>,
            user=<span class="hljs-string">'root'</span>,
            passwd=<span class="hljs-string">'数据库密码'</span>,
            charset=<span class="hljs-string">'utf8'</span>,
            cursorclass=MySQLdb.cursors.DictCursor,
            use_unicode=<span class="hljs-keyword">True</span>,
        )
        self.dbpool = adbapi.ConnectionPool(<span class="hljs-string">'MySQLdb'</span>, **dbargs)
    <span class="hljs-comment">##处理每个yeild的item</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_item</span><span class="hljs-params">(self, item, spider)</span>:</span>
        res = self.dbpool.runInteraction(self.insert_into_table, item)
        <span class="hljs-keyword">return</span> item

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">insert_into_table</span><span class="hljs-params">(self, conn, item)</span>:</span>
        ip = item[<span class="hljs-string">'ip'</span>]
        port = item[<span class="hljs-string">'port'</span>]
        <span class="hljs-comment"># 先查询存不存在</span>
        <span class="hljs-keyword">if</span> self.exsist(item,conn):
            <span class="hljs-keyword">return</span>

        <span class="hljs-comment"># 查询 此代理ip是否可用，可用就加入数据库</span>
        <span class="hljs-keyword">if</span> self.proxyIpCheck(item[<span class="hljs-string">'ip'</span>],item[<span class="hljs-string">'port'</span>]) <span class="hljs-keyword">is</span> <span class="hljs-keyword">False</span>:
            print(<span class="hljs-string">"此代理ip不可用，proxy:"</span>,item[<span class="hljs-string">'ip'</span>],<span class="hljs-string">':'</span>,str(item[<span class="hljs-string">'port'</span>]))
            <span class="hljs-keyword">return</span>

        sql = <span class="hljs-string">'insert into t_ips (ip,port,type) VALUES ('</span>
        sql = sql + <span class="hljs-string">'"'</span> + item[<span class="hljs-string">'ip'</span>] + <span class="hljs-string">'",'</span>
        sql = sql + str(item[<span class="hljs-string">'port'</span>]) + <span class="hljs-string">','</span>
        sql = sql + str(item[<span class="hljs-string">'httpType'</span>]) + <span class="hljs-string">','</span>
        sql = sql[<span class="hljs-number">0</span>:-<span class="hljs-number">1</span>]
        sql = sql + <span class="hljs-string">')'</span>

        <span class="hljs-keyword">try</span>:
            conn.execute(sql)
            print(sql)
        <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
            logging.log(logging.WARNING, <span class="hljs-string">"sqlsqlsqlsqlsqlsqlsql error&gt;&gt; "</span> + sql)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">exsist</span><span class="hljs-params">(self,item,conn)</span>:</span>
        sql = <span class="hljs-string">'select * from t_ips where ip="'</span> + item[<span class="hljs-string">'ip'</span>] + <span class="hljs-string">'" and port='</span> + str(item[<span class="hljs-string">'port'</span>]) + <span class="hljs-string">''</span>
        <span class="hljs-keyword">try</span>:
            <span class="hljs-comment"># 执行SQL语句</span>
            conn.execute(sql)
            <span class="hljs-comment"># 获取所有记录列表</span>
            results = conn.fetchall()
            <span class="hljs-keyword">if</span> len(results) &gt; <span class="hljs-number">0</span>:  <span class="hljs-comment">## 存在</span>
                <span class="hljs-comment">#print("此ip已经存在@@@@@@@@@@@@")</span>
                <span class="hljs-keyword">return</span> <span class="hljs-keyword">True</span>
        <span class="hljs-keyword">except</span>:
            <span class="hljs-keyword">return</span> <span class="hljs-keyword">False</span>
        <span class="hljs-keyword">return</span> <span class="hljs-keyword">False</span>

    <span class="hljs-comment">##判断代理ip是否可用</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">proxyIpCheck</span><span class="hljs-params">(self,ip, port)</span>:</span>
        server = ip + <span class="hljs-string">":"</span> + str(port)
        proxies = {<span class="hljs-string">'http'</span>: <span class="hljs-string">'http://'</span> + server, <span class="hljs-string">'https'</span>: <span class="hljs-string">'https://'</span> + server}
        <span class="hljs-keyword">try</span>:
            r = requests.get(<span class="hljs-string">'https://www.baidu.com/'</span>, proxies=proxies, timeout=<span class="hljs-number">1</span>)
            <span class="hljs-keyword">if</span> (r.status_code == <span class="hljs-number">200</span>):
                <span class="hljs-keyword">return</span> <span class="hljs-keyword">True</span>
            <span class="hljs-keyword">else</span>:
                <span class="hljs-keyword">return</span> <span class="hljs-keyword">False</span>
        <span class="hljs-keyword">except</span>:
            <span class="hljs-keyword">return</span> <span class="hljs-keyword">False</span></code></pre>
<p>2.6 测试爬虫 scrapy crwal 爬虫名</p>
<p><strong>3. 到此我们的 提取代理ip到数据库的 爬虫就写好了,接下来就是我们的任务定时器的编写</strong></p>
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment">#####在我们的爬虫项目的settings.py文件的同级目录新建一个start.py文件</span>


<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> pymysql
<span class="hljs-keyword">import</span> threading
<span class="hljs-keyword">from</span> settings <span class="hljs-keyword">import</span> *

<span class="hljs-comment">##定时器调用的run方法</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run</span><span class="hljs-params">()</span>:</span>
    clearIpPool()
    <span class="hljs-comment">### 循环定时器，不然执行一次就over了</span>
    timer = threading.Timer(TIMER_STOP_TIME, run)
    timer.start()

<span class="hljs-comment">########从这里开始执行</span>
print(<span class="hljs-string">"ip池定时器开始，间隔时间："</span>,str(TIMER_STOP_TIME),<span class="hljs-string">'s'</span>)
<span class="hljs-comment">########开启定时器 TIMER_STOP_TIME为settings.py中的配置</span>
timer = threading.Timer(TIMER_STOP_TIME,run)
timer.start()


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">clearIpPool</span><span class="hljs-params">()</span>:</span>
    print(<span class="hljs-string">"定时器执行，清扫ip数据库池"</span>)

    <span class="hljs-comment">## 利用 系统scrapy命令重新爬取代理ip</span>
    os.system(<span class="hljs-string">'scrapy crawl xiciSpider --nolog'</span>)

    <span class="hljs-comment"># 遍历数据库 去除无用的代理ip</span>
    removeUnSafeProxyFromDB()
    print(<span class="hljs-string">"定时器执行完毕"</span>)

<span class="hljs-comment">###### 查询数据库，找出无用的代理ip并且删除</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">removeUnSafeProxyFromDB</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-comment"># 打开数据库连接</span>
    db = pymysql.connect(<span class="hljs-string">"39.108.112.254"</span>, <span class="hljs-string">"root"</span>, <span class="hljs-string">"abc123|||456"</span>, <span class="hljs-string">"xici"</span>)
    <span class="hljs-comment"># 使用cursor()方法获取操作游标</span>
    cursor = db.cursor()
    <span class="hljs-comment"># SQL 查询语句</span>
    sql = <span class="hljs-string">"SELECT * FROM t_ips"</span>
    <span class="hljs-keyword">try</span>:
        <span class="hljs-comment"># 执行SQL语句</span>
        cursor.execute(sql)
        <span class="hljs-comment"># 获取所有记录列表</span>
        results = cursor.fetchall()
        <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> results:
            id = row[<span class="hljs-number">0</span>]
            ip = row[<span class="hljs-number">1</span>]
            port = row[<span class="hljs-number">2</span>]
            <span class="hljs-keyword">if</span> proxyIpCheck(ip, str(port)) <span class="hljs-keyword">is</span> <span class="hljs-keyword">False</span>:
                print(<span class="hljs-string">"此代理ip不可用，proxy:"</span>,ip, <span class="hljs-string">':'</span>, str(port))
                <span class="hljs-comment">## 执行删除</span>
                sql = <span class="hljs-string">"DELETE FROM t_ips WHERE id = "</span>+str(id)
                <span class="hljs-comment"># 执行SQL语句</span>
                cursor.execute(sql)
                print(sql)
                <span class="hljs-comment"># 提交修改</span>
                db.commit()
                <span class="hljs-keyword">return</span>
    <span class="hljs-keyword">except</span>:
        print(<span class="hljs-string">"Error: unable to fetch data"</span>)

    <span class="hljs-comment"># 关闭数据库连接</span>
    db.close()

<span class="hljs-comment">#####检测代理ip是否可用</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">proxyIpCheck</span><span class="hljs-params">(ip, port)</span>:</span>
    server = ip + <span class="hljs-string">":"</span> + str(port)
    proxies = {<span class="hljs-string">'http'</span>: <span class="hljs-string">'http://'</span> + server, <span class="hljs-string">'https'</span>: <span class="hljs-string">'https://'</span> + server}
    <span class="hljs-keyword">try</span>:
        r = requests.get(<span class="hljs-string">'https://www.baidu.com/'</span>, proxies=proxies, timeout=<span class="hljs-number">1</span>)
        <span class="hljs-keyword">if</span> (r.status_code == <span class="hljs-number">200</span>):
            <span class="hljs-keyword">return</span> <span class="hljs-keyword">True</span>
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">return</span> <span class="hljs-keyword">False</span>
    <span class="hljs-keyword">except</span>:
        <span class="hljs-keyword">return</span> <span class="hljs-keyword">False</span></code></pre>
<p><strong>4.如果你是windows平台，cmd执行运行 python start.py , 任务就会一直执行，除非关掉cmd</strong> <br /> <strong>5.如果你是linux平台，最好编写一个.sh文件 来执行，还可以把这个.sh搞成开机启动等等</strong></p>
<p>6.我的结果图 <br /> <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170913175334392?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTHVvWmhlbmc0Njk4NzI5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170913175418242?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvTHVvWmhlbmc0Njk4NzI5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述" title=""></p>
<p>这里我就爬取了一页数据，大家可以修改settings.py的MAX_PAGE值，此值就是要爬取的页数。谢谢大家，有啥问题加我qq讨论。</p>
<p><strong>老生常谈：深圳有爱好音乐的会打鼓（吉他，键盘，贝斯等）的程序员和其它职业可以一起交流加入我们乐队一起嗨。我的QQ：657455400</strong></p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-526ced5128.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>从信息泄密谈到爬虫</title>
		<link>https://uzzz.org/article/2207.html</link>
				<pubDate>Mon, 20 Mar 2017 01:33:50 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[爬虫]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2207.html</guid>
				<description><![CDATA[转载地址：http://www.hackbase.com/article-216889-1.html 从信息泄密谈到爬虫 2017-3-17 11:16&#124;投稿: xiaotiger&#124;来自: 互联网 摘要: 2016年8月，一位自称“Peace”的黑客声称盗取了2亿雅虎用户账户和密码，并寻求在暗网(dark web)上进行售卖。黑客所声称的2亿条信息的泄露似乎盗取自2012年，同时发生的还有MySpace（3.6亿条）和Linkedln（1亿条）两 &#8230; 2016年8月，一位自称“Peace”的黑客声称盗取了2亿雅虎用户账户和密码，并寻求在暗网(dark web)上进行售卖。黑客所声称的2亿条信息的泄露似乎盗取自2012年，同时发生的还有MySpace（3.6亿条）和Linkedln（1亿条）两家网站的信息泄露。 有趣的是 Linkedln 的泄露事件还间接导致了扎克伯格的推特账号被黑。因为扎克伯格在两个网站都使用了同一个密码：“dadada”…… 在信息化时代，数据泄露无处不在，这种风险可能来自于我们上网的每一个步骤。下面笔者将介绍一种批量获取信息的方式——爬虫。编程语言基于Python，如果对这门语言不是很熟悉可以先了解下它的语法结构。本文将对于爬虫做一个简单入门介绍。 关于爬虫 我们一直在说的爬虫究竟是个什么鬼？ 网络爬虫（web crawler），是一个自动提取网页的程序，它为搜索引擎从网路上下载网页。传统爬虫从一个或若干初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列，直到满足系统的一定停止条件。另外，所有被爬虫抓取的网页将会被系统存贮，进行一定的分析、过滤，并建立索引，以便之后的查询和检索。 （摘自百度百科） 简单来讲，爬虫是通过程序或者脚本获取网页上的一些文本、图片、音频的数据。 从笔者的经验来看，做一个简单的爬虫程序有以下几个步骤：确立需求、网页下载、网页分析与解析、保存。接下来大家可以跟随笔者的流程，我们来写个抓取豆瓣书籍信息的爬虫。 1、需求 以豆瓣读书为例，我们爬取豆瓣的书籍信息，需要获取的信息包括：图书名称，出版社，作者，年份，评分。 2、网页下载 页面下载分为静态和动态两种下载方式。 静态主要是纯 html 页面，动态是网页会使用 javascript 处理，并通过Ajax 异步获取的页面。在这里，我们下载的是静态页面。 在下载网页的过程中我们需要用到网络库。在 Python 中有自带的 urllib、urllib2 网络库，但是我们一般采用基于 urllib3 的第三方库Requests ，这是一个深受 Pythoner 喜爱的更为高效简洁的网络库，能满足我们目前的 web 需求。 3、网页分析与解析 1）网页分析： 选好网络库后我们需要做的是：分析我们要爬取的路径——也就是逻辑。 这个过程中我们要找到爬取的每一个入口，例如豆瓣读书的页面。已知图书标签的 url，点击每个 url 能得到图书列表，在图书列表中存放需要的图书信息，求解如何获得图书信息。 所以很简单！我们的爬取路径就是：图书标签 url —&#62; 图书列表—&#62;图书信息。 2）网页解析： 网页解析主要就是通过解析网页源代码获取我们需要的数据，网页解析的方式有很多种，如：正则表达式， BeautifulSoup， XPath 等等，在这里我们采用的是 XPath。Xpath 的语法很简单，是根据路径来进行定位。 举个栗子：上海的位置是 地球—中国—上海，语法表达为 //地球/中国[@城市名=上海] 接下来我们需要解析网页获取到图书的 tag 标签的url。打开网页，右击选择审查元素，然后就会出现调试工具，左上角点击获取我们需要的数据，下面的调试窗口就会直接定位到其所在代码。 根据其位置，写出其 Xpath 解析式：//table[@class=&#8217;tagCol&#8217;]//a 这里我们看到小说在一个&#60; table &#62;标签下的&#60; td &#62;标签的&#60; a &#62;标签里。&#60; table &#62; 标签可以用 class 属性进行定位。 以下是获取 tag 的 url 的代码： 获取完了 tag ，我们还需要获取到图书的信息，下面我们对图书列表页进行解析： 解析之后代码如下： 爬取的信息内容如下： 4、数据保存 获取到了数据之后，我们可以选择把数据保存在数据库中，或者直接写在文件中。这里我们把数据保存到了 mongodb。接下来做一些统计，例如使用图表插件 echarts，将我们的统计结果展示出来。 5、爬虫相关问题 1）网站限制： 爬虫过程中可能会遇到爬不到数据了的问题，这是因为相应网站做了一些反爬的处理来进行爬取限制，比如在爬取豆瓣的时候，就遇到了 403forbidden 。怎么办？这时候可以通过一些相应的方法来解决，比如使用代理服务器，降低爬取速度等，在这里我们采用每次请求 sleep2秒。 2）URL 去重： URL 去重是爬虫运行中一项关键的步骤，由于运行中的爬虫主要阻塞在网络交互中，因此避免重复的网络交互至关重要。爬虫一般会将待抓取的 URL 放在一个队列中，从抓取后的网页中提取到新的 URL，在他们被放入队列之前，首先要确定这些新的 URL 没有被抓取过，如果之前已经抓取过了，就不再放入队列了。 3）并发操作： Python 中的并发操作主要涉及的模型有：多线程模型、多进程模型、协程模型。在 Python 中可以通过：threading 模块、multiprocessing 模块、gevent 库 来实现多线程、多进程、或协程的并发操作。 scrapy —— 强大的爬虫框架 谈到爬虫，不得不提的是 Scrapy。Scrapy 是 Python 开发的一个快速，高层次的爬虫框架，用于抓取 web 站点并从页面中提取结构化的数据。Scrapy 用途广泛，可以用于数据挖掘、监测和自动化测试。 Scrapy 吸引人的地方在于它是一个框架，任何人都可以根据需求方便的修改。它也提供了多种类型爬虫的基类，如 BaseSpider、sitemap 爬虫等。 scrapy 的架构：请点击此处输入图片描述 其中绿线是数据流向，首先从初始 URL 开始，Scheduler 会将其交给 Downloader 进行下载，下载之后会交给 Spider 进行分析，需要保存的数据则会被送到 Item Pipeline，对数据进行后期处理。 另外，在数据流动的通道里还可以安装各种中间件，进行必要的处理。 因此在开发爬虫的时候，最好也先规划好各种模块。 注： Xpath 教程：http://www.w3school.com.cn/xpath/index.asp Requests官方文档：http://docs.python-requests.org/en/master/ 更多的 Scrapy 请参考：http://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html 本文作者：胡宇涵（点融黑帮），就职于点融网工程部infra团队运维开发工程师。爱自然，爱生活。 本文由@点融黑帮（ID：DianrongMafia）原创发布于今日头条，未经许可，禁止转载。 声明：本文搜集整理自互联网，版权归原作者所有，文中所述不代表本站观点，若有侵权或转载等不当之处请联系我们处理，请我们一起为维护良好的互联网秩序而努力，谢谢！联系方式见网站首页右下角。]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
<div class="mn">
<div class="bm vw">
<div class="h hm">
<h1 class="ph">转载地址：http://www.hackbase.com/article-216889-1.html</h1>
<h1 class="ph">从信息泄密谈到爬虫 </h1>
<p class="xg1">2017-3-17 11:16<span class="pipe">|</span>投稿: <a href="http://hackbase.com/space-uid-5183395.html" rel="nofollow" data-token="23340c90429cf5ae8a31136b3836b1cf"> xiaotiger</a><span class="pipe">|</span>来自: <a href="http://www.toutiao.com/a6398014451433537793/" rel="nofollow" data-token="4085046613058af932e1fffa9fd0aaa4"> 互联网</a></p>
</p></div>
<div id="diysummarytop" class="area"></div>
<div class="s">
<div>
      <strong>摘要</strong>: 2016年8月，一位自称“Peace”的黑客声称盗取了2亿雅虎用户账户和密码，并寻求在暗网(dark web)上进行售卖。黑客所声称的2亿条信息的泄露似乎盗取自2012年，同时发生的还有MySpace（3.6亿条）和Linkedln（1亿条）两 &#8230;
     </div>
</p></div>
<div id="diysummarybottom" class="area"></div>
<div class="d">
<div id="diycontenttop" class="area"></div>
<table cellpadding="0" cellspacing="0" class="vwtb">
<tbody>
<tr>
<td id="article_content">
<div>
<p>2016年8月，一位自称“Peace”的黑客声称盗取了2亿雅虎用户账户和密码，并寻求在暗网(dark web)上进行售卖。黑客所声称的2亿条信息的泄露似乎盗取自2012年，同时发生的还有MySpace（3.6亿条）和Linkedln（1亿条）两家网站的信息泄露。</p>
<p>有趣的是 Linkedln 的泄露事件还间接导致了扎克伯格的推特账号被黑。因为扎克伯格在两个网站都使用了同一个密码：“dadada”……</p>
<p>在信息化时代，数据泄露无处不在，这种风险可能来自于我们上网的每一个步骤。下面笔者将介绍一种批量获取信息的方式——爬虫。编程语言基于Python，如果对这门语言不是很熟悉可以先了解下它的语法结构。本文将对于爬虫做一个简单入门介绍。</p>
<h1>关于爬虫</h1>
<p>我们一直在说的爬虫究竟是个什么鬼？</p>
<blockquote>
<p>网络爬虫（web crawler），是一个自动提取网页的程序，它为搜索引擎从网路上下载网页。传统爬虫从一个或若干初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列，直到满足系统的一定停止条件。另外，所有被爬虫抓取的网页将会被系统存贮，进行一定的分析、过滤，并建立索引，以便之后的查询和检索。</p>
<p>（摘自百度百科）</p>
</blockquote>
<p>简单来讲，爬虫是通过程序或者脚本获取网页上的一些<strong>文本、图片、音频</strong>的数据。</p>
<p>从笔者的经验来看，做一个简单的爬虫程序有以下几个步骤：确立需求、网页下载、网页分析与解析、保存。接下来大家可以跟随笔者的流程，我们来写个抓取豆瓣书籍信息的爬虫。</p>
<p>1、<strong>需求</strong></p>
<p>以豆瓣读书为例，我们爬取豆瓣的书籍信息，需要获取的信息包括：图书名称，出版社，作者，年份，评分。</p>
<p><img src="http://www.hackbase.com/data/attachment/portal/201703/17/muZxJ9JFiI7ij77BY19B3i.jpg" alt=""></p>
<p>2、<strong>网页下载</strong></p>
<p>页面下载分为静态和动态两种下载方式。</p>
<p>静态主要是纯 html 页面，动态是网页会使用 javascript 处理，并通过Ajax 异步获取的页面。在这里，我们下载的是静态页面。</p>
<p>在下载网页的过程中我们需要用到网络库。在 Python 中有自带的 urllib、urllib2 网络库，但是我们一般采用基于 urllib3 的第三方库Requests ，这是一个深受 Pythoner 喜爱的更为高效简洁的网络库，能满足我们目前的 web 需求。</p>
<p>3、<strong>网页分析与解析</strong></p>
<p><strong>1）网页分析：</strong></p>
<p>选好网络库后我们需要做的是：分析我们要爬取的路径——也就是逻辑。</p>
<p>这个过程中我们要找到爬取的每一个入口，例如豆瓣读书的页面。已知图书标签的 url，点击每个 url 能得到图书列表，在图书列表中存放需要的图书信息，求解如何获得图书信息。</p>
<p>所以很简单！我们的爬取路径就是：图书标签 url —&gt; 图书列表—&gt;图书信息。</p>
<p><strong>2）网页解析：</strong></p>
<p>网页解析主要就是通过<strong>解析网页源代码</strong>获取我们需要的数据，网页解析的方式有很多种，如：正则表达式， BeautifulSoup， XPath 等等，在这里我们采用的是 XPath。Xpath 的语法很简单，是根据路径来进行定位。</p>
<p><img src="http://www.hackbase.com/data/attachment/portal/201703/17/eg2mN1tmNwgfg1mfnUGFUg.jpg" alt=""></p>
<p>举个栗子：上海的位置是 地球—中国—上海，语法表达为 //地球/中国[@城市名=上海]</p>
<p>接下来我们需要解析网页获取到图书的 tag 标签的<strong>url</strong>。打开网页，右击选择审查元素，然后就会出现调试工具，左上角点击获取我们需要的数据，下面的调试窗口就会直接定位到其所在代码。</p>
<p><img src="http://www.hackbase.com/data/attachment/portal/201703/17/Xs786QZUlaSw5j8SL68kAu.jpg" alt=""></p>
<p>根据其位置，写出其 Xpath 解析式：//table[@class=&#8217;tagCol&#8217;]//a</p>
<p>这里我们看到小说在一个&lt; table &gt;标签下的&lt; td &gt;标签的&lt; a &gt;标签里。&lt; table &gt; 标签可以用 class 属性进行定位。</p>
<p><img src="http://www.hackbase.com/data/attachment/portal/201703/17/k6o75O6x73EFOD3o86xoql.jpg" alt=""></p>
<p>以下是获取 tag 的 url 的代码：</p>
<p><img src="http://www.hackbase.com/data/attachment/portal/201703/17/YlUYZHpzlIy9tlHYHdILll.jpg" alt=""></p>
<p><img src="http://www.hackbase.com/data/attachment/portal/201703/17/oWME18888831pk443XqW8e.jpg" alt=""></p>
<p>获取完了 tag ，我们还需要获取到图书的信息，下面我们对图书列表页进行解析：</p>
<p><img src="http://www.hackbase.com/data/attachment/portal/201703/17/IYw62tCm7Gh6lAt66Zeu6C.jpg" alt=""></p>
<p>解析之后代码如下：</p>
<p><img src="http://www.hackbase.com/data/attachment/portal/201703/17/Pshn4b1nANHO4SY9z6B4Yy.jpg" alt=""></p>
<p>爬取的信息内容如下：</p>
<p><img src="http://www.hackbase.com/data/attachment/portal/201703/17/aO0R0PHt9OPj10WD9DARdp.jpg" alt=""></p>
<p>4、<strong>数据保存</strong></p>
<p>获取到了数据之后，我们可以选择把数据保存在数据库中，或者直接写在文件中。这里我们把数据保存到了 mongodb。接下来做一些统计，例如使用图表插件 echarts，将我们的统计结果展示出来。</p>
<p>5、<strong>爬虫相关问题</strong></p>
<p><strong>1）网站限制：</strong></p>
<p>爬虫过程中可能会遇到爬不到数据了的问题，这是因为相应网站做了一些反爬的处理来进行爬取限制，比如在爬取豆瓣的时候，就遇到了 403forbidden 。怎么办？这时候可以通过一些相应的方法来解决，比如使用代理服务器，降低爬取速度等，在这里我们采用每次请求 sleep2秒。</p>
<p><strong>2）URL 去重：</strong></p>
<p>URL 去重是爬虫运行中一项关键的步骤，由于运行中的爬虫主要阻塞在网络交互中，因此避免重复的网络交互至关重要。爬虫一般会将待抓取的 URL 放在一个队列中，从抓取后的网页中提取到新的 URL，在他们被放入队列之前，首先要确定这些新的 URL 没有被抓取过，如果之前已经抓取过了，就不再放入队列了。</p>
<p><strong>3）并发操作：</strong></p>
<p>Python 中的并发操作主要涉及的模型有：多线程模型、多进程模型、协程模型。在 Python 中可以通过：threading 模块、multiprocessing 模块、gevent 库 来实现多线程、多进程、或协程的并发操作。</p>
<h1>scrapy —— 强大的爬虫框架</h1>
<p>谈到爬虫，不得不提的是 Scrapy。Scrapy 是 Python 开发的一个快速，高层次的爬虫框架，用于抓取 web 站点并从页面中提取结构化的数据。Scrapy 用途广泛，可以用于数据挖掘、监测和自动化测试。</p>
<p>Scrapy 吸引人的地方在于它是一个框架，任何人都可以根据需求方便的修改。它也提供了多种类型爬虫的基类，如 BaseSpider、sitemap 爬虫等。</p>
<p><strong>scrapy 的架构：<img src="http://www.hackbase.com/data/attachment/portal/201703/17/ejhtvV0e89Ed09HiLhLG09.jpg" alt="">请点击此处输入图片描述</strong></p>
<p>其中绿线是数据流向，首先从初始 URL 开始，Scheduler 会将其交给 Downloader 进行下载，下载之后会交给 Spider 进行分析，需要保存的数据则会被送到 Item Pipeline，对数据进行后期处理。</p>
<p>另外，在数据流动的通道里还可以安装各种中间件，进行必要的处理。 因此在开发爬虫的时候，最好也先规划好各种模块。</p>
<p><strong>注：</strong></p>
<p>Xpath 教程：http://www.w3school.com.cn/xpath/index.asp</p>
<p>Requests官方文档：http://docs.python-requests.org/en/master/</p>
<p>更多的 Scrapy 请参考：http://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html</p>
<p><strong>本文作者</strong>：胡宇涵（点融黑帮），就职于点融网工程部infra团队运维开发工程师。爱自然，爱生活。</p>
<p>本文由<strong>@点融黑帮（ID：DianrongMafia）</strong>原创发布于今日头条，未经许可，禁止转载。</p>
</p></div>
</td>
</tr>
<tr>
<td></p>
<p><span style="color:#FF5050;">声明：本文搜集整理自互联网，版权归原作者所有，文中所述不代表本站观点，若有侵权或转载等不当之处请联系我们处理，请我们一起为维护良好的互联网秩序而努力，谢谢！联系方式见网站首页右下角。</span></p>
</td>
</tr>
</tbody>
</table>
<div id="diycontentbottom" class="area"></div>
<div id="click_div">
      
     </div>
</p></div>
</p></div>
<div id="diycontentcomment" class="area"></div>
</p></div>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>爬虫第二弹——隐网爬虫指南，AcFun评论爬取教程</title>
		<link>https://uzzz.org/article/2236.html</link>
				<pubDate>Sun, 03 Jul 2016 09:38:05 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[爬虫]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2236.html</guid>
				<description><![CDATA[爬虫第一弹：利用Scrapy爬取1905电影网 啊啊啊！！！！写完没保存！！！！还得重新写一遍！！！！！好气啊！！！！！！ 前言 AJAX即“Asynchronous Javascript And XML”（异步JavaScript和XML），是指一种创建交互式网页应用的网页开发技术。 通过在后台与服务器进行少量数据交换，AJAX 可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。 传统的网页（不使用 AJAX）如果需要更新内容，必须重载整个网页页面。 因此现在有很多网站都是用Ajax进行前后端数据交互的。 然而正是因为很多网站使用Ajax导致网络爬虫无法跟进。 这时就需要我们针对不同的网站定制爬虫。 AcFun 评论数据传输分析 页面分析 首先测试爬去使用Ajax传输数据的网站的效果，我们以AcFun视频为例。 打开页面http://www.acfun.tv/v/ac2860882，下图所示是当前页面是直接在浏览器打开的效果，可以看到红色框内的就是当前页面的评论。 下图是通过爬虫爬去该网页的结果，可以看到红色框内并没有评论，因为当前评论没有传过来。 寻找数据 我们现在利用Chrome浏览器的Developer Tools寻找数据。 打开原页面，在当前页面上邮件选择检查，进入到Developer Tools后选择Network。在Network中选择XHR(数据)，再刷新当前页面，从左侧列表中寻找评论数据（注意：数据需要从右侧栏需选择Preview栏才可以看到） 构建评论url 我们发现评论是通过comment_list_json.aspx页面发送过来的。 这时我们点开该页面的Query String Parameters查看参数 这个json请求一共有两个参数，且其含义也不难理解 contentId(2860882)：当前页面编号，与当前页面URL（http://www.acfun.tv/v/ac2860882）相同 currentPage(1)：评论页号 于是这个json请求的url如下所示： http://www.acfun.tv/comment_list_json.aspx?contentId=%1&#38;currentPage=%2 得知url的形式后就可以开始写爬虫了！ 程序 理论上所有图灵完备的语言都可以写爬虫，但为了效率与保持心情愉悦我推荐用Python。 我平时喜欢用Scrapy或urllib2写，Scrapy适合中到大规模且爬取逻辑简单的爬虫，如果遇到非常复杂或者是非常简单的爬虫，那么就需要用urllib2定制了，想学Scrapy可以看我的爬虫第一弹 本文致力于一切从简的思想，使用urllib2写一个小的Demo. #encoding=utf-8 import urllib2 import json def get_page(url): try: timeout = 5 request = urllib2.Request(url) #伪装HTTP请求 request.add_header('User-agent', 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36') request.add_header('connection','keep-alive') request.add_header('referer', url) # request.add_header('Accept-Encoding', 'gzip') # gzip可提高传输速率，但占用计算资源 response = urllib2.urlopen(request, timeout = timeout) html = response.read() #if(response.headers.get('content-encoding', None) == 'gzip'): # html = gzip.GzipFile(fileobj=StringIO.StringIO(html)).read() response.close() return html except Exception as e: print 'URL Request Error:', e return None def anlysis_json(json_data): s = json.loads(json_data) # 如果爬取失败，则返回 if not s['success']: return # commentList中保存评论的编号 # commentContentArr中是以 c + 评论编号 作为索引的 # 因此需要先从commentList中获取编号 comment_id_list = s['data']['commentList'] for id in comment_id_list: id = 'c'+str(int(id)) if s['data']['commentContentArr'][id]['isDelete']: continue print str(s['data']['commentContentArr'][id]['content']).encode('utf-8') if __name__ == '__main__': contentID = 2860882 # 页面ID currentPage = 1 # 评论页ID basic_url = 'http://www.acfun.tv/comment_list_json.aspx?contentId=%d&#38;currentPage=%d' url = basic_url % (contentID,currentPage) json_data = get_page(url) anlysis_json(json_data) 结果如下图所示 总结 爬虫是一门很有意思的技术，学习曲线就像y=x的图像一样，好入门，坡度也不陡，但想要十分精通也很难。我能力有限，只是想将知识分享出来大家一起讨论进步。 爬虫下一弹我打算做Scrapy简单网站的登陆，也不知道啥时候能做出来，慢慢做吧。]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<p>爬虫第一弹：<a href="http://blog.csdn.net/wds2006sdo/article/details/51210896" rel="nofollow" data-token="7fdb0731a36fb87c505a71c447c44969">利用Scrapy爬取1905电影网</a></p>
<p>啊啊啊！！！！写完没保存！！！！还得重新写一遍！！！！！好气啊！！！！！！</p>
<h1 id="前言">前言</h1>
<p>AJAX即“Asynchronous Javascript And XML”（异步JavaScript和XML），是指一种创建交互式网页应用的网页开发技术。 <br /> 通过在后台与服务器进行少量数据交换，AJAX 可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。 <br /> 传统的网页（不使用 AJAX）如果需要更新内容，必须重载整个网页页面。 <br /> 因此现在有很多网站都是用Ajax进行前后端数据交互的。</p>
<p>然而正是因为很多网站使用Ajax导致网络爬虫无法跟进。 <br /> 这时就需要我们针对不同的网站定制爬虫。</p>
<h1 id="acfun-评论数据传输分析">AcFun 评论数据传输分析</h1>
<h2 id="页面分析">页面分析</h2>
<p>首先测试爬去使用Ajax传输数据的网站的效果，我们以AcFun视频为例。</p>
<p>打开页面<a href="http://www.acfun.tv/v/ac2860882" rel="nofollow" data-token="f92155f3ac0e7aad16dc8d132feecc6a">http://www.acfun.tv/v/ac2860882</a>，下图所示是当前页面是直接在浏览器打开的效果，可以看到红色框内的就是当前页面的评论。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20160704000925075" alt="这里写图片描述" title=""></p>
<p>下图是通过爬虫爬去该网页的结果，可以看到红色框内并没有评论，因为当前评论没有传过来。 <br /> <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20160704001047671" alt="这里写图片描述" title=""></p>
<h2 id="寻找数据">寻找数据</h2>
<p>我们现在利用Chrome浏览器的Developer Tools寻找数据。 <br /> 打开原页面，在当前页面上邮件选择检查，进入到Developer Tools后选择Network。在Network中选择XHR(数据)，再刷新当前页面，从左侧列表中寻找评论数据（注意：数据需要从右侧栏需选择Preview栏才可以看到） <br /> <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20160704001450681" alt="这里写图片描述" title=""></p>
<h2 id="构建评论url">构建评论url</h2>
<p>我们发现评论是通过comment_list_json.aspx页面发送过来的。 <br /> 这时我们点开该页面的Query String Parameters查看参数</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20160704003831775" alt="这里写图片描述" title=""></p>
<p>这个json请求一共有两个参数，且其含义也不难理解</p>
<ul>
<li>contentId(2860882)：当前页面编号，与当前页面URL（<a href="http://www.acfun.tv/v/ac2860882" rel="nofollow" data-token="f92155f3ac0e7aad16dc8d132feecc6a">http://www.acfun.tv/v/ac2860882</a>）相同</li>
<li>currentPage(1)：评论页号</li>
</ul>
<p>于是这个json请求的url如下所示： <br /> <a href="http://www.acfun.tv/comment_list_json.aspx?contentId=%1&amp;currentPage=%2" rel="nofollow" data-token="497105d3d65b50d747359265cbf2e36f">http://www.acfun.tv/comment_list_json.aspx?contentId=%1&amp;currentPage=%2</a></p>
<p>得知url的形式后就可以开始写爬虫了！</p>
<h1 id="程序">程序</h1>
<p>理论上所有图灵完备的语言都可以写爬虫，但为了效率与保持心情愉悦我推荐用Python。</p>
<p>我平时喜欢用Scrapy或urllib2写，Scrapy适合中到大规模且爬取逻辑简单的爬虫，如果遇到非常复杂或者是非常简单的爬虫，那么就需要用urllib2定制了，想学Scrapy可以看我的<a href="http://blog.csdn.net/wds2006sdo/article/details/51210896" rel="nofollow" data-token="7fdb0731a36fb87c505a71c447c44969">爬虫第一弹</a></p>
<p>本文致力于一切从简的思想，使用urllib2写一个小的Demo.</p>
<pre class="prettyprint"><code class=" hljs python"><span class="hljs-comment">#encoding=utf-8</span>

<span class="hljs-keyword">import</span> urllib2
<span class="hljs-keyword">import</span> json

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_page</span><span class="hljs-params">(url)</span>:</span>
    <span class="hljs-keyword">try</span>:
        timeout = <span class="hljs-number">5</span>
        request = urllib2.Request(url)
        <span class="hljs-comment">#伪装HTTP请求</span>
        request.add_header(<span class="hljs-string">'User-agent'</span>, <span class="hljs-string">'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'</span>)
        request.add_header(<span class="hljs-string">'connection'</span>,<span class="hljs-string">'keep-alive'</span>)
        request.add_header(<span class="hljs-string">'referer'</span>, url)
        <span class="hljs-comment"># request.add_header('Accept-Encoding', 'gzip') # gzip可提高传输速率，但占用计算资源</span>
        response = urllib2.urlopen(request, timeout = timeout)
        html = response.read()
        <span class="hljs-comment">#if(response.headers.get('content-encoding', None) == 'gzip'):</span>
        <span class="hljs-comment"># html = gzip.GzipFile(fileobj=StringIO.StringIO(html)).read()</span>
        response.close()
        <span class="hljs-keyword">return</span> html
    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        <span class="hljs-keyword">print</span> <span class="hljs-string">'URL Request Error:'</span>, e
        <span class="hljs-keyword">return</span> <span class="hljs-keyword">None</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">anlysis_json</span><span class="hljs-params">(json_data)</span>:</span>
    s = json.loads(json_data)

    <span class="hljs-comment"># 如果爬取失败，则返回</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> s[<span class="hljs-string">'success'</span>]:
        <span class="hljs-keyword">return</span>

    <span class="hljs-comment"># commentList中保存评论的编号</span>
    <span class="hljs-comment"># commentContentArr中是以 c + 评论编号 作为索引的</span>
    <span class="hljs-comment"># 因此需要先从commentList中获取编号</span>
    comment_id_list = s[<span class="hljs-string">'data'</span>][<span class="hljs-string">'commentList'</span>]

    <span class="hljs-keyword">for</span> id <span class="hljs-keyword">in</span> comment_id_list:
        id = <span class="hljs-string">'c'</span>+str(int(id))
        <span class="hljs-keyword">if</span> s[<span class="hljs-string">'data'</span>][<span class="hljs-string">'commentContentArr'</span>][id][<span class="hljs-string">'isDelete'</span>]:
            <span class="hljs-keyword">continue</span>

        <span class="hljs-keyword">print</span> str(s[<span class="hljs-string">'data'</span>][<span class="hljs-string">'commentContentArr'</span>][id][<span class="hljs-string">'content'</span>]).encode(<span class="hljs-string">'utf-8'</span>)

<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">'__main__'</span>:
    contentID = <span class="hljs-number">2860882</span>     <span class="hljs-comment"># 页面ID</span>
    currentPage = <span class="hljs-number">1</span>         <span class="hljs-comment"># 评论页ID</span>
    basic_url = <span class="hljs-string">'http://www.acfun.tv/comment_list_json.aspx?contentId=%d&amp;currentPage=%d'</span>

    url = basic_url % (contentID,currentPage)

    json_data = get_page(url)
    anlysis_json(json_data)</code></pre>
<p>结果如下图所示 <br /> <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20160704003045329" alt="这里写图片描述" title=""></p>
<h1 id="总结">总结</h1>
<p>爬虫是一门很有意思的技术，学习曲线就像y=x的图像一样，好入门，坡度也不陡，但想要十分精通也很难。我能力有限，只是想将知识分享出来大家一起讨论进步。</p>
<p>爬虫下一弹我打算做Scrapy简单网站的登陆，也不知道啥时候能做出来，慢慢做吧。</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-526ced5128.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
	</channel>
</rss>
