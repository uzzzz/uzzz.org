<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>网络优化 &#8211; 有组织在!</title>
	<atom:link href="https://uzzz.org/category/wangluoyouhua/feed" rel="self" type="application/rss+xml" />
	<link>https://uzzz.org/</link>
	<description></description>
	<lastBuildDate>Thu, 21 Mar 2019 10:42:48 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.4</generator>

<image>
	<url>https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png</url>
	<title>网络优化 &#8211; 有组织在!</title>
	<link>https://uzzz.org/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>学习笔记：匿名通信与暗网研究综述</title>
		<link>https://uzzz.org/article/1833.html</link>
				<pubDate>Thu, 21 Mar 2019 10:42:48 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[网络优化]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/1833.html</guid>
				<description><![CDATA[本文仅为作者学习笔记，内容源自论文“匿名通信与暗网研究综述——罗军舟等”本身以及相关网络搜索 1、匿名通信与暗网 匿名通信指采取一定的措施隐蔽通信流中的通信关系，使窃听者难以获取或推知通信双方的关系及内容。匿名通信的目的就是隐蔽通信双方的身份或通信关系，保护网络用户的个人通信隐私。——MBA智库百科 匿名通信是一项技术，用于保护通信双方的隐私，使窃听者无法获取通信双方的关系及内容。匿名通信系统是提供匿名通信服务的一套完整网络，主要由提供加密服务的节点组成。暗网是匿名通信系统的其中一种表现形式，利用隐藏服务机制，使监管者无法有效监控暗网中的非法活动。 学术界对匿名通信系统研究的2个方向 专注于匿名通信系统本身，分析其安全性并提出相关的隐私增强技术：包括 匿名接入、 匿名路由、 隐藏服务等机制的优化， 流量混淆和 协议伪装等技术的设计和应用。 针对匿名通信系统和暗网的攻击和监管：利用流量分析，系统/协议漏洞分析，提出去匿名化的攻击方法，包括 暗网隐藏节点发现， 隐藏服务定位， 暗网用户的网络行为分析， 暗网流量追踪， 通信关系确认等。 2、匿名通信系统的分类和基本工作原理：Tor、I2P、Freenet、Zeronet 3、匿名通信关键技术 3-1、匿名接入：bridge、meek、FTE、Flashproxy 3-2、匿名路由：洋葱路由、大蒜路由、基于DHT的路由 3-3、暗网服务技术：Tor、I2P、Freenet、Zeronet 4、匿名通信攻防研究 4-1、匿名通信攻击技术：主动攻击、被动攻击、单端攻击、端到端攻击 4-2、匿名通信增强技术 目前已存在一些用来防御各种攻击的研究工作，从广义地讲，可以从网络层、协议层和应用层3个角度进行部署。 阻止网络层攻击的基本思路是去除与用户相关连流量的特征，包括：数据包大小分布、数据包顺序、流速率、流量时间等。 包填充技术可以基于一定填充策略填充数据包大小，以便去除数据包长度相关特征。 Wright等人提出了流量整形技术，使数据包长度与目标页面流量数据包分布相似，可以有效防御Liberatore等人提出的攻击方法。 Chan-Tin等人提出了一种基于聚类的流量整形技术，使簇中的不同网站流量特征看起来基本相同，从而使分类精度从70%下降到不足1%。 流量填充技术可以将伪数据包注入到用户的原始流量中，以便模糊流量大小。例如，为了阻止网站指纹攻击，网络服务器可以首先选择目标页面，然后模仿目标网页的数据包大小分布。 为了应对Panchenko等人的网站指纹攻击，Tor项目提出通过随机化HTTP流水线管道中的最大请求数来进行防御。 Nithyanad等人指出，在Tor暗网中40%的链路易受AS级别的流量关联攻击，为此他们提出了一种新的Tor客户端Astoria，通过网络测量和路径预测来进行智能路由选择，降低来自AS级别的威胁。 Juen等人通过路径预测技术来抵御来自AS和网络交换机的流量分析攻击，但该方法仍然有提升空间。 在协议层，报文填充和流量填充技术可以隐藏与用户相关联的流量特征。Secure Shell（SSH）、TLS和IPSec应用这一的报文填充技术可以将明文与块密码边界对齐，从而在一定程度上模糊包大小。为了进一步提高安全性，可以选择随机数量的数据包进行填充。此外还可以使用协议级流量填充技术。例如，Tor通常不将填充数据单元的功能用于链路级填充，因为它会显著降低链路的性能，因此可以设计协议级报文填充和流量填充技术，以在一定程度上减少开销。 在应用层，可以利用HTTP特征和背景流量（即诱饵网页）从用户流中移除流量特征。例如，HTTP流水线技术和HTTP range字段可以用于调整传入和传出的数据包大小。此外，在客户端改变HTTP请求的顺序可以在一定程度上改变流量模式。为了在应用层使用背景流量技术，当用户浏览目标网页时，可以在背景浏览中加载诱饵网页。但这种类型的防御技术只能用于一些特定的协议（如HTTP），不能广泛应用于所有协议。Wang等人提出了一种高效的网站指纹防御技术Walkie-Talkie，通过修改浏览器以半双工模式进行通信，可以模拟突发序列，使敏感和非敏感页面的浏览模式类似。实验结果表明Walkie-Talkie能够以较低的带宽和时间开销防御网站指纹攻击。 5、暗网治理技术研究 针对暗网服务滥用的问题，提出了对暗网bridge节点的发现方法。通过分析典型匿名通信系统Tor节点的选择算法，提出以节点注入方式枚举在线bridge节点的方法，并利用协议特征以区分客户端、正常匿名入口节点和bridge节点。通过理论分析推算枚举所有节点的时间，并根据理论结果指导实际部署，以达到最优的资源配置和最快的bridge节点捕获速率。此外，在获取部分bridge节点的基础上，通过对受控网络的监控，分析出入流量中存在的bridge节点网络连接特征，从而关联更多的bridge节点。 针对匿名通信系统Tor的暗网因此服务节点，提出了隐藏服务定位技术，为暗网中的非法内容监管提供支持。在分析Tor隐藏服务节点与客户端之间的匿名通信协议的基础上，部署部分Tor入口节点，控制客户端不断向目标隐藏服务节点发起连接，以产生具有可检测特征的隐蔽、快速流量，并在受控制节点观察流量特征以定位和确认隐藏服务节点。通过对暗网通信系统选路算法的理论分析，推算其隐藏服务节点选择受控制节点的概率，以此指导节点部署规模，以最小资源代价实现暗网隐藏服务节点的捕获。 针对滥用暗网的用户，提出暗网用户的上网行为分析技术，为暗网用户的监管提供支撑。暗网用户流量经过加密、混淆等操作后，其识别难度大大增加，为此提出了暗网通信流量识别技术。针对典型暗网系统，分别从协议设计和实现机制入手，研究匿名通信性系统中通信数据的缓存、封装和调度机制。同对数据包分布、流量统计属性等不同层面特性的综合分析，筛选匿名通信流量的可区分特性，并从理论上分析证明特征选择的合理性。在此基础上，研究并选取合适的机器识别方法，实现对匿名通信流量的快速、准确在线识别。 在识别暗网用户的基础上，为进一步推测暗网用户加密流量中的应用类型，提出了暗网通信应用分类技术。在各种交互式和非交互式暗网通信应用流量进行深入分析的基础上，建立包括上下行流量比、并发连接数等指标的流统计模型，并将其作为先验知识。针对采集的流量样本使用特征选择算法，获取不同匿名协议相应的对噪声不敏感、区分度高的特征。在此基础上，选择抗干扰分类性能稳定的分类模型，对匿名通信流量上应用层进行分类，建立分类评价模型，根据分类误差率、计算时间复杂度等指标，对所选特征和分类模型进行评价。 针对暗网用户和网站访问流量，提出了暗网加密流量内容分析技术，以推测用户访问的站点。分布针对单跳和多跳匿名通信系统，研究其采用的安全传输和匿名协议，并深入分析上层应用对暗网通信包长、时间间隔、以及并发匿名链路数量等流量特征的影响。在此基础上，重点针对HTTP等典型暗网通信流量，通过被动和主动方式采集并预处理流量数据，提取特征生成指纹，从而建立所关注木匾站点的指纹库，使用高效分类模型将未知暗网流量与指纹库中的数据进行匹配，分析可能的暗网通信目的端。 对于访问非法站点的用户，提出了不同层次的通信追踪技术，能够快速、准确、隐蔽地确认匿名流量之间的通信关系。通过分析匿名通信机制以及各种网络干肉对网络流量时间特征的影响，利用随机过程理论建立干扰环境下匿名通信流量时间特征变化的数据模型。在通用流水印追踪架构的基础上，根据水印追踪的隐蔽性和健壮性要求，利用信息论方法评估基于现有流水印机制构建的隐蔽信道的容量。在此基础上，引入最优停止理论，研究并设计了自适应的水印嵌入与检测机制。]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div id="content_views" class="markdown_views prism-atom-one-light">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<p>本文仅为作者学习笔记，内容源自论文“<em>匿名通信与暗网研究综述——罗军舟等</em>”本身以及相关网络搜索</p>
<h3><a id="1_2"></a>1、匿名通信与暗网</h3>
<blockquote>
<p>匿名通信指采取一定的措施隐蔽通信流中的通信关系，使窃听者难以获取或推知通信双方的关系及内容。匿名通信的目的就是隐蔽通信双方的身份或通信关系，保护网络用户的个人通信隐私。——<a href="https://wiki.mbalib.com/wiki/%E5%8C%BF%E5%90%8D%E9%80%9A%E4%BF%A1" rel="nofollow" data-token="6677dc8552b3fbdbf17b7084adf080b0">MBA智库百科</a></p>
</blockquote>
<p><strong>匿名通信</strong>是一项技术，用于保护通信双方的隐私，使窃听者无法获取通信双方的关系及内容。<strong>匿名通信系统</strong>是提供匿名通信服务的一套完整网络，主要由提供加密服务的节点组成。<strong>暗网</strong>是匿名通信系统的其中一种表现形式，利用隐藏服务机制，使监管者无法有效监控暗网中的非法活动。</p>
<dl>
<dt>
    学术界对匿名通信系统研究的2个方向
   </dt>
<dd>
    专注于匿名通信系统本身，分析其安全性并提出相关的隐私增强技术：包括<br />
    <strong>匿名接入</strong>、<br />
    <strong>匿名路由</strong>、<br />
    <strong>隐藏服务</strong>等机制的优化，<br />
    <strong>流量混淆</strong>和<br />
    <strong>协议伪装</strong>等技术的设计和应用。
   </dd>
<dd>
    针对匿名通信系统和暗网的攻击和监管：利用流量分析，系统/协议漏洞分析，提出去匿名化的攻击方法，包括<br />
    <strong>暗网隐藏节点发现</strong>，<br />
    <strong>隐藏服务定位</strong>，<br />
    <strong>暗网用户的网络行为分析</strong>，<br />
    <strong>暗网流量追踪</strong>，<br />
    <strong>通信关系确认</strong>等。
   </dd>
</dl>
<h3><a id="2TorI2PFreenetZeronethttpsblogcsdnnetu011848397articledetails88371309_12"></a>2、<a href="https://blog.csdn.net/u011848397/article/details/88371309" rel="nofollow" data-token="275577241d22c758571e14d41fcd9452">匿名通信系统的分类和基本工作原理：Tor、I2P、Freenet、Zeronet</a></h3>
<h3><a id="3_15"></a>3、匿名通信关键技术</h3>
<h4><a id="31bridgemeekFTEFlashproxyhttpsblogcsdnnetu011848397articledetails88371641_16"></a>3-1、<a href="https://blog.csdn.net/u011848397/article/details/88371641" rel="nofollow" data-token="1b575e000e373eb002d7d53ca2f264f6">匿名接入：bridge、meek、FTE、Flashproxy</a></h4>
<h4><a id="32DHThttpsblogcsdnnetu011848397articledetails88371647_18"></a>3-2、<a href="https://blog.csdn.net/u011848397/article/details/88371647" rel="nofollow" data-token="e0cd04dff3416f69cb321a580d29b2c4">匿名路由：洋葱路由、大蒜路由、基于DHT的路由</a></h4>
<h4><a id="33TorI2PFreenetZeronethttpsblogcsdnnetu011848397articledetails88371656_20"></a>3-3、<a href="https://blog.csdn.net/u011848397/article/details/88371656" rel="nofollow" data-token="c2ac21e72e0699d53d4f767e31c48632">暗网服务技术：Tor、I2P、Freenet、Zeronet</a></h4>
<h3><a id="4_22"></a>4、匿名通信攻防研究</h3>
<h4><a id="41httpsblogcsdnnetu011848397articledetails88598881_23"></a><a href="https://blog.csdn.net/u011848397/article/details/88598881" rel="nofollow" data-token="f7c2a0b108110d29311c981628670a60">4-1、匿名通信攻击技术：主动攻击、被动攻击、单端攻击、端到端攻击</a></h4>
<h4><a id="42_25"></a>4-2、匿名通信增强技术</h4>
<p>目前已存在一些用来防御各种攻击的研究工作，从广义地讲，可以从网络层、协议层和应用层3个角度进行部署。</p>
<p>阻止<strong>网络层</strong>攻击的基本思路是去除与用户相关连流量的特征，包括：数据包大小分布、数据包顺序、流速率、流量时间等。</p>
<p><strong>包填充技术</strong>可以基于一定填充策略填充数据包大小，以便去除数据包长度相关特征。</p>
<ol>
<li>Wright等人提出了流量整形技术，使数据包长度与目标页面流量数据包分布相似，可以有效防御Liberatore等人提出的攻击方法。</li>
<li>Chan-Tin等人提出了一种基于聚类的流量整形技术，使簇中的不同网站流量特征看起来基本相同，从而使分类精度从70%下降到不足1%。</li>
</ol>
<p><strong>流量填充技术</strong>可以将伪数据包注入到用户的原始流量中，以便模糊流量大小。例如，为了阻止网站指纹攻击，网络服务器可以首先选择目标页面，然后模仿目标网页的数据包大小分布。</p>
<ol>
<li>为了应对Panchenko等人的网站指纹攻击，Tor项目提出通过随机化HTTP流水线管道中的最大请求数来进行防御。</li>
<li>Nithyanad等人指出，在Tor暗网中40%的链路易受AS级别的流量关联攻击，为此他们提出了一种新的Tor客户端Astoria，通过网络测量和路径预测来进行智能路由选择，降低来自AS级别的威胁。</li>
<li>Juen等人通过路径预测技术来抵御来自AS和网络交换机的流量分析攻击，但该方法仍然有提升空间。</li>
</ol>
<p>在<strong>协议层</strong>，报文填充和流量填充技术可以隐藏与用户相关联的流量特征。Secure Shell（SSH）、TLS和IPSec应用这一的报文填充技术可以将明文与块密码边界对齐，从而在一定程度上模糊包大小。为了进一步提高安全性，可以选择随机数量的数据包进行填充。此外还可以使用<strong>协议级流量填充技术</strong>。例如，Tor通常不将填充数据单元的功能用于链路级填充，因为它会显著降低链路的性能，因此可以设计协议级报文填充和流量填充技术，以在一定程度上减少开销。</p>
<p>在<strong>应用层</strong>，可以利用HTTP特征和背景流量（即诱饵网页）从用户流中移除流量特征。例如，HTTP流水线技术和HTTP range字段可以用于调整传入和传出的数据包大小。此外，在客户端改变HTTP请求的顺序可以在一定程度上改变流量模式。为了在应用层使用背景流量技术，当用户浏览目标网页时，可以在背景浏览中加载诱饵网页。但这种类型的防御技术只能用于一些特定的协议（如HTTP），不能广泛应用于所有协议。Wang等人提出了一种高效的<em>网站指纹防御技术Walkie-Talkie</em>，通过修改浏览器以半双工模式进行通信，可以模拟突发序列，使敏感和非敏感页面的浏览模式类似。实验结果表明Walkie-Talkie能够以较低的带宽和时间开销防御网站指纹攻击。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190318165356719.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTE4NDgzOTc=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h3><a id="5_45"></a>5、暗网治理技术研究</h3>
<p><strong>针对暗网服务滥用的问题，提出了对暗网bridge节点的发现方法</strong>。通过分析典型匿名通信系统Tor节点的选择算法，提出以节点注入方式枚举在线bridge节点的方法，并利用协议特征以区分客户端、正常匿名入口节点和bridge节点。通过理论分析推算枚举所有节点的时间，并根据理论结果指导实际部署，以达到最优的资源配置和最快的bridge节点捕获速率。此外，在获取部分bridge节点的基础上，通过对受控网络的监控，分析出入流量中存在的bridge节点网络连接特征，从而关联更多的bridge节点。</p>
<p><strong>针对匿名通信系统Tor的暗网因此服务节点，提出了隐藏服务定位技术</strong>，为暗网中的非法内容监管提供支持。在分析Tor隐藏服务节点与客户端之间的匿名通信协议的基础上，部署部分Tor入口节点，控制客户端不断向目标隐藏服务节点发起连接，以产生具有可检测特征的隐蔽、快速流量，并在受控制节点观察流量特征以定位和确认隐藏服务节点。通过对暗网通信系统选路算法的理论分析，推算其隐藏服务节点选择受控制节点的概率，以此指导节点部署规模，以最小资源代价实现暗网隐藏服务节点的捕获。</p>
<p><strong>针对滥用暗网的用户，提出暗网用户的上网行为分析技术</strong>，为暗网用户的监管提供支撑。暗网用户流量经过加密、混淆等操作后，其识别难度大大增加，为此提出了<strong>暗网通信流量识别技术</strong>。针对典型暗网系统，分别从协议设计和实现机制入手，研究匿名通信性系统中通信数据的缓存、封装和调度机制。同对数据包分布、流量统计属性等不同层面特性的综合分析，筛选匿名通信流量的可区分特性，并从理论上分析证明<em>特征选择</em>的合理性。在此基础上，研究并选取合适的机器识别方法，实现对匿名通信流量的快速、准确在线识别。</p>
<p>在识别暗网用户的基础上，为进一步推测暗网用户加密流量中的应用类型，提出了<strong>暗网通信应用分类技术</strong>。在各种交互式和非交互式暗网通信应用流量进行深入分析的基础上，建立包括上下行流量比、并发连接数等指标的流统计模型，并将其作为先验知识。针对采集的流量样本使用特征选择算法，获取不同匿名协议相应的对噪声不敏感、区分度高的特征。在此基础上，选择抗干扰分类性能稳定的分类模型，对匿名通信流量上应用层进行分类，建立分类评价模型，根据分类误差率、计算时间复杂度等指标，对所选特征和分类模型进行评价。</p>
<p><strong>针对暗网用户和网站访问流量，提出了暗网加密流量内容分析技术</strong>，以推测用户访问的站点。分布针对单跳和多跳匿名通信系统，研究其采用的安全传输和匿名协议，并深入分析上层应用对暗网通信包长、时间间隔、以及并发匿名链路数量等流量特征的影响。在此基础上，重点针对HTTP等典型暗网通信流量，通过被动和主动方式采集并预处理流量数据，提取特征生成指纹，从而建立所关注木匾站点的指纹库，使用高效分类模型将未知暗网流量与指纹库中的数据进行匹配，分析可能的暗网通信目的端。</p>
<p><strong>对于访问非法站点的用户，提出了不同层次的通信追踪技术</strong>，能够快速、准确、隐蔽地确认匿名流量之间的通信关系。通过分析匿名通信机制以及各种网络干肉对网络流量时间特征的影响，利用<em>随机过程理论</em>建立干扰环境下匿名通信流量时间特征变化的数据模型。在通用流水印追踪架构的基础上，根据水印追踪的隐蔽性和健壮性要求，利用信息论方法评估基于现有流水印机制构建的隐蔽信道的容量。在此基础上，引入最优停止理论，研究并设计了自适应的水印嵌入与检测机制。</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-526ced5128.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>最新史上最大数据泄露，名为“Collection #1”的7.73亿数据！</title>
		<link>https://uzzz.org/article/2500.html</link>
				<pubDate>Tue, 22 Jan 2019 01:50:50 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[网络优化]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2500.html</guid>
				<description><![CDATA[独家分享！ 炸了，炸了！社工裤再次强大了一波！ 这几天某黑客论坛出现重磅“炸弹”，一个87GB的数据包开始流通，其中 7.73 亿电子邮件地址和 2122 万个唯一密码!!! 看上去不错，嗯？你很认真的看到了这里？ 客官别急，这时候不妨点根烟，泡杯茶往下慢慢看。 国外安全研究员 Troy Hunt 披露，上周他被告知一个流行的黑客论坛正在讨论 MEGA 上的一个公开数据集，其容量超过 87GB。在对原数据进行清理过滤后，最终整理的内容包含了 7.73 亿电子邮件地址和 2122 万个唯一密码。这个庞大的数据量，使其成为有史以来载入HIBP网站的最大的漏洞。 数据集的根文件夹名字叫 Collection #1，因此这一次的数据泄露被称为 “Collection #1”。Hunt 称，他检查了这个数据集，发现自己的电子邮件地址和曾使用的旧密码都在里面，而且竟然还是正确的，所幸密码已不再适用。 今天一大堆人QQ弹我找我要数据下载地址，导致我的心情很复杂，这些人实在劝不住，不过把眼瘾不痛快。。 但是我不得不说，现如今有如此执着与单纯的心，实属不易。 群号：151929869（后续若有数据第一时间分享）]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<p>独家分享！</p>
<p>炸了，炸了！社工裤再次强大了一波！<br /> 这几天某黑客论坛出现重磅“炸弹”，一个87GB的数据包开始流通，其中 7.73 亿电子邮件地址和 2122 万个唯一密码!!!<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190122094733142.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xsdzI1ODA=,size_16,color_FFFFFF,t_70" alt="data"><br /> 看上去不错，嗯？你很认真的看到了这里？<br /> 客官别急，这时候不妨点根烟，泡杯茶往下慢慢看。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190122094900816.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xsdzI1ODA=,size_16,color_FFFFFF,t_70" alt="Query"><br /> 国外安全研究员 Troy Hunt 披露，上周他被告知一个流行的黑客论坛正在讨论 MEGA 上的一个公开数据集，其容量超过 87GB。在对原数据进行清理过滤后，最终整理的内容包含了 7.73 亿电子邮件地址和 2122 万个唯一密码。这个庞大的数据量，使其成为有史以来载入HIBP网站的最大的漏洞。</p>
<p>数据集的根文件夹名字叫 Collection #1，因此这一次的数据泄露被称为 “Collection #1”。Hunt 称，他检查了这个数据集，发现自己的电子邮件地址和曾使用的旧密码都在里面，而且竟然还是正确的，所幸密码已不再适用。</p>
<hr>
<p>今天一大堆人QQ弹我找我要数据下载地址，导致我的心情很复杂，这些人实在劝不住，不过把眼瘾不痛快。。<br /> 但是我不得不说，现如今有如此执着与单纯的心，实属不易。<br /> 群号：151929869（后续若有数据第一时间分享）</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-526ced5128.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>目标检测网络的知识蒸馏</title>
		<link>https://uzzz.org/article/2571.html</link>
				<pubDate>Thu, 06 Sep 2018 09:01:21 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[DeepLearning]]></category>
		<category><![CDATA[模型压缩]]></category>
		<category><![CDATA[网络优化]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2571.html</guid>
				<description><![CDATA[&#8220;Learning Efficient Object Detection Models with Knowledge Distillation&#8221;这篇文章通过知识蒸馏（Knowledge Distillation）与Hint指导学习（Hint Learning），提升了主干精简的多分类目标检测网络的推理精度（文章以Faster RCNN为例），例如Faster RCNN-Alexnet、Faster-RCNN-VGGM等，具体框架如下图所示： 教师网络的暗知识提取分为三点：中间层Feature Maps的Hint；RPN/RCN中分类层的暗知识；以及RPN/RCN中回归层的暗知识。具体如下： 具体指导学生网络学习时，RPN与RCN的分类损失由分类层softmax输出与hard target的交叉熵loss、以及分类层softmax输出与soft target的交叉熵loss构成： 由于检测器需要鉴别的不同类别之间存在样本不均衡（imbalance），因此在L_soft中需要对不同类别的交叉熵分配不同的权重，其中背景类的权重为1.5（较大的比例），其他分类的权重均为1.0： RPN与RCN的回归损失由正常的smooth L1 loss、以及文章所定义的teacher bounded regression loss构成： 其中Ls_L1表示正常的smooth L1 loss，Lb表示文章定义的teacher bounded regression loss。当学生网络的位置回归与ground truth的L2距离超过教师网络的位置回归与ground truth的L2距离、且大于某一阈值时，Lb取学生网络的位置回归与ground truth之间的L2距离，否则Lb置0。 Hint learning需要计算教师网络与学生网络中间层输出的Feature Maps之间的L2 loss，并且在学生网络中需要添加可学习的适配层（adaptation layer），以确保guided layer输出的Feature Maps与教师网络输出的Hint维度一致： 通过知识蒸馏、Hint指导学习，提升了精简网络的泛化性、并有助于加快收敛，最后取得了良好的实验结果，具体见文章实验部分。 以SSD为例，KD loss与Teacher bounded L2 loss设计如下： # -*- coding: utf-8 -*- import torch import torch.nn as nn import torch.nn.functional as F from ..box_utils import match, log_sum_exp eps = 1e-5 def KL_div(p, q, pos_w, neg_w): p = p + eps q = q + eps log_p = p * torch.log(p / q) log_p[:,0] *= neg_w log_p[:,1:] *= pos_w return torch.sum(log_p) class MultiBoxLoss(nn.Module): def __init__(self, num_classes, overlap_thresh, prior_for_matching, bkg_label, neg_mining, neg_pos, neg_overlap, encode_target, cfg, use_gpu=True, neg_w=1.5, pos_w=1.0, Temp=1., reg_m=0.): super(MultiBoxLoss, self).__init__() self.use_gpu = use_gpu self.num_classes = num_classes # 21 self.threshold = overlap_thresh # 0.5 self.background_label = bkg_label # 0 self.encode_target = encode_target # False self.use_prior_for_matching = prior_for_matching # True self.do_neg_mining = neg_mining # True self.negpos_ratio = neg_pos # 3 self.neg_overlap = neg_overlap # 0.5 self.variance = cfg['variance'] # soft-target loss self.neg_w = neg_w self.pos_w = pos_w self.Temp = Temp self.reg_m = reg_m]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
<p>&#8220;Learning Efficient Object Detection Models with Knowledge Distillation&#8221;这篇文章通过知识蒸馏（Knowledge Distillation）与Hint指导学习（Hint Learning），提升了主干精简的多分类目标检测网络的推理精度（文章以Faster RCNN为例），例如Faster RCNN-Alexnet、Faster-RCNN-VGGM等，具体框架如下图所示：</p>
<p style="text-align:center;"><img alt="" class="has" height="400" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180906162230300?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="877"></p>
<p>教师网络的暗知识提取分为三点：中间层Feature Maps的Hint；RPN/RCN中分类层的暗知识；以及RPN/RCN中回归层的暗知识。具体如下：</p>
<p style="text-align:center;"><img alt="" class="has" height="350" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180906162833258?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="887"></p>
<p>具体指导学生网络学习时，RPN与RCN的分类损失由分类层softmax输出与hard target的交叉熵loss、以及分类层softmax输出与soft target的交叉熵loss构成：</p>
<p style="text-align:center;"><img alt="" class="has" height="36" src="https://uzshare.com/_p?https://img-blog.csdn.net/2018090616323328?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="470"></p>
<p>由于检测器需要鉴别的不同类别之间存在样本不均衡（imbalance），因此在L_soft中需要对不同类别的交叉熵分配不同的权重，其中背景类的权重为1.5（较大的比例），其他分类的权重均为1.0：</p>
<p style="text-align:center;"><img alt="" class="has" height="45" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180906163747127?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="346"></p>
<p>RPN与RCN的回归损失由正常的smooth L1 loss、以及文章所定义的teacher bounded regression loss构成：</p>
<p style="text-align:center;"><img alt="" class="has" height="110" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180906164250751?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="647"></p>
<p>其中Ls_L1表示正常的smooth L1 loss，Lb表示文章定义的teacher bounded regression loss。当学生网络的位置回归与ground truth的L2距离超过教师网络的位置回归与ground truth的L2距离、且大于某一阈值时，Lb取学生网络的位置回归与ground truth之间的L2距离，否则Lb置0。</p>
<p>Hint learning需要计算教师网络与学生网络中间层输出的Feature Maps之间的L2 loss，并且在学生网络中需要添加可学习的适配层（adaptation layer），以确保guided layer输出的Feature Maps与教师网络输出的Hint维度一致：</p>
<p style="text-align:center;"><img alt="" class="has" height="45" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180906165400268?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="255"></p>
<p>通过知识蒸馏、Hint指导学习，提升了精简网络的泛化性、并有助于加快收敛，最后取得了良好的实验结果，具体见文章实验部分。</p>
<p>以SSD为例，KD loss与Teacher bounded L2 loss设计如下：</p>
<pre class="has">
<code class="language-python"># -*- coding: utf-8 -*-
import torch
import torch.nn as nn
import torch.nn.functional as F
from ..box_utils import match, log_sum_exp

eps = 1e-5

def KL_div(p, q, pos_w, neg_w):
    p = p + eps
    q = q + eps
    log_p = p * torch.log(p / q)
    log_p[:,0] *= neg_w
    log_p[:,1:] *= pos_w
    return torch.sum(log_p)

class MultiBoxLoss(nn.Module):

    def __init__(self, num_classes, overlap_thresh, prior_for_matching,
                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,
                 cfg, use_gpu=True, neg_w=1.5, pos_w=1.0, Temp=1., reg_m=0.):
        super(MultiBoxLoss, self).__init__()
        self.use_gpu = use_gpu
        self.num_classes = num_classes                   # 21
        self.threshold = overlap_thresh                  # 0.5
        self.background_label = bkg_label                # 0
        self.encode_target = encode_target               # False
        self.use_prior_for_matching = prior_for_matching # True
        self.do_neg_mining = neg_mining                  # True
        self.negpos_ratio = neg_pos                      # 3
        self.neg_overlap = neg_overlap                   # 0.5
        self.variance = cfg['variance']

        # soft-target loss
        self.neg_w = neg_w
        self.pos_w = pos_w
        self.Temp  = Temp
        self.reg_m = reg_m

    def forward(self, predictions, pred_t, targets):
        """Multibox Loss
        Args:
            predictions (tuple): A tuple containing loc preds, conf preds,
            and prior boxes from SSD net.
                conf shape: torch.size(batch_size,num_priors,num_classes)
                loc shape: torch.size(batch_size,num_priors,4)
                priors shape: torch.size(num_priors,4)
            pred_t (tuple): teacher's predictions

            targets (tensor): Ground truth boxes and labels for a batch,
                shape: [batch_size,num_objs,5] (last idx is the label).
        """
        loc_data, conf_data, priors = predictions
        num = loc_data.size(0)
        priors = priors[:loc_data.size(1), :]
        num_priors = (priors.size(0))
        num_classes = self.num_classes

        # predictions of teachers
        loc_teach1, conf_teach1 = pred_t[0]

        # match priors (default boxes) and ground truth boxes
        loc_t = torch.Tensor(num, num_priors, 4)
        conf_t = torch.LongTensor(num, num_priors)
        for idx in range(num):
            truths = targets[idx][:, :-1].data
            labels = targets[idx][:, -1].data
            defaults = priors.data
            match(self.threshold, truths, defaults, self.variance, labels,
                  loc_t, conf_t, idx)

        # wrap targets
        with torch.no_grad():
            if self.use_gpu:
                loc_t = loc_t.cuda(non_blocking=True)
                conf_t = conf_t.cuda(non_blocking=True)

        pos = conf_t &gt; 0 # (1, 0, 1, ...)
        num_pos = pos.sum(dim=1, keepdim=True) # [num, 1], number of positives

        # Localization Loss (Smooth L1)
        # Shape: [batch,num_priors,4]
        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data) # [batch,num_priors,1] before expand_as
        loc_p = loc_data[pos_idx].view(-1, 4)
        loc_t = loc_t[pos_idx].view(-1, 4)
        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)

        # knowledge transfer for loc regression
        # teach1
        loc_teach1_p = loc_teach1[pos_idx].view(-1, 4)
        l2_dis_s = (loc_p - loc_t).pow(2).sum(1)
        l2_dis_s_m = l2_dis_s + self.reg_m
        l2_dis_t = (loc_teach1_p - loc_t).pow(2).sum(1)
        l2_num = l2_dis_s_m &gt; l2_dis_t
        l2_loss_teach1 = l2_dis_s[l2_num].sum()

        l2_loss = l2_loss_teach1

        # Compute max conf across batch for hard negative mining
        batch_conf = conf_data.view(-1, self.num_classes)
        loss_c = log_sum_exp(batch_conf.float()) - batch_conf.gather(1, conf_t.view(-1, 1)).float()

        # Hard Negative Mining
        loss_c[pos.view(-1, 1)] = 0
        loss_c = loss_c.view(num, -1)
        #loss_c[pos] = 0  # filter out pos boxes for now
        _, loss_idx = loss_c.sort(1, descending=True)
        _, idx_rank = loss_idx.sort(1)
        num_pos = pos.long().sum(1, keepdim=True)
        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)
        neg = idx_rank &lt; num_neg.expand_as(idx_rank)

        # Confidence Loss Including Positive and Negative Examples
        # CrossEntropy loss
        pos_idx = pos.unsqueeze(2).expand_as(conf_data) # [batch,num_priors,cls]
        neg_idx = neg.unsqueeze(2).expand_as(conf_data)
        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)
        targets_weighted = conf_t[(pos+neg).gt(0)]
        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)

        # soft loss for Knowledge Distillation
        # teach1
        conf_p_teach = conf_teach1[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)
        pt = F.softmax(conf_p_teach/self.Temp, dim=1)
        if self.neg_w &gt; 1.:
            ps = F.softmax(conf_p/self.Temp, dim=1)
            soft_loss1 = KL_div(pt, ps, self.pos_w, self.neg_w) * (self.Temp**2)
        else:
            ps = F.log_softmax(conf_p/self.Temp, dim=1)
            soft_loss1 = nn.KLDivLoss(size_average=False)(ps, pt) * (self.Temp**2)
        soft_loss = soft_loss1

        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N
        N = num_pos.data.sum().float()
        loss_l = loss_l.float()
        loss_c = loss_c.float()
        loss_l /= N
        loss_c /= N
        l2_loss /= N
        soft_loss /= N
        return loss_l, loss_c, soft_loss, l2_loss
</code></pre>
<p><strong>Paper地址：</strong><a href="https://papers.nips.cc/paper/6676-learning-efficient-object-detection-models-with-knowledge-distillation.pdf" rel="nofollow" data-token="fca54545bdf036a169cb91c82d6f4d07">https://papers.nips.cc/paper/6676-learning-efficient-object-detection-models-with-knowledge-distillation.pdf</a></p>
<p><strong>PyTorch版SSD：</strong><a href="https://github.com/amdegroot/ssd.pytorch" rel="nofollow" data-token="9b07a8a42c632f303511224696b01a07">https://github.com/amdegroot/ssd.pytorch</a></p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>米安代码审计 06 PHPYUN V3.0 任意文件上传漏洞</title>
		<link>https://uzzz.org/article/2734.html</link>
				<pubDate>Tue, 24 Jul 2018 04:48:52 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[网络优化]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2734.html</guid>
				<description><![CDATA[本文记录 PHP 代码审计的学习过程，教程为暗月 2015 版的 PHP 代码审计课程 PHP 代码审计博客目录 1. 简介 web环境： phpstudy apache+php5.2 程序版本：phpyun3.0 2013-11-02 2. 测试 首先注册一个会员 然后访问 http://192.168.171.128/member/index.php?XDEBUG_SESSION_START=netbeans-xdebug&#38;M=index&#38;C=save_avatar&#38;type=small&#38;photoId=1.php POST: &#60;?php phpinfo(); ?&#62; 构造文件上传成功 我们访问下 http://192.168.171.128/upload/user/user_small/1.php]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div id="content_views" class="markdown_views prism-atelier-sulphurpool-light">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<blockquote>
<p>本文记录 PHP 代码审计的学习过程，教程为暗月 2015 版的 PHP 代码审计课程</p>
<p><a href="https://mp.csdn.net/mdeditor/81107149" rel="nofollow" data-token="8049fa152fd61ccc1d5669dbe1bdceb5">PHP 代码审计博客目录</a></p>
</blockquote>
<h1><a id="1__4"></a>1. 简介</h1>
<p>web环境： phpstudy apache+php5.2</p>
<p>程序版本：phpyun3.0 2013-11-02</p>
<h1><a id="2__10"></a>2. 测试</h1>
<p>首先注册一个会员 然后访问</p>
<p><a href="http://192.168.171.128/member/index.php?XDEBUG_SESSION_START=netbeans-xdebug&amp;M=index&amp;C=save_avatar&amp;type=small&amp;photoId=1.php" rel="nofollow" data-token="07490f39242eb0d05e650b13d37f27c7">http://192.168.171.128/member/index.php?XDEBUG_SESSION_START=netbeans-xdebug&amp;M=index&amp;C=save_avatar&amp;type=small&amp;photoId=1.php</a></p>
<p>POST:</p>
<p> &lt;?php phpinfo(); ?&gt; </p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190413161724814.png" alt="在这里插入图片描述"></p>
<p>构造文件上传成功 我们访问下</p>
<p><a href="http://192.168.171.128/upload/user/user_small/1.php" rel="nofollow" data-token="e69e3b7397114f7db6e7fba9d7598ce7">http://192.168.171.128/upload/user/user_small/1.php</a></p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190413161728270.png" alt="在这里插入图片描述"></p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-526ced5128.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>知识蒸馏（Knowledge Distillation）</title>
		<link>https://uzzz.org/article/2457.html</link>
				<pubDate>Mon, 04 Jun 2018 08:55:38 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[DeepLearning]]></category>
		<category><![CDATA[模型压缩]]></category>
		<category><![CDATA[网络优化]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2457.html</guid>
				<description><![CDATA[1、Distilling the Knowledge in a Neural Network Hinton的文章&#8221;Distilling the Knowledge in a Neural Network&#8221;首次提出了知识蒸馏（暗知识提取）的概念，通过引入与教师网络（teacher network：复杂、但推理性能优越）相关的软目标（soft-target）作为total loss的一部分，以诱导学生网络（student network：精简、低复杂度）的训练，实现知识迁移（knowledge transfer）。 如上图所示，教师网络（左侧）的预测输出除以温度参数（Temperature）之后、再做softmax变换，可以获得软化的概率分布（软目标），数值介于0~1之间，取值分布较为缓和。Temperature数值越大，分布越缓和；而Temperature数值减小，容易放大错误分类的概率，引入不必要的噪声。针对较困难的分类或检测任务，Temperature通常取1，确保教师网络中正确预测的贡献。硬目标则是样本的真实标注，可以用one-hot矢量表示。total loss设计为软目标与硬目标所对应的交叉熵的加权平均（表示为KD loss与CE loss），其中软目标交叉熵的加权系数越大，表明迁移诱导越依赖教师网络的贡献，这对训练初期阶段是很有必要的，有助于让学生网络更轻松的鉴别简单样本，但训练后期需要适当减小软目标的比重，让真实标注帮助鉴别困难样本。另外，教师网络的推理性能通常要优于学生网络，而模型容量则无具体限制，且教师网络推理精度越高，越有利于学生网络的学习。 教师网络与学生网络也可以联合训练，此时教师网络的暗知识及学习方式都会影响学生网络的学习，具体如下（式中三项分别为教师网络softmax输出的交叉熵loss、学生网络softmax输出的交叉熵loss、以及教师网络数值输出与学生网络softmax输出的交叉熵loss）： 联合训练的Paper地址：https://arxiv.org/abs/1711.05852 2、Exploring Knowledge Distillation of Deep Neural Networks for Efficient Hardware Solutions 这篇文章将total loss重新定义如下： GitHub地址：https://github.com/peterliht/knowledge-distillation-pytorch total loss的Pytorch代码如下，引入了精简网络输出与教师网络输出的KL散度，并在诱导训练期间，先将teacher network的预测输出缓存到CPU内存中，可以减轻GPU显存的overhead： def loss_fn_kd(outputs, labels, teacher_outputs, params): """ Compute the knowledge-distillation (KD) loss given outputs, labels. "Hyperparameters": temperature and alpha NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher and student expects the input tensor to be log probabilities! See Issue #2 """ alpha = params.alpha T = params.temperature KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1), F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \ &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;F.cross_entropy(outputs, labels) * (1. - alpha) return KD_loss 3、Ensemble of Multiple Teachers 第一种算法：多个教师网络输出的soft label按加权组合，构成统一的soft label，然后指导学生网络的训练： 第二种算法：由于加权平均方式会弱化、平滑多个教师网络的预测结果，因此可以随机选择某个教师网络的soft label作为guidance： 第三种算法：同样地，为避免加权平均带来的平滑效果，首先采用教师网络输出的soft label重新标注样本、增广数据、再用于模型训练，该方法能够让模型学会从更多视角观察同一样本数据的不同功能： Paper地址： https://www.researchgate.net/publication/319185356_Efficient_Knowledge_Distillation_from_an_Ensemble_of_Teachers 4、Hint-based Knowledge Transfer 为了能够诱导训练更深、更纤细的学生网络（deeper and thinner FitNet），需要考虑教师网络中间层的Feature Maps（作为Hint），用来指导学生网络中相应的Guided layer。此时需要引入L2 loss指导训练过程，该loss计算为教师网络Hint layer与学生网络Guided layer输出Feature Maps之间的差别，若二者输出的Feature Maps形状不一致，Guided layer需要通过一个额外的回归层，具体如下： 具体训练过程分两个阶段完成：第一个阶段利用Hint-based loss诱导学生网络达到一个合适的初始化状态（只更新W_Guided与W_r）；第二个阶段利用教师网络的soft label指导整个学生网络的训练（即知识蒸馏），且total loss中soft target相关部分所占比重逐渐降低，从而让学生网络能够全面辨别简单样本与困难样本（教师网络能够有效辨别简单样本，而困难样本则需要借助真实标注，即hard target）： Paper地址：https://arxiv.org/abs/1412.6550 GitHub地址：https://github.com/adri-romsor/FitNets 5、Attention to Attention Transfer 通过网络中间层的attention map，完成teacher network与student network之间的知识迁移。考虑给定的tensor A，基于activation的attention map可以定义为如下三种之一： 随着网络层次的加深，关键区域的attention-level也随之提高。文章最后采用了第二种形式的attention map，取p=2，并且activation-based attention map的知识迁移效果优于gradient-based attention map，loss定义及迁移过程如下： Paper地址：https://arxiv.org/abs/1612.03928 GitHub地址：https://github.com/szagoruyko/attention-transfer 6、Flow&#160;of the Solution Procedure 暗知识亦可表示为训练的求解过程（FSP: Flow of the Solution Procedure），教师网络或学生网络的FSP矩阵定义如下（Gram形式的矩阵）： 训练的第一阶段：最小化教师网络FSP矩阵与学生网络FSP矩阵之间的L2 Loss，初始化学生网络的可训练参数： 训练的第二阶段：在目标任务的数据集上fine-tune学生网络。从而达到知识迁移、快速收敛、以及迁移学习的目的。 Paper地址：]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
<h1>1、Distilling the Knowledge in a Neural Network</h1>
<p>Hinton的文章&#8221;Distilling the Knowledge in a Neural Network&#8221;首次提出了知识蒸馏（暗知识提取）的概念，通过引入与教师网络（teacher network：复杂、但推理性能优越）相关的软目标（soft-target）作为total loss的一部分，以诱导学生网络（student network：精简、低复杂度）的训练，实现知识迁移（knowledge transfer）。</p>
<p style="text-align:center;"><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180604160949186?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
<p>如上图所示，教师网络（左侧）的预测输出除以温度参数（Temperature）之后、再做softmax变换，可以获得软化的概率分布（软目标），数值介于0~1之间，取值分布较为缓和。Temperature数值越大，分布越缓和；而Temperature数值减小，容易放大错误分类的概率，引入不必要的噪声。针对较困难的分类或检测任务，Temperature通常取1，确保教师网络中正确预测的贡献。硬目标则是样本的真实标注，可以用one-hot矢量表示。total loss设计为软目标与硬目标所对应的交叉熵的加权平均（表示为KD loss与CE loss），其中软目标交叉熵的加权系数越大，表明迁移诱导越依赖教师网络的贡献，这对训练初期阶段是很有必要的，有助于让学生网络更轻松的鉴别简单样本，但训练后期需要适当减小软目标的比重，让真实标注帮助鉴别困难样本。另外，教师网络的推理性能通常要优于学生网络，而模型容量则无具体限制，且教师网络推理精度越高，越有利于学生网络的学习。</p>
<p>教师网络与学生网络也可以联合训练，此时教师网络的暗知识及学习方式都会影响学生网络的学习，具体如下（式中三项分别为教师网络softmax输出的交叉熵loss、学生网络softmax输出的交叉熵loss、以及教师网络数值输出与学生网络softmax输出的交叉熵loss）：</p>
<p><strong>联合训练的Paper地址：</strong><a href="https://arxiv.org/abs/1711.05852" rel="nofollow" data-token="7c9a7dab2065570ef96d38a9c0285090">https://arxiv.org/abs/1711.05852</a></p>
<p style="text-align:center;"><img alt="" class="has" height="44" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180906092731238?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="540"></p>
<p style="text-align:center;"><img alt="" class="has" height="331" src="https://uzshare.com/_p?https://img-blog.csdn.net/201809060928044?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="673"></p>
<h1>2、Exploring Knowledge Distillation of Deep Neural Networks for Efficient Hardware Solutions</h1>
<p>这篇文章将total loss重新定义如下：</p>
<p style="text-align:center;"><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180604162913410?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
<p><strong>GitHub地址：</strong><a href="https://github.com/peterliht/knowledge-distillation-pytorch" rel="nofollow" data-token="fbeb707ed47634087a5e836b433b1f02">https://github.com/peterliht/knowledge-distillation-pytorch</a></p>
<p>total loss的Pytorch代码如下，引入了精简网络输出与教师网络输出的KL散度，并在诱导训练期间，先将teacher network的预测输出缓存到CPU内存中，可以减轻GPU显存的overhead：</p>
<pre class="has">
<code class="language-python">def loss_fn_kd(outputs, labels, teacher_outputs, params):
    """
    Compute the knowledge-distillation (KD) loss given outputs, labels.
    "Hyperparameters": temperature and alpha
    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher
    and student expects the input tensor to be log probabilities! See Issue #2
    """
    alpha = params.alpha
    T = params.temperature
    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1),
                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;F.cross_entropy(outputs, labels) * (1. - alpha)

    return KD_loss</code></pre>
<h1>3、Ensemble of Multiple Teachers</h1>
<p><strong>第一种算法：</strong>多个教师网络输出的soft label按加权组合，构成统一的soft label，然后指导学生网络的训练：</p>
<p style="text-align:center;"><img alt="" class="has" height="127" src="https://uzshare.com/_p?https://img-blog.csdn.net/2018090414465243?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="530"></p>
<p style="text-align:center;"><img alt="" class="has" height="270" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180904144737333?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="535"></p>
<p><strong>第二种算法：</strong>由于加权平均方式会弱化、平滑多个教师网络的预测结果，因此可以随机选择某个教师网络的soft label作为guidance：</p>
<p style="text-align:center;"><img alt="" class="has" height="200" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180904145125595?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="539"></p>
<p><strong>第三种算法：</strong>同样地，为避免加权平均带来的平滑效果，首先采用教师网络输出的soft label重新标注样本、增广数据、再用于模型训练，该方法能够让模型学会从更多视角观察同一样本数据的不同功能：</p>
<p style="text-align:center;"><img alt="" class="has" height="224" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180904145903860?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="536"></p>
<p><strong>Paper地址：</strong></p>
<p><a href="https://www.researchgate.net/publication/319185356_Efficient_Knowledge_Distillation_from_an_Ensemble_of_Teachers" rel="nofollow" data-token="6cdd0157e63dc26e6c9f039d431913ad">https://www.researchgate.net/publication/319185356_Efficient_Knowledge_Distillation_from_an_Ensemble_of_Teachers</a></p>
<h1>4、Hint-based Knowledge Transfer</h1>
<p>为了能够诱导训练更深、更纤细的学生网络（deeper and thinner FitNet），需要考虑教师网络中间层的Feature Maps（作为Hint），用来指导学生网络中相应的Guided layer。此时需要引入L2 loss指导训练过程，该loss计算为教师网络Hint layer与学生网络Guided layer输出Feature Maps之间的差别，若二者输出的Feature Maps形状不一致，Guided layer需要通过一个额外的回归层，具体如下：</p>
<p style="text-align:center;"><img alt="" class="has" height="59" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180904151630725?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="709"></p>
<p style="text-align:center;"><img alt="" class="has" height="317" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180904150852311?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="863"></p>
<p><strong>具体训练过程分两个阶段完成：</strong>第一个阶段利用Hint-based loss诱导学生网络达到一个合适的初始化状态（只更新W_Guided与W_r）；第二个阶段利用教师网络的soft label指导整个学生网络的训练（即知识蒸馏），且total loss中soft target相关部分所占比重逐渐降低，从而让学生网络能够全面辨别简单样本与困难样本（教师网络能够有效辨别简单样本，而困难样本则需要借助真实标注，即hard target）：</p>
<p style="text-align:center;"><img alt="" class="has" height="434" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180904152348516?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="927"></p>
<p><strong>Paper地址：</strong><a href="https://arxiv.org/abs/1412.6550" rel="nofollow" data-token="63791d3bb0e636fd89563858176c43bc">https://arxiv.org/abs/1412.6550</a></p>
<p><strong>GitHub地址：</strong><a href="https://github.com/adri-romsor/FitNets" rel="nofollow" data-token="e73553587403dd4563b3accafc83a9c4">https://github.com/adri-romsor/FitNets</a></p>
<h1>5、Attention to Attention Transfer</h1>
<p>通过网络中间层的attention map，完成teacher network与student network之间的知识迁移。考虑给定的tensor A，基于activation的attention map可以定义为如下三种之一：</p>
<p style="text-align:center;"><img alt="" class="has" height="114" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180925094725131?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="830"></p>
<p>随着网络层次的加深，关键区域的attention-level也随之提高。文章最后采用了第二种形式的attention map，取p=2，并且activation-based attention map的知识迁移效果优于gradient-based attention map，loss定义及迁移过程如下：</p>
<p style="text-align:center;"><img alt="" class="has" height="67" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180925100235962?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="459"></p>
<p style="text-align:center;"><img alt="" class="has" height="32" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180925100347190?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="414"></p>
<p style="text-align:center;"><img alt="" class="has" height="217" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180925100418174?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="866"></p>
<p><strong>Paper地址：</strong><a href="https://arxiv.org/abs/1612.03928" rel="nofollow" data-token="29a0b52ce6f95ffa88749696b80cb572">https://arxiv.org/abs/1612.03928</a></p>
<p><strong>GitHub地址：</strong><a href="https://github.com/szagoruyko/attention-transfer" rel="nofollow" data-token="712ec1aa5770d9c436f3ec4d433bad42">https://github.com/szagoruyko/attention-transfer</a></p>
<h1>6、Flow&nbsp;of the Solution Procedure</h1>
<p>暗知识亦可表示为训练的求解过程（FSP: Flow of the Solution Procedure），教师网络或学生网络的FSP矩阵定义如下（Gram形式的矩阵）：</p>
<p style="text-align:center;"><img alt="" class="has" height="91" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180908190729629?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="576"></p>
<p style="text-align:center;"><img alt="" class="has" height="343" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180908190755611?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="611"></p>
<p>训练的第一阶段：最小化教师网络FSP矩阵与学生网络FSP矩阵之间的L2 Loss，初始化学生网络的可训练参数：</p>
<p style="text-align:center;"><img alt="" class="has" height="420" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180908191130435?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="918"></p>
<p>训练的第二阶段：在目标任务的数据集上fine-tune学生网络。从而达到知识迁移、快速收敛、以及迁移学习的目的。</p>
<p><strong>Paper地址：</strong></p>
<p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf" rel="nofollow" data-token="7e1d838d35fcd3bb2deaa4697418f1da">http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf</a></p>
<h1>7、Knowledge Distillation with Adversarial Samples&nbsp;Supporting Decision Boundary</h1>
<p>从分类的决策边界角度分析，知识迁移过程亦可理解为教师网络诱导学生网络有效鉴别决策边界的过程，鉴别能力越强意味着模型的泛化能力越好：</p>
<p style="text-align:center;"><img alt="" class="has" height="323" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180920145521968?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="886"></p>
<p>文章首先利用对抗攻击策略（adversarial attacking）将基准类样本（base class sample）转为目标类样本、且位于决策边界附近（BSS: boundary supporting sample），进而利用对抗生成的样本诱导学生网络的训练，可有效提升学生网络对决策边界的鉴别能力。文章采用迭代方式生成对抗样本，需要沿loss function（基准类得分与目标类得分之差）的梯度负方向调整样本，直到满足停止条件为止：</p>
<p style="text-align:center;"><img alt="" class="has" height="283" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180920150428101?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="716"></p>
<p>loss function：</p>
<p style="text-align:center;"><img alt="" class="has" height="30" src="https://uzshare.com/_p?https://img-blog.csdn.net/2018092015152089?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="234"></p>
<p>沿loss function的梯度负方向调整样本：</p>
<p style="text-align:center;"><img alt="" class="has" height="68" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180920151540465?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="408"></p>
<p>停止条件（只要满足三者之一）：</p>
<p style="text-align:center;"><img alt="" class="has" height="136" src="https://uzshare.com/_p?https://img-blog.csdn.net/2018092015160434?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="439"></p>
<p>结合对抗生成的样本，利用教师网络训练学生网络所需的total loss包含CE loss、KD loss以及boundary supporting loss（BS loss）：</p>
<p style="text-align:center;"><img alt="" class="has" height="302" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180920151838819?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="886"></p>
<p><strong>Paper地址：</strong><a href="https://arxiv.org/abs/1805.05532" rel="nofollow" data-token="74570507cb0a0f46a8e3d48366718b46">https://arxiv.org/abs/1805.05532</a></p>
<h1>8、Label Refinery：Improving ImageNet Classification through Label Progression</h1>
<p>这篇文章通过迭代式的诱导训练，主要解决训练期间样本的crop与label不一致的问题，以增强label的质量，从而进一步增强模型的泛化能力：</p>
<p style="text-align:center;"><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180604163723669?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
<p>诱导过程中，total loss表示为本次迭代（t&gt;1）网络的预测输出（概率分布）与上一次迭代输出（Label Refinery：类似于教师网络的角色）的KL散度：</p>
<p style="text-align:center;"><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180604164358160?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
<p>文章实验部分表明，不仅可以用训练网络作为Label Refinery Network，也可以用其他高质量网络（如Resnet50）作为Label Refinery Network。并在诱导过程中，能够对抗生成样本，实现数据增强。</p>
<p><strong>GitHub地址：</strong><a href="https://github.com/hessamb/label-refinery" rel="nofollow" data-token="e1838d413d06c4f8fdae77d271ab51dc">https://github.com/hessamb/label-refinery</a></p>
<h1>9、Miscellaneous</h1>
<p>&#8212;&#8212;&#8211; 知识蒸馏可以与量化结合使用，考虑了中间层Feature Maps之间的关系，<strong>可参考：</strong></p>
<p><a href="https://blog.csdn.net/nature553863/article/details/82147933" rel="nofollow" data-token="63b58e5f5b16fd5081d344dea82399be">https://blog.csdn.net/nature553863/article/details/82147933</a></p>
<p>&#8212;&#8212;&#8211; 知识蒸馏与Hint Learning相结合，可以训练精简的Faster-RCNN，<strong>可参考：</strong></p>
<p><a href="https://blog.csdn.net/nature553863/article/details/82463249" rel="nofollow" data-token="c73c3b240a8489852a4ec253d9651f42">https://blog.csdn.net/nature553863/article/details/82463249</a></p>
<p>&#8212;&#8212;&#8211; 模型压缩方面，更为详细的讨论，<strong>请参考：</strong></p>
<p><a href="https://blog.csdn.net/nature553863/article/details/81083955" rel="nofollow" data-token="23973bd49e9fd025e976be4cc0a83d29">https://blog.csdn.net/nature553863/article/details/81083955</a></p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>CCS7.3 安装使用教程</title>
		<link>https://uzzz.org/article/3062.html</link>
				<pubDate>Sat, 28 Oct 2017 05:14:50 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[网络优化]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/3062.html</guid>
				<description><![CDATA[CCS7.3 安装使用教程 •Code Composer Studio™ IDE，是TI的集成开发环境。 Code Composer Studio™ Desktop IDE for all TI microcontrollers, processors, and wireless connectivity platforms。TI所有的产品都可以用ccs进行开发。 1.首先到TI官网进行下载最新本CCS7.3。 2.下载完成后，双击CCS_SETUP.EXE进行安装。安装路径注意不要有中文 3.按照提示，点击continue，next…………. 到达如下界面选择性安装。选择的产品越多，安装占用的硬盘空间越大，安装时间越长 剩下的就是按照提示进行等待，安装时间根据个人电脑配置而异。 CCS7.3新建工程 1. 双击桌面上CCS图标， 就会出现工作空间指定界面。这里要设置好工作空间路径。 软件打开后会看到左侧的project管理窗口。在这个工作空间内的所有project都会在这里显示。 新建工程 点击File-&#62; New -&#62; CCS Project 出现工程设置界面如下 1选择器件系列，2选择具体器件型号3设置工程名字4指定编译器5指定工程模板 这样一个新的project工程就新建好了。 补充： 1.CCS导入工程，CCS是eclipse结构软件，不能像KEIL软件那样直接打开工程。CSS工程文件只能通过导入的方式打开。在project管理窗口空白处右键 -&#62;import -&#62;CCS project。 浏览找到要打开的工程文件路径即可导入工程，导入工程的界面左下角有一个copy project into workspace 的复选框，勾选上后 导入的project就会复制到你的工作空间中。]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<h2 id="ccs73-安装使用教程">CCS7.3 安装使用教程</h2>
<p>•Code Composer Studio<img src="https://s.w.org/images/core/emoji/12.0.0-1/72x72/2122.png" alt="™" class="wp-smiley" style="height: 1em; max-height: 1em;" /> IDE，是TI的集成开发环境。 <br /> Code Composer Studio<img src="https://s.w.org/images/core/emoji/12.0.0-1/72x72/2122.png" alt="™" class="wp-smiley" style="height: 1em; max-height: 1em;" /> Desktop IDE for all TI microcontrollers, processors, and wireless connectivity platforms。TI所有的产品都可以用ccs进行开发。 <br /> 1.首先到TI官网进行下载最新本CCS7.3。</p>
<p>2.下载完成后，双击CCS_SETUP.EXE进行安装。安装路径注意不要有中文</p>
<p>3.按照提示，点击continue，next…………. <br /> 到达如下界面选择性安装。选择的产品越多，安装占用的硬盘空间越大，安装时间越长 <br /> 剩下的就是按照提示进行等待，安装时间根据个人电脑配置而异。</p>
<p>CCS7.3新建工程 <br /> 1. 双击桌面上CCS图标，</p>
<p>就会出现工作空间指定界面。这里要设置好工作空间路径。</p>
<ol>
<li>
<p>软件打开后会看到左侧的project管理窗口。在这个工作空间内的所有project都会在这里显示。</p>
</li>
<li>
<p>新建工程 <br /> 点击File-&gt; New -&gt; CCS Project 出现工程设置界面如下</p>
</li>
</ol>
<p>1选择器件系列，2选择具体器件型号3设置工程名字4指定编译器5指定工程模板 <br /> 这样一个新的project工程就新建好了。 <br /> 补充： <br /> 1.CCS导入工程，CCS是eclipse结构软件，不能像KEIL软件那样直接打开工程。CSS工程文件只能通过导入的方式打开。在project管理窗口空白处右键 -&gt;import -&gt;CCS project。 浏览找到要打开的工程文件路径即可导入工程，导入工程的界面左下角有一个copy project into workspace 的复选框，勾选上后 导入的project就会复制到你的工作空间中。</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-526ced5128.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>ArcGIS无法移动要素，坐标或测量值超出范围的问题</title>
		<link>https://uzzz.org/article/3337.html</link>
				<pubDate>Sat, 13 Aug 2016 02:39:12 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[网络优化]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/3337.html</guid>
				<description><![CDATA[&#160; &#160; &#160; &#160;在ArcGIS中平移要素时经常出现“无法移动要素。坐标或测量值超出范围”的问题，这种情况通常有由于坐标系的空间范围问题造成的，如果你还想使用这个坐标系，有一个很简单的处理方法： 1.首先去掉数据的坐标系 具体的操作见http://blog.csdn.net/gisuuser/article/details/52197909 2.平移数据，不会出现任何问题 根据自己的需要平移 3.再添加原来的坐标系]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
<p><span style="font-size:24px;">&nbsp; &nbsp; &nbsp; &nbsp;在ArcGIS中平移要素时经常出现“无法移动要素。坐标或测量值超出范围”的问题，这种情况通常有由于坐标系的空间范围问题造成的，如果你还想使用这个坐标系，有一个很简单的处理方法：</span></p>
<p><span style="font-size:24px;"><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20160813103240512?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""></span></p>
<p><span style="font-size:24px;">1.首先去掉数据的坐标系</span></p>
<p><span style="font-size:24px;">具体的操作见<a href="http://blog.csdn.net/gisuuser/article/details/52197909" rel="nofollow" data-token="6745c9731bcfae9bfef3d228db72b9a5">http://blog.csdn.net/gisuuser/article/details/52197909</a></span></p>
<p><span style="font-size:24px;">2.平移数据，不会出现任何问题</span></p>
<p><span style="font-size:24px;">根据自己的需要平移</span></p>
<p><span style="font-size:24px;"><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20160813103717154?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br /></span></p>
<p><span style="font-size:24px;">3.再添加原来的坐标系</span></p>
<p><span style="font-size:24px;"><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20160813103723565?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt=""><br /></span></p>
</p></div>
</div>
]]></content:encoded>
										</item>
	</channel>
</rss>
