<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>模型压缩 &#8211; 有组织在!</title>
	<atom:link href="https://uzzz.org/category/moxingyasuo/feed" rel="self" type="application/rss+xml" />
	<link>https://uzzz.org/</link>
	<description></description>
	<lastBuildDate>Thu, 06 Sep 2018 09:01:21 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.4</generator>

<image>
	<url>https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png</url>
	<title>模型压缩 &#8211; 有组织在!</title>
	<link>https://uzzz.org/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>目标检测网络的知识蒸馏</title>
		<link>https://uzzz.org/article/2571.html</link>
				<pubDate>Thu, 06 Sep 2018 09:01:21 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[DeepLearning]]></category>
		<category><![CDATA[模型压缩]]></category>
		<category><![CDATA[网络优化]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2571.html</guid>
				<description><![CDATA[&#8220;Learning Efficient Object Detection Models with Knowledge Distillation&#8221;这篇文章通过知识蒸馏（Knowledge Distillation）与Hint指导学习（Hint Learning），提升了主干精简的多分类目标检测网络的推理精度（文章以Faster RCNN为例），例如Faster RCNN-Alexnet、Faster-RCNN-VGGM等，具体框架如下图所示： 教师网络的暗知识提取分为三点：中间层Feature Maps的Hint；RPN/RCN中分类层的暗知识；以及RPN/RCN中回归层的暗知识。具体如下： 具体指导学生网络学习时，RPN与RCN的分类损失由分类层softmax输出与hard target的交叉熵loss、以及分类层softmax输出与soft target的交叉熵loss构成： 由于检测器需要鉴别的不同类别之间存在样本不均衡（imbalance），因此在L_soft中需要对不同类别的交叉熵分配不同的权重，其中背景类的权重为1.5（较大的比例），其他分类的权重均为1.0： RPN与RCN的回归损失由正常的smooth L1 loss、以及文章所定义的teacher bounded regression loss构成： 其中Ls_L1表示正常的smooth L1 loss，Lb表示文章定义的teacher bounded regression loss。当学生网络的位置回归与ground truth的L2距离超过教师网络的位置回归与ground truth的L2距离、且大于某一阈值时，Lb取学生网络的位置回归与ground truth之间的L2距离，否则Lb置0。 Hint learning需要计算教师网络与学生网络中间层输出的Feature Maps之间的L2 loss，并且在学生网络中需要添加可学习的适配层（adaptation layer），以确保guided layer输出的Feature Maps与教师网络输出的Hint维度一致： 通过知识蒸馏、Hint指导学习，提升了精简网络的泛化性、并有助于加快收敛，最后取得了良好的实验结果，具体见文章实验部分。 以SSD为例，KD loss与Teacher bounded L2 loss设计如下： # -*- coding: utf-8 -*- import torch import torch.nn as nn import torch.nn.functional as F from ..box_utils import match, log_sum_exp eps = 1e-5 def KL_div(p, q, pos_w, neg_w): p = p + eps q = q + eps log_p = p * torch.log(p / q) log_p[:,0] *= neg_w log_p[:,1:] *= pos_w return torch.sum(log_p) class MultiBoxLoss(nn.Module): def __init__(self, num_classes, overlap_thresh, prior_for_matching, bkg_label, neg_mining, neg_pos, neg_overlap, encode_target, cfg, use_gpu=True, neg_w=1.5, pos_w=1.0, Temp=1., reg_m=0.): super(MultiBoxLoss, self).__init__() self.use_gpu = use_gpu self.num_classes = num_classes # 21 self.threshold = overlap_thresh # 0.5 self.background_label = bkg_label # 0 self.encode_target = encode_target # False self.use_prior_for_matching = prior_for_matching # True self.do_neg_mining = neg_mining # True self.negpos_ratio = neg_pos # 3 self.neg_overlap = neg_overlap # 0.5 self.variance = cfg['variance'] # soft-target loss self.neg_w = neg_w self.pos_w = pos_w self.Temp = Temp self.reg_m = reg_m]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
<p>&#8220;Learning Efficient Object Detection Models with Knowledge Distillation&#8221;这篇文章通过知识蒸馏（Knowledge Distillation）与Hint指导学习（Hint Learning），提升了主干精简的多分类目标检测网络的推理精度（文章以Faster RCNN为例），例如Faster RCNN-Alexnet、Faster-RCNN-VGGM等，具体框架如下图所示：</p>
<p style="text-align:center;"><img alt="" class="has" height="400" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180906162230300?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="877"></p>
<p>教师网络的暗知识提取分为三点：中间层Feature Maps的Hint；RPN/RCN中分类层的暗知识；以及RPN/RCN中回归层的暗知识。具体如下：</p>
<p style="text-align:center;"><img alt="" class="has" height="350" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180906162833258?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="887"></p>
<p>具体指导学生网络学习时，RPN与RCN的分类损失由分类层softmax输出与hard target的交叉熵loss、以及分类层softmax输出与soft target的交叉熵loss构成：</p>
<p style="text-align:center;"><img alt="" class="has" height="36" src="https://uzshare.com/_p?https://img-blog.csdn.net/2018090616323328?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="470"></p>
<p>由于检测器需要鉴别的不同类别之间存在样本不均衡（imbalance），因此在L_soft中需要对不同类别的交叉熵分配不同的权重，其中背景类的权重为1.5（较大的比例），其他分类的权重均为1.0：</p>
<p style="text-align:center;"><img alt="" class="has" height="45" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180906163747127?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="346"></p>
<p>RPN与RCN的回归损失由正常的smooth L1 loss、以及文章所定义的teacher bounded regression loss构成：</p>
<p style="text-align:center;"><img alt="" class="has" height="110" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180906164250751?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="647"></p>
<p>其中Ls_L1表示正常的smooth L1 loss，Lb表示文章定义的teacher bounded regression loss。当学生网络的位置回归与ground truth的L2距离超过教师网络的位置回归与ground truth的L2距离、且大于某一阈值时，Lb取学生网络的位置回归与ground truth之间的L2距离，否则Lb置0。</p>
<p>Hint learning需要计算教师网络与学生网络中间层输出的Feature Maps之间的L2 loss，并且在学生网络中需要添加可学习的适配层（adaptation layer），以确保guided layer输出的Feature Maps与教师网络输出的Hint维度一致：</p>
<p style="text-align:center;"><img alt="" class="has" height="45" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180906165400268?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="255"></p>
<p>通过知识蒸馏、Hint指导学习，提升了精简网络的泛化性、并有助于加快收敛，最后取得了良好的实验结果，具体见文章实验部分。</p>
<p>以SSD为例，KD loss与Teacher bounded L2 loss设计如下：</p>
<pre class="has">
<code class="language-python"># -*- coding: utf-8 -*-
import torch
import torch.nn as nn
import torch.nn.functional as F
from ..box_utils import match, log_sum_exp

eps = 1e-5

def KL_div(p, q, pos_w, neg_w):
    p = p + eps
    q = q + eps
    log_p = p * torch.log(p / q)
    log_p[:,0] *= neg_w
    log_p[:,1:] *= pos_w
    return torch.sum(log_p)

class MultiBoxLoss(nn.Module):

    def __init__(self, num_classes, overlap_thresh, prior_for_matching,
                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,
                 cfg, use_gpu=True, neg_w=1.5, pos_w=1.0, Temp=1., reg_m=0.):
        super(MultiBoxLoss, self).__init__()
        self.use_gpu = use_gpu
        self.num_classes = num_classes                   # 21
        self.threshold = overlap_thresh                  # 0.5
        self.background_label = bkg_label                # 0
        self.encode_target = encode_target               # False
        self.use_prior_for_matching = prior_for_matching # True
        self.do_neg_mining = neg_mining                  # True
        self.negpos_ratio = neg_pos                      # 3
        self.neg_overlap = neg_overlap                   # 0.5
        self.variance = cfg['variance']

        # soft-target loss
        self.neg_w = neg_w
        self.pos_w = pos_w
        self.Temp  = Temp
        self.reg_m = reg_m

    def forward(self, predictions, pred_t, targets):
        """Multibox Loss
        Args:
            predictions (tuple): A tuple containing loc preds, conf preds,
            and prior boxes from SSD net.
                conf shape: torch.size(batch_size,num_priors,num_classes)
                loc shape: torch.size(batch_size,num_priors,4)
                priors shape: torch.size(num_priors,4)
            pred_t (tuple): teacher's predictions

            targets (tensor): Ground truth boxes and labels for a batch,
                shape: [batch_size,num_objs,5] (last idx is the label).
        """
        loc_data, conf_data, priors = predictions
        num = loc_data.size(0)
        priors = priors[:loc_data.size(1), :]
        num_priors = (priors.size(0))
        num_classes = self.num_classes

        # predictions of teachers
        loc_teach1, conf_teach1 = pred_t[0]

        # match priors (default boxes) and ground truth boxes
        loc_t = torch.Tensor(num, num_priors, 4)
        conf_t = torch.LongTensor(num, num_priors)
        for idx in range(num):
            truths = targets[idx][:, :-1].data
            labels = targets[idx][:, -1].data
            defaults = priors.data
            match(self.threshold, truths, defaults, self.variance, labels,
                  loc_t, conf_t, idx)

        # wrap targets
        with torch.no_grad():
            if self.use_gpu:
                loc_t = loc_t.cuda(non_blocking=True)
                conf_t = conf_t.cuda(non_blocking=True)

        pos = conf_t &gt; 0 # (1, 0, 1, ...)
        num_pos = pos.sum(dim=1, keepdim=True) # [num, 1], number of positives

        # Localization Loss (Smooth L1)
        # Shape: [batch,num_priors,4]
        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data) # [batch,num_priors,1] before expand_as
        loc_p = loc_data[pos_idx].view(-1, 4)
        loc_t = loc_t[pos_idx].view(-1, 4)
        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)

        # knowledge transfer for loc regression
        # teach1
        loc_teach1_p = loc_teach1[pos_idx].view(-1, 4)
        l2_dis_s = (loc_p - loc_t).pow(2).sum(1)
        l2_dis_s_m = l2_dis_s + self.reg_m
        l2_dis_t = (loc_teach1_p - loc_t).pow(2).sum(1)
        l2_num = l2_dis_s_m &gt; l2_dis_t
        l2_loss_teach1 = l2_dis_s[l2_num].sum()

        l2_loss = l2_loss_teach1

        # Compute max conf across batch for hard negative mining
        batch_conf = conf_data.view(-1, self.num_classes)
        loss_c = log_sum_exp(batch_conf.float()) - batch_conf.gather(1, conf_t.view(-1, 1)).float()

        # Hard Negative Mining
        loss_c[pos.view(-1, 1)] = 0
        loss_c = loss_c.view(num, -1)
        #loss_c[pos] = 0  # filter out pos boxes for now
        _, loss_idx = loss_c.sort(1, descending=True)
        _, idx_rank = loss_idx.sort(1)
        num_pos = pos.long().sum(1, keepdim=True)
        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)
        neg = idx_rank &lt; num_neg.expand_as(idx_rank)

        # Confidence Loss Including Positive and Negative Examples
        # CrossEntropy loss
        pos_idx = pos.unsqueeze(2).expand_as(conf_data) # [batch,num_priors,cls]
        neg_idx = neg.unsqueeze(2).expand_as(conf_data)
        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)
        targets_weighted = conf_t[(pos+neg).gt(0)]
        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)

        # soft loss for Knowledge Distillation
        # teach1
        conf_p_teach = conf_teach1[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)
        pt = F.softmax(conf_p_teach/self.Temp, dim=1)
        if self.neg_w &gt; 1.:
            ps = F.softmax(conf_p/self.Temp, dim=1)
            soft_loss1 = KL_div(pt, ps, self.pos_w, self.neg_w) * (self.Temp**2)
        else:
            ps = F.log_softmax(conf_p/self.Temp, dim=1)
            soft_loss1 = nn.KLDivLoss(size_average=False)(ps, pt) * (self.Temp**2)
        soft_loss = soft_loss1

        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N
        N = num_pos.data.sum().float()
        loss_l = loss_l.float()
        loss_c = loss_c.float()
        loss_l /= N
        loss_c /= N
        l2_loss /= N
        soft_loss /= N
        return loss_l, loss_c, soft_loss, l2_loss
</code></pre>
<p><strong>Paper地址：</strong><a href="https://papers.nips.cc/paper/6676-learning-efficient-object-detection-models-with-knowledge-distillation.pdf" rel="nofollow" data-token="fca54545bdf036a169cb91c82d6f4d07">https://papers.nips.cc/paper/6676-learning-efficient-object-detection-models-with-knowledge-distillation.pdf</a></p>
<p><strong>PyTorch版SSD：</strong><a href="https://github.com/amdegroot/ssd.pytorch" rel="nofollow" data-token="9b07a8a42c632f303511224696b01a07">https://github.com/amdegroot/ssd.pytorch</a></p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>知识蒸馏（Knowledge Distillation）</title>
		<link>https://uzzz.org/article/2457.html</link>
				<pubDate>Mon, 04 Jun 2018 08:55:38 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[DeepLearning]]></category>
		<category><![CDATA[模型压缩]]></category>
		<category><![CDATA[网络优化]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2457.html</guid>
				<description><![CDATA[1、Distilling the Knowledge in a Neural Network Hinton的文章&#8221;Distilling the Knowledge in a Neural Network&#8221;首次提出了知识蒸馏（暗知识提取）的概念，通过引入与教师网络（teacher network：复杂、但推理性能优越）相关的软目标（soft-target）作为total loss的一部分，以诱导学生网络（student network：精简、低复杂度）的训练，实现知识迁移（knowledge transfer）。 如上图所示，教师网络（左侧）的预测输出除以温度参数（Temperature）之后、再做softmax变换，可以获得软化的概率分布（软目标），数值介于0~1之间，取值分布较为缓和。Temperature数值越大，分布越缓和；而Temperature数值减小，容易放大错误分类的概率，引入不必要的噪声。针对较困难的分类或检测任务，Temperature通常取1，确保教师网络中正确预测的贡献。硬目标则是样本的真实标注，可以用one-hot矢量表示。total loss设计为软目标与硬目标所对应的交叉熵的加权平均（表示为KD loss与CE loss），其中软目标交叉熵的加权系数越大，表明迁移诱导越依赖教师网络的贡献，这对训练初期阶段是很有必要的，有助于让学生网络更轻松的鉴别简单样本，但训练后期需要适当减小软目标的比重，让真实标注帮助鉴别困难样本。另外，教师网络的推理性能通常要优于学生网络，而模型容量则无具体限制，且教师网络推理精度越高，越有利于学生网络的学习。 教师网络与学生网络也可以联合训练，此时教师网络的暗知识及学习方式都会影响学生网络的学习，具体如下（式中三项分别为教师网络softmax输出的交叉熵loss、学生网络softmax输出的交叉熵loss、以及教师网络数值输出与学生网络softmax输出的交叉熵loss）： 联合训练的Paper地址：https://arxiv.org/abs/1711.05852 2、Exploring Knowledge Distillation of Deep Neural Networks for Efficient Hardware Solutions 这篇文章将total loss重新定义如下： GitHub地址：https://github.com/peterliht/knowledge-distillation-pytorch total loss的Pytorch代码如下，引入了精简网络输出与教师网络输出的KL散度，并在诱导训练期间，先将teacher network的预测输出缓存到CPU内存中，可以减轻GPU显存的overhead： def loss_fn_kd(outputs, labels, teacher_outputs, params): """ Compute the knowledge-distillation (KD) loss given outputs, labels. "Hyperparameters": temperature and alpha NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher and student expects the input tensor to be log probabilities! See Issue #2 """ alpha = params.alpha T = params.temperature KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1), F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \ &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;F.cross_entropy(outputs, labels) * (1. - alpha) return KD_loss 3、Ensemble of Multiple Teachers 第一种算法：多个教师网络输出的soft label按加权组合，构成统一的soft label，然后指导学生网络的训练： 第二种算法：由于加权平均方式会弱化、平滑多个教师网络的预测结果，因此可以随机选择某个教师网络的soft label作为guidance： 第三种算法：同样地，为避免加权平均带来的平滑效果，首先采用教师网络输出的soft label重新标注样本、增广数据、再用于模型训练，该方法能够让模型学会从更多视角观察同一样本数据的不同功能： Paper地址： https://www.researchgate.net/publication/319185356_Efficient_Knowledge_Distillation_from_an_Ensemble_of_Teachers 4、Hint-based Knowledge Transfer 为了能够诱导训练更深、更纤细的学生网络（deeper and thinner FitNet），需要考虑教师网络中间层的Feature Maps（作为Hint），用来指导学生网络中相应的Guided layer。此时需要引入L2 loss指导训练过程，该loss计算为教师网络Hint layer与学生网络Guided layer输出Feature Maps之间的差别，若二者输出的Feature Maps形状不一致，Guided layer需要通过一个额外的回归层，具体如下： 具体训练过程分两个阶段完成：第一个阶段利用Hint-based loss诱导学生网络达到一个合适的初始化状态（只更新W_Guided与W_r）；第二个阶段利用教师网络的soft label指导整个学生网络的训练（即知识蒸馏），且total loss中soft target相关部分所占比重逐渐降低，从而让学生网络能够全面辨别简单样本与困难样本（教师网络能够有效辨别简单样本，而困难样本则需要借助真实标注，即hard target）： Paper地址：https://arxiv.org/abs/1412.6550 GitHub地址：https://github.com/adri-romsor/FitNets 5、Attention to Attention Transfer 通过网络中间层的attention map，完成teacher network与student network之间的知识迁移。考虑给定的tensor A，基于activation的attention map可以定义为如下三种之一： 随着网络层次的加深，关键区域的attention-level也随之提高。文章最后采用了第二种形式的attention map，取p=2，并且activation-based attention map的知识迁移效果优于gradient-based attention map，loss定义及迁移过程如下： Paper地址：https://arxiv.org/abs/1612.03928 GitHub地址：https://github.com/szagoruyko/attention-transfer 6、Flow&#160;of the Solution Procedure 暗知识亦可表示为训练的求解过程（FSP: Flow of the Solution Procedure），教师网络或学生网络的FSP矩阵定义如下（Gram形式的矩阵）： 训练的第一阶段：最小化教师网络FSP矩阵与学生网络FSP矩阵之间的L2 Loss，初始化学生网络的可训练参数： 训练的第二阶段：在目标任务的数据集上fine-tune学生网络。从而达到知识迁移、快速收敛、以及迁移学习的目的。 Paper地址：]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
<h1>1、Distilling the Knowledge in a Neural Network</h1>
<p>Hinton的文章&#8221;Distilling the Knowledge in a Neural Network&#8221;首次提出了知识蒸馏（暗知识提取）的概念，通过引入与教师网络（teacher network：复杂、但推理性能优越）相关的软目标（soft-target）作为total loss的一部分，以诱导学生网络（student network：精简、低复杂度）的训练，实现知识迁移（knowledge transfer）。</p>
<p style="text-align:center;"><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180604160949186?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
<p>如上图所示，教师网络（左侧）的预测输出除以温度参数（Temperature）之后、再做softmax变换，可以获得软化的概率分布（软目标），数值介于0~1之间，取值分布较为缓和。Temperature数值越大，分布越缓和；而Temperature数值减小，容易放大错误分类的概率，引入不必要的噪声。针对较困难的分类或检测任务，Temperature通常取1，确保教师网络中正确预测的贡献。硬目标则是样本的真实标注，可以用one-hot矢量表示。total loss设计为软目标与硬目标所对应的交叉熵的加权平均（表示为KD loss与CE loss），其中软目标交叉熵的加权系数越大，表明迁移诱导越依赖教师网络的贡献，这对训练初期阶段是很有必要的，有助于让学生网络更轻松的鉴别简单样本，但训练后期需要适当减小软目标的比重，让真实标注帮助鉴别困难样本。另外，教师网络的推理性能通常要优于学生网络，而模型容量则无具体限制，且教师网络推理精度越高，越有利于学生网络的学习。</p>
<p>教师网络与学生网络也可以联合训练，此时教师网络的暗知识及学习方式都会影响学生网络的学习，具体如下（式中三项分别为教师网络softmax输出的交叉熵loss、学生网络softmax输出的交叉熵loss、以及教师网络数值输出与学生网络softmax输出的交叉熵loss）：</p>
<p><strong>联合训练的Paper地址：</strong><a href="https://arxiv.org/abs/1711.05852" rel="nofollow" data-token="7c9a7dab2065570ef96d38a9c0285090">https://arxiv.org/abs/1711.05852</a></p>
<p style="text-align:center;"><img alt="" class="has" height="44" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180906092731238?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="540"></p>
<p style="text-align:center;"><img alt="" class="has" height="331" src="https://uzshare.com/_p?https://img-blog.csdn.net/201809060928044?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="673"></p>
<h1>2、Exploring Knowledge Distillation of Deep Neural Networks for Efficient Hardware Solutions</h1>
<p>这篇文章将total loss重新定义如下：</p>
<p style="text-align:center;"><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180604162913410?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
<p><strong>GitHub地址：</strong><a href="https://github.com/peterliht/knowledge-distillation-pytorch" rel="nofollow" data-token="fbeb707ed47634087a5e836b433b1f02">https://github.com/peterliht/knowledge-distillation-pytorch</a></p>
<p>total loss的Pytorch代码如下，引入了精简网络输出与教师网络输出的KL散度，并在诱导训练期间，先将teacher network的预测输出缓存到CPU内存中，可以减轻GPU显存的overhead：</p>
<pre class="has">
<code class="language-python">def loss_fn_kd(outputs, labels, teacher_outputs, params):
    """
    Compute the knowledge-distillation (KD) loss given outputs, labels.
    "Hyperparameters": temperature and alpha
    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher
    and student expects the input tensor to be log probabilities! See Issue #2
    """
    alpha = params.alpha
    T = params.temperature
    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1),
                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \
              &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;F.cross_entropy(outputs, labels) * (1. - alpha)

    return KD_loss</code></pre>
<h1>3、Ensemble of Multiple Teachers</h1>
<p><strong>第一种算法：</strong>多个教师网络输出的soft label按加权组合，构成统一的soft label，然后指导学生网络的训练：</p>
<p style="text-align:center;"><img alt="" class="has" height="127" src="https://uzshare.com/_p?https://img-blog.csdn.net/2018090414465243?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="530"></p>
<p style="text-align:center;"><img alt="" class="has" height="270" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180904144737333?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="535"></p>
<p><strong>第二种算法：</strong>由于加权平均方式会弱化、平滑多个教师网络的预测结果，因此可以随机选择某个教师网络的soft label作为guidance：</p>
<p style="text-align:center;"><img alt="" class="has" height="200" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180904145125595?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="539"></p>
<p><strong>第三种算法：</strong>同样地，为避免加权平均带来的平滑效果，首先采用教师网络输出的soft label重新标注样本、增广数据、再用于模型训练，该方法能够让模型学会从更多视角观察同一样本数据的不同功能：</p>
<p style="text-align:center;"><img alt="" class="has" height="224" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180904145903860?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="536"></p>
<p><strong>Paper地址：</strong></p>
<p><a href="https://www.researchgate.net/publication/319185356_Efficient_Knowledge_Distillation_from_an_Ensemble_of_Teachers" rel="nofollow" data-token="6cdd0157e63dc26e6c9f039d431913ad">https://www.researchgate.net/publication/319185356_Efficient_Knowledge_Distillation_from_an_Ensemble_of_Teachers</a></p>
<h1>4、Hint-based Knowledge Transfer</h1>
<p>为了能够诱导训练更深、更纤细的学生网络（deeper and thinner FitNet），需要考虑教师网络中间层的Feature Maps（作为Hint），用来指导学生网络中相应的Guided layer。此时需要引入L2 loss指导训练过程，该loss计算为教师网络Hint layer与学生网络Guided layer输出Feature Maps之间的差别，若二者输出的Feature Maps形状不一致，Guided layer需要通过一个额外的回归层，具体如下：</p>
<p style="text-align:center;"><img alt="" class="has" height="59" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180904151630725?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="709"></p>
<p style="text-align:center;"><img alt="" class="has" height="317" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180904150852311?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="863"></p>
<p><strong>具体训练过程分两个阶段完成：</strong>第一个阶段利用Hint-based loss诱导学生网络达到一个合适的初始化状态（只更新W_Guided与W_r）；第二个阶段利用教师网络的soft label指导整个学生网络的训练（即知识蒸馏），且total loss中soft target相关部分所占比重逐渐降低，从而让学生网络能够全面辨别简单样本与困难样本（教师网络能够有效辨别简单样本，而困难样本则需要借助真实标注，即hard target）：</p>
<p style="text-align:center;"><img alt="" class="has" height="434" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180904152348516?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="927"></p>
<p><strong>Paper地址：</strong><a href="https://arxiv.org/abs/1412.6550" rel="nofollow" data-token="63791d3bb0e636fd89563858176c43bc">https://arxiv.org/abs/1412.6550</a></p>
<p><strong>GitHub地址：</strong><a href="https://github.com/adri-romsor/FitNets" rel="nofollow" data-token="e73553587403dd4563b3accafc83a9c4">https://github.com/adri-romsor/FitNets</a></p>
<h1>5、Attention to Attention Transfer</h1>
<p>通过网络中间层的attention map，完成teacher network与student network之间的知识迁移。考虑给定的tensor A，基于activation的attention map可以定义为如下三种之一：</p>
<p style="text-align:center;"><img alt="" class="has" height="114" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180925094725131?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="830"></p>
<p>随着网络层次的加深，关键区域的attention-level也随之提高。文章最后采用了第二种形式的attention map，取p=2，并且activation-based attention map的知识迁移效果优于gradient-based attention map，loss定义及迁移过程如下：</p>
<p style="text-align:center;"><img alt="" class="has" height="67" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180925100235962?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="459"></p>
<p style="text-align:center;"><img alt="" class="has" height="32" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180925100347190?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="414"></p>
<p style="text-align:center;"><img alt="" class="has" height="217" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180925100418174?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="866"></p>
<p><strong>Paper地址：</strong><a href="https://arxiv.org/abs/1612.03928" rel="nofollow" data-token="29a0b52ce6f95ffa88749696b80cb572">https://arxiv.org/abs/1612.03928</a></p>
<p><strong>GitHub地址：</strong><a href="https://github.com/szagoruyko/attention-transfer" rel="nofollow" data-token="712ec1aa5770d9c436f3ec4d433bad42">https://github.com/szagoruyko/attention-transfer</a></p>
<h1>6、Flow&nbsp;of the Solution Procedure</h1>
<p>暗知识亦可表示为训练的求解过程（FSP: Flow of the Solution Procedure），教师网络或学生网络的FSP矩阵定义如下（Gram形式的矩阵）：</p>
<p style="text-align:center;"><img alt="" class="has" height="91" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180908190729629?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="576"></p>
<p style="text-align:center;"><img alt="" class="has" height="343" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180908190755611?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="611"></p>
<p>训练的第一阶段：最小化教师网络FSP矩阵与学生网络FSP矩阵之间的L2 Loss，初始化学生网络的可训练参数：</p>
<p style="text-align:center;"><img alt="" class="has" height="420" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180908191130435?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="918"></p>
<p>训练的第二阶段：在目标任务的数据集上fine-tune学生网络。从而达到知识迁移、快速收敛、以及迁移学习的目的。</p>
<p><strong>Paper地址：</strong></p>
<p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf" rel="nofollow" data-token="7e1d838d35fcd3bb2deaa4697418f1da">http://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf</a></p>
<h1>7、Knowledge Distillation with Adversarial Samples&nbsp;Supporting Decision Boundary</h1>
<p>从分类的决策边界角度分析，知识迁移过程亦可理解为教师网络诱导学生网络有效鉴别决策边界的过程，鉴别能力越强意味着模型的泛化能力越好：</p>
<p style="text-align:center;"><img alt="" class="has" height="323" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180920145521968?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="886"></p>
<p>文章首先利用对抗攻击策略（adversarial attacking）将基准类样本（base class sample）转为目标类样本、且位于决策边界附近（BSS: boundary supporting sample），进而利用对抗生成的样本诱导学生网络的训练，可有效提升学生网络对决策边界的鉴别能力。文章采用迭代方式生成对抗样本，需要沿loss function（基准类得分与目标类得分之差）的梯度负方向调整样本，直到满足停止条件为止：</p>
<p style="text-align:center;"><img alt="" class="has" height="283" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180920150428101?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="716"></p>
<p>loss function：</p>
<p style="text-align:center;"><img alt="" class="has" height="30" src="https://uzshare.com/_p?https://img-blog.csdn.net/2018092015152089?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="234"></p>
<p>沿loss function的梯度负方向调整样本：</p>
<p style="text-align:center;"><img alt="" class="has" height="68" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180920151540465?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="408"></p>
<p>停止条件（只要满足三者之一）：</p>
<p style="text-align:center;"><img alt="" class="has" height="136" src="https://uzshare.com/_p?https://img-blog.csdn.net/2018092015160434?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="439"></p>
<p>结合对抗生成的样本，利用教师网络训练学生网络所需的total loss包含CE loss、KD loss以及boundary supporting loss（BS loss）：</p>
<p style="text-align:center;"><img alt="" class="has" height="302" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180920151838819?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="886"></p>
<p><strong>Paper地址：</strong><a href="https://arxiv.org/abs/1805.05532" rel="nofollow" data-token="74570507cb0a0f46a8e3d48366718b46">https://arxiv.org/abs/1805.05532</a></p>
<h1>8、Label Refinery：Improving ImageNet Classification through Label Progression</h1>
<p>这篇文章通过迭代式的诱导训练，主要解决训练期间样本的crop与label不一致的问题，以增强label的质量，从而进一步增强模型的泛化能力：</p>
<p style="text-align:center;"><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180604163723669?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
<p>诱导过程中，total loss表示为本次迭代（t&gt;1）网络的预测输出（概率分布）与上一次迭代输出（Label Refinery：类似于教师网络的角色）的KL散度：</p>
<p style="text-align:center;"><img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdn.net/20180604164358160?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25hdHVyZTU1Mzg2Mw==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70"></p>
<p>文章实验部分表明，不仅可以用训练网络作为Label Refinery Network，也可以用其他高质量网络（如Resnet50）作为Label Refinery Network。并在诱导过程中，能够对抗生成样本，实现数据增强。</p>
<p><strong>GitHub地址：</strong><a href="https://github.com/hessamb/label-refinery" rel="nofollow" data-token="e1838d413d06c4f8fdae77d271ab51dc">https://github.com/hessamb/label-refinery</a></p>
<h1>9、Miscellaneous</h1>
<p>&#8212;&#8212;&#8211; 知识蒸馏可以与量化结合使用，考虑了中间层Feature Maps之间的关系，<strong>可参考：</strong></p>
<p><a href="https://blog.csdn.net/nature553863/article/details/82147933" rel="nofollow" data-token="63b58e5f5b16fd5081d344dea82399be">https://blog.csdn.net/nature553863/article/details/82147933</a></p>
<p>&#8212;&#8212;&#8211; 知识蒸馏与Hint Learning相结合，可以训练精简的Faster-RCNN，<strong>可参考：</strong></p>
<p><a href="https://blog.csdn.net/nature553863/article/details/82463249" rel="nofollow" data-token="c73c3b240a8489852a4ec253d9651f42">https://blog.csdn.net/nature553863/article/details/82463249</a></p>
<p>&#8212;&#8212;&#8211; 模型压缩方面，更为详细的讨论，<strong>请参考：</strong></p>
<p><a href="https://blog.csdn.net/nature553863/article/details/81083955" rel="nofollow" data-token="23973bd49e9fd025e976be4cc0a83d29">https://blog.csdn.net/nature553863/article/details/81083955</a></p>
</p></div>
</div>
]]></content:encoded>
										</item>
	</channel>
</rss>
