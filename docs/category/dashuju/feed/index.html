<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>大数据 &#8211; 有组织在!</title>
	<atom:link href="https://uzzz.org/category/dashuju/feed" rel="self" type="application/rss+xml" />
	<link>https://uzzz.org/</link>
	<description></description>
	<lastBuildDate>Mon, 26 Aug 2019 06:22:59 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.4</generator>

<image>
	<url>https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png</url>
	<title>大数据 &#8211; 有组织在!</title>
	<link>https://uzzz.org/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>一周新闻纵览：工信部组织召开综合整治骚扰电话专项行动；智能锁百万指纹泄密；4G不会降速5G网速会更快</title>
		<link>https://uzzz.org/article/869.html</link>
				<pubDate>Mon, 26 Aug 2019 06:22:59 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[信息安全]]></category>
		<category><![CDATA[大数据]]></category>
		<category><![CDATA[数据安全]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/869.html</guid>
				<description><![CDATA[不知不觉 又到了周五了 伴随着一周的小尾巴 开始我们今天的一周IT新闻盘点吧 &#160; 我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系。—杨绛 &#160; 1 &#160; 网络黑产无孔不入 &#160; 随着社会进入数字经济时代，网络黑产不再是散兵游勇式的单打独斗，已经演化成具有专业分工、链条化运作特征的产业。提及网络黑产，就绕不开暗网，暗网复杂隐蔽，承载着大量的信息资源且鱼龙混杂，成为助长网络犯罪的重要工具。 &#160; 2 工信部组织召开综合整治骚扰电话专项行动 &#160; 8月15日，工信部信息通信管理局在北京组织召开综合整治骚扰电话专项行动工作会，进一步加大骚扰电话源头治理力度，深入推进骚扰电话综合整治。最高人民法院、最高人民检察院、教育部、公安部、司法部等十三部门参加会议。 &#160; 3 &#160;智能锁百万指纹泄密，攻击者可进入任何地方！ &#160; 近日，研究人员在生物识别公司Suprema的智能锁系统中发现了一个可公开访问的生物识别数据库安全漏洞，能够访问超过100万人的身份验证数据。该数据库超过2780万条记录的数据，包括指纹、面部识别数据，甚至还有员工的个人信息。 &#160; 4 三大运营商回应降速传言&#160;4G不会降速 &#160; 近日，有传言称4G网络速度下降，目的是“为了推广50”。对此，记者分别致电三大运营商，相关客服工作人员均对此表示否认，并表示5G速度会更快，没有接到4G会降速的通知。 &#160; 对于5G推出了4G是否会降速的疑问，一位客服明确表示，“5G网速会更快，4G不会降速。”他表示，没有接到公司通知。 &#160; 独立电信分析师付亮指出，从日常使用来看，4G的下载速率通常在数十兆的量级，20-30Mbps、30-40Mbps的速率均属于正常范围，也较为常见。关于5G推广对4G网速是否会有影响，付亮认为，目前阶段5G非独立组网与4G有共用资源，从这角度看有可能对部分双载波4G+的百兆以上超高网速略有降低，但对普通4G用户来说不会有影响，并不会带来4G降速。 &#160; 三大运营商客服昨日回复相关咨询时均表示，用户只需更换一部56手机，不换卡不换号即可接收到56信号。同时运营商目前相应的5G体验包提供免费体验流量。 &#160; 5 数字人民币初露真容：非网络支付或电子钱包 &#160; 在日前举行的第三届中国金融四十人伊春论坛上，央行支付结算司副司长穆长春表示，央行下属的数字货币研究所早于2018年就开始了数字货币系统的开发，央行数字货币已是“呼之欲出”。目前，该所已经申请了74项涉及数字货币技术的专利。央行数字货币是基于国家信用、由央行发行的法定数字货币，与比特币等“虚拟货币”有着本质区别。穆长春表示，以往电子支付工具的资金转移必须通过传统银行账户才能完成，而央行数字货币可脱离传统银行账户实现价值转移，使交易环节对账户依赖程度大为降低。 &#160; 6 蓝牙存在泄露隐私风险，你一直开着吗? &#160; &#160;波士顿大学研究人员发现，部分蓝牙设备上，蓝牙通信协议存在的漏洞，会导致个人信息被窃取，允许第三方追踪设备所在位置。Windows 10、ioS、macOS等软件系统以及Apple Watch、Fitbit手环等设备上都存在此漏洞。 &#160; 7 App存隐忧莫让便民利器成扰民工具 &#160; &#160;在对上百款App代码进行了“全景扫描”后发现，有超8成App安装包中均含有超出其业务范围的权限代码；有过半数的App含有索取用户通讯录的代码。这意味着，用户对“隐私协议”点下“我同意”的按钮时，也向App敞开了隐私的大门。 &#160; 8 &#160;App超半数留索取用户通讯录&#8221;后门” &#160; 据报道，109款App中嘀嗒出行、百合婚恋、和包支付、瑞钱包、e代驾、飞嘀打车、中国工商银行、悟空理财、平安好医生、开心消消乐10个App申请的敏感权限最多；83.6%的App含越界代码，中移动旗下的和包支付“越界”严重。 &#160; 9 大量App存在读写用户设备文件等异常行为 &#160; 国家互联网应急中心8月13日发布《2019年上半年我国互联网网络安全态势》。这份报告指出，我国移动APP违法违规使用个人信息问题十分突出，大量APP存在探测其他APP或读写用户设备文件等异常行为，对用户的个人信息安全造成潜在安全威胁。 &#160; 10 iOS 12.4系统遭黑客破解 漏洞危及数百万用户 &#160; 在最近的一次iOS 12.4系统更新中，苹果公司无意中恢复了在先前版本中修复过的一个漏洞，从而使得当前iOS 12.4版本系统遭到了黑客的破解，继而为众多iPhone用户带来了多年以来唯一一次最新系统的公开越狱机会，而这也将为苹果公司带来一系列的安全风险。&#160;]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<p>不知不觉</p>
<p>又到了周五了</p>
<p>伴随着一周的小尾巴</p>
<p>开始我们今天的一周IT新闻盘点吧</p>
<p>&nbsp;</p>
<p>我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系。—杨绛</p>
<p>&nbsp;</p>
<p>1</p>
<p>&nbsp; 网络黑产无孔不入</p>
<p>&nbsp;</p>
<p>随着社会进入数字经济时代，网络黑产不再是散兵游勇式的单打独斗，已经演化成具有专业分工、链条化运作特征的产业。提及网络黑产，就绕不开暗网，暗网复杂隐蔽，承载着大量的信息资源且鱼龙混杂，成为助长网络犯罪的重要工具。</p>
<p>&nbsp;</p>
<p>2</p>
<p>工信部组织召开综合整治骚扰电话专项行动</p>
<p>&nbsp;</p>
<p>8月15日，工信部信息通信管理局在北京组织召开综合整治骚扰电话专项行动工作会，进一步加大骚扰电话源头治理力度，深入推进骚扰电话综合整治。最高人民法院、最高人民检察院、教育部、公安部、司法部等十三部门参加会议。</p>
<p>&nbsp;</p>
<p>3</p>
<p>&nbsp;智能锁百万指纹泄密，攻击者可进入任何地方！</p>
<p>&nbsp;</p>
<p>近日，研究人员在生物识别公司Suprema的智能锁系统中发现了一个可公开访问的生物识别数据库安全漏洞，能够访问超过100万人的身份验证数据。该数据库超过2780万条记录的数据，包括指纹、面部识别数据，甚至还有员工的个人信息。</p>
<p>&nbsp;</p>
<p>4</p>
<p>三大运营商回应降速传言&nbsp;4G不会降速</p>
<p>&nbsp;</p>
<p>近日，有传言称4G网络速度下降，目的是“为了推广50”。对此，记者分别致电三大运营商，相关客服工作人员均对此表示否认，并表示5G速度会更快，没有接到4G会降速的通知。</p>
<p>&nbsp;</p>
<p>对于5G推出了4G是否会降速的疑问，一位客服明确表示，“5G网速会更快，4G不会降速。”他表示，没有接到公司通知。</p>
<p>&nbsp;</p>
<p>独立电信分析师付亮指出，从日常使用来看，4G的下载速率通常在数十兆的量级，20-30Mbps、30-40Mbps的速率均属于正常范围，也较为常见。关于5G推广对4G网速是否会有影响，付亮认为，目前阶段5G非独立组网与4G有共用资源，从这角度看有可能对部分双载波4G+的百兆以上超高网速略有降低，但对普通4G用户来说不会有影响，并不会带来4G降速。</p>
<p>&nbsp;</p>
<p>三大运营商客服昨日回复相关咨询时均表示，用户只需更换一部56手机，不换卡不换号即可接收到56信号。同时运营商目前相应的5G体验包提供免费体验流量。</p>
<p>&nbsp;</p>
<p>5</p>
<p>数字人民币初露真容：非网络支付或电子钱包</p>
<p>&nbsp;</p>
<p>在日前举行的第三届中国金融四十人伊春论坛上，央行支付结算司副司长穆长春表示，央行下属的数字货币研究所早于2018年就开始了数字货币系统的开发，央行数字货币已是“呼之欲出”。目前，该所已经申请了74项涉及数字货币技术的专利。央行数字货币是基于国家信用、由央行发行的法定数字货币，与比特币等“虚拟货币”有着本质区别。穆长春表示，以往电子支付工具的资金转移必须通过传统银行账户才能完成，而央行数字货币可脱离传统银行账户实现价值转移，使交易环节对账户依赖程度大为降低。</p>
<p>&nbsp;</p>
<p>6</p>
<p>蓝牙存在泄露隐私风险，你一直开着吗?</p>
<p>&nbsp;</p>
<p>&nbsp;波士顿大学研究人员发现，部分蓝牙设备上，蓝牙通信协议存在的漏洞，会导致个人信息被窃取，允许第三方追踪设备所在位置。Windows 10、ioS、macOS等软件系统以及Apple Watch、Fitbit手环等设备上都存在此漏洞。</p>
<p>&nbsp;</p>
<p>7</p>
<p>App存隐忧莫让便民利器成扰民工具</p>
<p>&nbsp;</p>
<p>&nbsp;在对上百款App代码进行了“全景扫描”后发现，有超8成App安装包中均含有超出其业务范围的权限代码；有过半数的App含有索取用户通讯录的代码。这意味着，用户对“隐私协议”点下“我同意”的按钮时，也向App敞开了隐私的大门。</p>
<p>&nbsp;</p>
<p>8</p>
<p>&nbsp;App超半数留索取用户通讯录&#8221;后门”</p>
<p>&nbsp;</p>
<p>据报道，109款App中嘀嗒出行、百合婚恋、和包支付、瑞钱包、e代驾、飞嘀打车、中国工商银行、悟空理财、平安好医生、开心消消乐10个App申请的敏感权限最多；83.6%的App含越界代码，中移动旗下的和包支付“越界”严重。</p>
<p>&nbsp;</p>
<p>9</p>
<p>大量App存在读写用户设备文件等异常行为</p>
<p>&nbsp;</p>
<p>国家互联网应急中心8月13日发布《2019年上半年我国互联网网络安全态势》。这份报告指出，我国移动APP违法违规使用个人信息问题十分突出，大量APP存在探测其他APP或读写用户设备文件等异常行为，对用户的个人信息安全造成潜在安全威胁。</p>
<p>&nbsp;</p>
<p>10</p>
<p>iOS 12.4系统遭黑客破解 漏洞危及数百万用户</p>
<p>&nbsp;</p>
<p>在最近的一次iOS 12.4系统更新中，苹果公司无意中恢复了在先前版本中修复过的一个漏洞，从而使得当前iOS 12.4版本系统遭到了黑客的破解，继而为众多iPhone用户带来了多年以来唯一一次最新系统的公开越狱机会，而这也将为苹果公司带来一系列的安全风险。&nbsp;</p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>Graph Convolutional Neural Networks for Web-Scale Recommender Systems（用于Web级推荐系统的图形卷积神经网络）</title>
		<link>https://uzzz.org/article/1588.html</link>
				<pubDate>Sun, 07 Jul 2019 01:52:26 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[DeepLearning]]></category>
		<category><![CDATA[人工智能]]></category>
		<category><![CDATA[大数据]]></category>
		<category><![CDATA[搜索]]></category>
		<category><![CDATA[论文]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1588.html</guid>
				<description><![CDATA[Graph Convolutional Neural Networks for Web-Scale Recommender Systems 用于Web级推荐系统的图形卷积神经网络 ABSTRACT Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge. Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model. We deploy PinSage at Pinterest and train it on 7.5 billion exam-ples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<h4><a id="Graph_Convolutional_Neural_Networks_for_WebScale_Recommender_Systems_0"></a>Graph Convolutional Neural Networks for Web-Scale Recommender Systems</h4>
<h3><a id="Web_2"></a>用于Web级推荐系统的图形卷积神经网络</h3>
<h6><a id="ABSTRACT_4"></a>ABSTRACT</h6>
<p>Recent advancements in deep neural networks for graph-structured data have led to state-of-the-art performance on recommender system benchmarks. However, making these methods practical and scalable to web-scale recommendation tasks with billions of items and hundreds of millions of users remains a challenge.</p>
<p>Here we describe a large-scale deep recommendation engine that we developed and deployed at Pinterest. We develop a data-efficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings of nodes (i.e., items) that incorporate both graph structure as well as node feature information. Compared to prior GCN approaches, we develop a novel method based on highly efficient random walks to structure the convolutions and design a novel training strategy that relies on harder-and-harder training examples to improve robustness and convergence of the model.</p>
<p>We deploy PinSage at Pinterest and train it on 7.5 billion exam-ples on a graph with 3 billion nodes representing pins and boards, and 18 billion edges. According to offline metrics, user studies and A/B tests, PinSage generates higher-quality recommendations than comparable deep learning and graph-based alternatives. To our knowledge, this is the largest application of deep graph embed-dings to date and paves the way for a new generation of web-scale recommender systems based on graph convolutional architectures.<br /> 用于图形结构数据的深度神经网络的最新进展已经在推荐器系统基准上产生了最先进的性能。然而，使这些方法实用且可扩展到具有数十亿项目和数亿用户的网络规模推荐任务仍然是一项挑战。</p>
<p>在这里，我们描述了我们在Pinterest开发和部署的大规模深度推荐引擎。我们开发了一种数据有效的图形卷积网络（GCN）算法PinSage，它结合了有效的随机游走和图形卷积，以生成包含图形结构和节点特征信息的节点（即项目）的嵌入。与之前的GCN方法相比，我们开发了一种基于高效随机游走的新方法来构建卷积并设计一种新的训练策略，该策略依赖于更难和更难的训练示例来提高模型的鲁棒性和收敛性。</p>
<p>我们在Pinterest部署了PinSage，并在图表上培训了75亿个例子，其中30亿个节点代表引脚和电路板，以及180亿个边缘。根据离线指标，用户研究和A / B测试，PinSage可提供比可比较的深度学习和基于图形的替代方案更高质量的建议。据我们所知，这是迄今为止最大的深度图嵌入应用，并为基于图卷积结构的新一代Web级推荐系统铺平了道路。</p>
<h4><a id="1	INTRODUCTION_17"></a>1 INTRODUCTION</h4>
<p>Deep learning methods have an increasingly critical role in rec-ommender system applications, being used to learn useful low-dimensional embeddings of images, text, and even individual users [9, 12]. The representations learned using deep models can be used to complement, or even replace, traditional recommendation algo-rithms like collaborative filtering. and these learned representations have high utility because they can be re-used in various recom-mendation tasks. For example, item embeddings learned using a deep model can be used for item-item recommendation and also to recommended themed collections (e.g., playlists, or “feed” content).</p>
<p>Recent years have seen significant developments in this space— especially the development of new deep learning methods that are capable of learning on graph-structured data, which is fundamen-tal for recommendation applications (e.g., to exploit user-to-item interaction graphs as well as social graphs) [6, 19, 21, 24, 29, 30].</p>
<p>Most prominent among these recent advancements is the suc-cess of deep learning architectures known as Graph Convolutional Networks (GCNs) [19, 21, 24, 29]. The core idea behind GCNs is to learn how to iteratively aggregate feature information from lo-cal graph neighborhoods using neural networks (Figure 1). Here a single “convolution” operation transforms and aggregates feature information from a node’s one-hop graph neighborhood, and by stacking multiple such convolutions information can be propagated across far reaches of a graph. Unlike purely content-based deep models (e.g., recurrent neural networks [3]), GCNs leverage both content information as well as graph structure. GCN-based methods have set a new standard on countless recommender system bench-marks (see [19] for a survey). However, these gains on benchmark tasks have yet to be translated to gains in real-world production environments.</p>
<p>深度学习方法在调用系统应用程序中具有越来越重要的作用，用于学习图像，文本甚至个人用户的有用的低维嵌入[9,12]。使用深度模型学习的表示可用于补充甚至替代传统的推荐算法，如协同过滤。并且这些学习的表示具有很高的实用性，因为它们可以在各种推荐任务中重复使用。例如，使用深度模型学习的项目嵌入可以用于项目项目推荐以及推荐的主题集合（例如，播放列表或“馈送”内容）。</p>
<p>近年来，这一领域取得了重大进展 &#8211; 尤其是新的深度学习方法的开发，这些方法能够学习图形结构数据，这是推荐应用的基础（例如，利用用户到项目的交互图形以及社交图表）[6,19,21,24,29,30]。</p>
<p>这些最新进展中最突出的是深度学习架构的成功，称为图形卷积网络（GCN）[19,21,24,29]。 GCN背后的核心思想是学习如何使用神经网络从lo-cal图形邻域迭代地聚合特征信息（图1）。这里，单个“卷积”操作转换并聚合来自节点的单跳图邻域的特征信息，并且通过堆叠多个这样的卷积信息可以在图的远端传播。与纯粹基于内容的深层模型（例如，递归神经网络[3]）不同，GCN利用内容信息和图形结构。基于GCN的方法为无数的推荐系统基准设定了新的标准（参见[19]的一项调查）。但是，基准任务的这些收益尚未转化为实际生产环境中的收益。</p>
<p>The main challenge is to scale both the training as well as in-ference of GCN-based node embeddings to graphs with billions of nodes and tens of billions of edges. Scaling up GCNs is difficult because many of the core assumptions underlying their design are violated when working in a big data environment. For example, all existing GCN-based recommender systems require operating on the full graph Laplacian during training—an assumption that is infeasible when the underlying graph has billions of nodes and whose structure is constantly evolving.</p>
<p>Present work. Here we present a highly-scalable GCN framework that we have developed and deployed in production at Pinterest. Our framework, a random-walk-based GCN named PinSage, operates on a massive graph with 3 billion nodes and 18 billion edges—a graph that is 10, 000× larger than typical applications of GCNs. PinSage leverages several key insights to drastically improve the scalability of GCNs:<br /> 主要的挑战是将基于GCN的节点嵌入的训练和推理扩展到具有数十亿个节点和数百亿个边缘的图形。 扩展GCN很困难，因为在大数据环境中工作时，其设计的许多核心假设都会受到侵犯。 例如，所有现有的基于GCN的推荐系统都需要在训练期间对完整图拉普拉斯算子进行操作 &#8211; 当基础图具有数十亿个节点且其结构不断发展时，这种假设是不可行的。</p>
<p>目前的工作。 在这里，我们提出了一个高度可扩展的GCN框架，我们在Pinterest的生产中开发和部署了该框架。 我们的框架是一个名为PinSage的基于随机游走的GCN，它运行在一个包含30亿个节点和180亿个边缘的大型图形上 &#8211; 图形比GCN的典型应用程序大10,000倍。 PinSage利用几个关键见解来大幅提高GCN的可扩展性：</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707091551510.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707091641450.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 图1：使用深度2卷积的模型架构概述（最好以彩色查看）。 左：一个小例子输入</p>
<p>图形。 右：使用前一层表示计算节点A的嵌入hA（2）的2层神经网络，<br /> 节点A的hA（1）和其邻域N（A）（节点B，C，D）的hA（1）。 （然而，邻域的概念是通用的，并不是所有邻居都需要包括在内（第3.2节）。）底部：计算输入图的每个节点的嵌入的神经网络。 虽然神经网络在节点之间不同，但它们都共享相同的参数集（即，卷积（1）和卷积（2）函数的参数;算法1）。 具有相同阴影图案的框共享参数; γ表示重要性汇集函数; 薄矩形框表示密集连接的多层神经网络。</p>
<ul>
<li>
<p>•On-the-fly convolutions: Traditional GCN algorithms per-form graph convolutions by multiplying feature matrices by powers of the full graph Laplacian. In contrast, our PinSage algo-rithm performs efficient, localized convolutions by sampling the neighborhood around a node and dynamically constructing a computation graph from this sampled neighborhood. These dy-namically constructed computation graphs (Fig. 1) specify how to perform a localized convolution around a particular node, and alleviate the need to operate on the entire graph during training.</p>
</li>
<li>
<p>•Producer-consumer minibatch construction: We develop a producer-consumer architecture for constructing minibatches that ensures maximal GPU utilization during model training. A large-memory, CPU-bound producer efficiently samples node network neighborhoods and fetches the necessary features to define local convolutions, while a GPU-bound TensorFlow model consumes these pre-defined computation graphs to efficiently run stochastic gradient decent.<br /> •Efficient MapReduce inference: Given a fully-trained GCN model, we design an efficient MapReduce pipeline that can dis-tribute the trained model to generate embeddings for billions of nodes, while minimizing repeated computations.</p>
</li>
</ul>
<p>In addition to these fundamental advancements in scalability, we also introduce new training techniques and algorithmic innova-tions. These innovations improve the quality of the representations learned by PinSage, leading significant performance gains in down-stream recommender system tasks:</p>
<ul>
<li>Constructing convolutions via random walks: Taking full neighborhoods of nodes to perform convolutions (Fig. 1) would result in huge computation graphs, so we resort to sampling. However, random sampling is suboptimal, and we develop a new technique using short random walks to sample the computa-tion graph. An additional benefit is that each node now has an importance score, which we use in the pooling/aggregation step.</li>
<li>•Importance pooling: A core component of graph convolutions is the aggregation of feature information from local neighbor-hoods in the graph. We introduce a method to weigh the impor-tance of node features in this aggregation based upon random-walk similarity measures, leading to a 46% performance gain in offline evaluation metrics.</li>
<li>•Curriculum training: We design a curriculum training scheme, where the algorithm is fed harder-and-harder examples during training, resulting in a 12% performance gain.<br /> We have deployed PinSage for a variety of recommendation tasks at Pinterest, a popular content discovery and curation appli-cation where users interact with pins, which are visual bookmarks to online content (e.g., recipes they want to cook, or clothes they want to purchase). Users organize these pins into boards, which con-tain collections of similar pins. Altogether, Pinterest is the world’s largest user-curated graph of images, with over 2 billion unique pins collected into over 1 billion boards.</li>
</ul>
<p>Through extensive offline metrics, controlled user studies, and A/B tests, we show that our approach achieves state-of-the-art performance compared to other scalable deep content-based rec-ommendation algorithms, in both an item-item recommendation task (i.e., related-pin recommendation), as well as a “homefeed” recommendation task. In offline ranking metrics we improve over the best performing baseline by more than 40%, in head-to-head human evaluations our recommendations are preferred about 60% of the time, and the A/B tests show 30% to 100% improvements in user engagement across various settings.</p>
<p>To our knowledge, this is the largest-ever application of deep graph embeddings and paves the way for new generation of rec-ommendation systems based on graph convolutional architectures.</p>
<ul>
<li>
<p>动态卷积：传统的GCN算法通过将特征矩阵乘以完整图拉普拉斯算子的幂来进行每个图形卷积。相比之下，我们的PinSage算法通过对节点周围的邻域进行采样并从该采样邻域动态构建计算图来执行高效的局部卷积。这些动态构建的计算图（图1）指定了如何在特定节点周围执行局部卷积，并减少了在训练期间对整个图进行操作的需要。</p>
</li>
<li>
<p>生产者 &#8211; 消费者小批量建设：我们开发了一个生产者 &#8211; 消费者体系结构，用于构建微型计算机，确保在模型培训期间最大限度地利用GPU。一个大内存，受CPU限制的生产者有效地采样节点网络邻域并获取必要的特征来定义局部卷积，而受GPU约束的TensorFlow模型使用这些预定义的计算图来有效地运行随机梯度体面。</p>
</li>
<li>
<p>高效的MapReduce推理：给定一个完全训练的GCN模型，我们设计了一个有效的MapReduce管道，可以分解训练的模型，为数十亿个节点生成嵌入，同时最大限度地减少重复计算。</p>
</li>
</ul>
<p>除了可扩展性的这些基本进步之外，我们还引入了新的培训技术和算法创新。这些创新提高了PinSage所学习的表示质量，在下游推荐系统任务中带来了显着的性能提升：</p>
<ul>
<li>通过随机游走构建卷积：使用完整的节点邻域来执行卷积（图1）将导致巨大的计算图，因此我们求助于采样。然而，随机抽样不是最理想的，我们开发了一种使用短随机游走来抽样计算图的新技术。另一个好处是每个节点现在都有一个重要性分数，我们在池化/聚合步骤中使用它。</li>
<li>•重要性池：图卷的核心组件是图中本地邻居的特征信息的聚合。我们引入了一种方法来基于随机游走相似性度量来衡量此聚合中节点特征的重要性，从而使离线评估指标的性能提高46％。</li>
<li>课程培训：我们设计了一个课程培训方案，在培训过程中，算法得到了越来越难的实例，从而使性能提高了12％。<br /> 我们在Pinterest上部署了PinSage用于各种推荐任务，Pinterest是一种流行的内容发现和策展应用，用户可以在其中与引脚交互，引脚是在线内容的可视书签（例如，他们想要烹饪的食谱，或者他们想要购买的衣服） ）。用户将这些引脚组织成板，其中包含类似引脚的集合。总而言之，Pinterest是世界上最大的用户策划图像图形，超过20亿个独特的引脚被收集到超过10亿个电路板中。</li>
</ul>
<p>通过广泛的离线指标，受控用户研究和A / B测试，我们表明，与项目项目推荐任务中的其他可扩展的基于深度内容的推荐算法相比，我们的方法实现了最先进的性能（即相关引脚推荐），以及“主页”推荐任务。在离线排名指标中，我们在最佳绩效基线上的表现提高了40％以上，在人机对话评估中，我们的建议在60％的时间内是首选，A / B测试显示在30％到100％的情况下，用户参与各种设置。</p>
<p>据我们所知，这是有史以来最大的深度图嵌入应用，并为基于图卷积结构的新一代推荐系统铺平了道路。</p>
<h3><a id="2	RELATED_WORK_81"></a>2 RELATED WORK</h3>
<p>Our work builds upon a number of recent advancements in deep learning methods for graph-structured data.</p>
<p>The notion of neural networks for graph data was first outlined in Gori et al. (2005) [15] and further elaborated on in Scarselli et al. (2009) [27]. However, these initial approaches to deep learning on graphs required running expensive neural “message-passing” algorithms to convergence and were prohibitively expensive on large graphs. Some limitations were addressed by Gated Graph Sequence Neural Networks [22]—which employs modern recurrent neural architectures—but the approach remains computationally expensive and has mainly been used on graphs with &lt;10, 000 nodes.</p>
<p>More recently, there has been a surge of methods that rely on the notion of “graph convolutions” or Graph Convolutional Net-works (GCNs). This approach originated with the work of Bruna et al. (2013), which developed a version of graph convolutions based on spectral graph thery [7]. Following on this work, a number of authors proposed improvements, extensions, and approximations of these spectral convolutions [6, 10, 11, 13, 18, 21, 24, 29, 31], lead-ing to new state-of-the-art results on benchmarks such as node classification, link prediction, as well as recommender system tasks (e.g., the MovieLens benchmark [24]). These approaches have con-sistently outperformed techniques based upon matrix factorization or random walks (e.g., node2vec [17] and DeepWalk [26]), and their success has led to a surge of interest in applying GCN-based methods to applications ranging from recommender systems [24] to drug design [20, 31]. Hamilton et al. (2017b) [19] and Bronstein et al. (2017) [6] provide comprehensive surveys of recent advancements.</p>
<p>However, despite the successes of GCN algorithms, no previous works have managed to apply them to production-scale data with billions of nodes and edges—a limitation that is primarily due to the fact that traditional GCN methods require operating on the entire graph Laplacian during training. Here we fill this gap and show that GCNs can be scaled to operate in a production-scale recommender system setting involving billions of nodes/items. Our work also demonstrates the substantial impact that GCNs have on recommendation performance in a real-world environment.</p>
<p>In terms of algorithm design, our work is most closely related to Hamilton et al. (2017a)’s GraphSAGE algorithm [18] and the closely related follow-up work of Chen et al. (2018) [8]. GraphSAGE is an inductive variant of GCNs that we modify to avoid operating on the entire graph Laplacian. We fundamentally improve upon GraphSAGE by removing the limitation that the whole graph be stored in GPU memory, using low-latency random walks to sample<br /> 我们的工作建立在图形结构数据深度学习方法的最新进展之上。</p>
<p>Gori等人首次概述了图数据的神经网络概念。 （2005）[15]并在Scarselli等人的文章中进一步阐述。 （2009）[27]。然而，这些在图上进行深度学习的初始方法需要运行昂贵的神经“消息传递”算法来收敛，并且在大图上非常昂贵。门控图形序列神经网络[22]解决了一些局限性 &#8211; 它采用了现代的递归神经架构 &#8211; 但该方法计算成本仍然很高，主要用于节点数&lt;10,000的图形。</p>
<p>最近，出现了大量依赖“图形卷积”或图形卷积网络（GCN）概念的方法。这种方法起源于Bruna等人的工作。 （2013），基于光谱图[7]开发了一个图形卷积的版本。继这项工作之后，许多作者提出了这些光谱卷积的改进，扩展和近似[6,10,11,13,18,21,24,29,31]，导致了新的状态。 -art结果基于节点分类，链接预测以及推荐系统任务（例如，MovieLens基准测试[24]）等基准测试。这些方法始终优于基于矩阵分解或随机游走的技术（例如，node2vec [17]和DeepWalk [26]），并且它们的成功引起了人们对将基于GCN的方法应用于推荐器等应用的兴趣。系统[24]到药物设计[20,31]。汉密尔顿等人。 （2017b）[19]和Bronstein等。 （2017）[6]提供了最近进展的综合调查。</p>
<p>然而，尽管GCN算法取得了成功，但之前的工作没有设法将它们应用于具有数十亿节点和边缘的生产规模数据 &#8211; 这主要是因为传统的GCN方法需要在整个图形拉普拉斯算子上运行。训练。在这里，我们填补了这一空白，并表明GCN可以扩展到在涉及数十亿节点/项目的生产规模推荐系统设置中运行。我们的工作还证明了GCN对现实环境中的推荐性能产生的重大影响。</p>
<p>在算法设计方面，我们的工作与Hamilton等人的关系最为密切。 （2017a）的GraphSAGE算法[18]和陈等人的密切相关的后续工作。 （2018）[8]。 GraphSAGE是GCN的归纳变体，我们修改它以避免在整个图拉普拉斯算子上运行。我们通过消除整个图存储在GPU内存中的限制，从而使用低延迟随机游走来获取样本，从根本上改进了GraphSAGE</p>
<p>graph neighborhoods in a producer-consumer architecture. We also introduce a number of new training techniques to improve performance and a MapReduce inference pipeline to scale up to graphs with billions of nodes.</p>
<p>Lastly, also note that graph embedding methods like node2vec</p>
<p>[17]and DeepWalk [26] cannot be applied here. First, these are unsupervised methods. Second, they cannot include node feature information. Third, they directly learn embeddings of nodes and thus the number of model parameters is linear with the size of the graph, which is prohibitive for our setting.<br /> 生产者 &#8211; 消费者体系结构中的图形邻域。 我们还介绍了许多用于提高性能的新培训技术和MapReduce推理管道，以扩展到具有数十亿节点的图形。</p>
<p>最后，还要注意图形嵌入方法，如node2vec</p>
<p>[17]和DeepWalk [26]不能在这里应用。 首先，这些是无监督的方法。 其次，它们不能包含节点特征信息。 第三，它们直接学习节点的嵌入，因此模型参数的数量与图形的大小成线性关系，这对我们的设置来说是禁止的。</p>
<h3><a id="3_METHOD_114"></a>3 METHOD</h3>
<p>In this section, we describe the technical details of the PinSage archi-tecture and training, as well as a MapReduce pipeline to efficiently generate embeddings using a trained PinSage model.</p>
<p>The key computational workhorse of our approach is the notion of localized graph convolutions.1 To generate the embedding for a node (i.e., an item), we apply multiple convolutional modules that aggregate feature information (e.g., visual, textual features) from the node’s local graph neighborhood (Figure 1). Each module learns how to aggregate information from a small graph neighbor-hood, and by stacking multiple such modules, our approach can gain information about the local network topology. Importantly, parameters of these localized convolutional modules are shared across all nodes, making the parameter complexity of our approach independent of the input graph size.</p>
<p>在本节中，我们将介绍PinSage架构和培训的技术细节，以及使用经过训练的PinSage模型高效生成嵌入的MapReduce管道。</p>
<p>我们方法的关键计算主力是局部图卷积的概念.1为了生成节点（即项目）的嵌入，我们应用多个卷积模块来聚合来自节点的特征信息（例如，视觉，文本特征）。 局部图邻域（图1）。 每个模块都学习如何聚合来自小图邻居的信息，并通过堆叠多个这样的模块，我们的方法可以获得有关本地网络拓扑的信息。 重要的是，这些局部卷积模块的参数在所有节点之间共享，使得我们方法的参数复杂度与输入图形大小无关。</p>
<h5><a id="31	Problem_Setup_124"></a>3.1 Problem Setup</h5>
<p>Pinterest is a content discovery application where users interact with pins, which are visual bookmarks to online content (e.g., recipes they want to cook, or clothes they want to purchase). Users organize these pins into boards, which contain collections of pins that the user deems to be thematically related. Altogether, the Pinterest graph contains 2 billion pins, 1 billion boards, and over 18 billion edges (i.e., memberships of pins to their corresponding boards).</p>
<p>Our task is to generate high-quality embeddings or representa-tions of pins that can be used for recommendation (e.g., via nearest-neighbor lookup for related pin recommendation, or for use in a downstream re-ranking system). In order to learn these embed-dings, we model the Pinterest environment as a bipartite graph consisting of nodes in two disjoint sets, I (containing pins) and C (containing boards). Note, however, that our approach is also naturally generalizable, with I being viewed as a set of items and C as a set of user-defined contexts or collections.</p>
<p>In addition to the graph structure, we also assume that the pins/items u ∈ I are associated with real-valued attributes, xu ∈ Rd . In general, these attributes may specify metadata or content information about an item, and in the case of Pinterest, we have that pins are associated with both rich text and image features. Our goal is to leverage both these input attributes as well as the structure of the bipartite graph to generate high-quality embed-dings. These embeddings are then used for recommender system candidate generation via nearest neighbor lookup (i.e., given a pin, find related pins) or as features in machine learning systems for ranking the candidates.<br /> Pinterest是一种内容发现应用程序，其中用户与引脚交互，引脚是在线内容的可视书签（例如，他们想要烹饪的食谱，或者他们想要购买的衣服）。用户将这些引脚组织成电路板，其中包含用户认为与主题相关的引脚集合。总而言之，Pinterest图包含20亿个引脚，10亿个电路板和超过180亿个边沿（即，引脚到其相应电路板的成员资格）。</p>
<p>我们的任务是生成可用于推荐的高质量嵌入或引脚表示（例如，通过最近邻居查找以获得相关引脚推荐，或用于下游重新排序系统）。为了学习这些嵌入，我们将Pinterest环境建模为由两个不相交集合中的节点组成的二分图，I（包含引脚）和C（包含板）。但请注意，我们的方法也可以自然地推广，我将其视为一组项目，将C视为一组用户定义的上下文或集合。</p>
<p>除了图形结构之外，我们还假设引脚/项u∈I与实值属性xu∈Rd相关联。通常，这些属性可以指定关于项目的元数据或内容信息，并且在Pinterest的情况下，我们将这些引脚与富文本和图像特征相关联。我们的目标是利用这些输入属性以及二分图的结构来生成高质量的嵌入式数据。然后，这些嵌入用于通过最近邻居查找生成推荐系统候选（即，给定引脚，找到相关引脚）或用作机器学习系统中用于对候选进行排序的特征。</p>
<p>For notational convenience and generality, when we describe the PinSage algorithm, we simply refer to the node set of the full graph with V = I ∪ C and do not explicitly distinguish between pin and board nodes (unless strictly necessary), using the more general term “node” whenever possible.<br /> 为了符号方便性和通用性，当我们描述PinSage算法时，我们简单地引用完整图的节点集，其中V =I∪C并且没有明确区分引脚和板节点（除非严格必要），使用更一般 尽可能使用术语“节点”。</p>
<h5><a id="32	Model_Architecture_139"></a>3.2 Model Architecture</h5>
<p>We use localized convolutional modules to generate embeddings for nodes. We start with input node features and then learn neural networks that transform and aggregate features over the graph to compute the node embeddings (Figure 1).</p>
<p><strong>Forward propagation algorithm.</strong> We consider the task of gener-ating an embedding, zu for a node u, which depends on the node’s input features and the graph structure around this node.<br /> 我们使用局部卷积模块为节点生成嵌入。 我们从输入节点特征开始，然后学习神经网络，在图形上转换和聚合特征以计算节点嵌入（图1）。</p>
<p><strong>前向传播算法</strong> 我们考虑为节点u生成嵌入，zu的任务，这取决于节点的输入特征和该节点周围的图形结构。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707091950238.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>The core of our PinSage algorithm is a localized convolution operation, where we learn how to aggregate information from u’s neighborhood (Figure 1). This procedure is detailed in Algorithm 1 convolve. The basic idea is that we transform the representations zv , ∀v ∈ N(u) of u’s neighbors through a dense neural network and then apply a aggregator/pooling fuction (e.g., a element-wise mean or weighted sum, denoted as γ ) on the resulting set of vectors (Line 1). This aggregation step provides a vector representation, nu , of u’s local neighborhood, N(u). We then concatenate the ag-gregated neighborhood vector nu with u’s current representation hu and transform the concatenated vector through another dense neural network layer (Line 2). Empirically we observe significant performance gains when using concatenation operation instead of the average operation as in [21]. Additionally, the normalization in Line 3 makes training more stable, and it is more efficient to perform approximate nearest neighbor search for normalized embeddings (Section 3.5). The output of the algorithm is a representation of u that incorporates both information about itself and its local graph neighborhood.</p>
<p>Importance-based neighborhoods. An important innovation in our approach is how we define node neighborhoods N(u), i.e., how we select the set of neighbors to convolve over in Algorithm 1. Whereas previous GCN approaches simply examine k-hop graph neighborhoods, in PinSage we define importance-based neighbor-hoods, where the neighborhood of a node u is defined as the T nodes that exert the most influence on node u. Concretely, we simulate random walks starting from node u and compute the L1-normalized</p>
<p>visit count of nodes visited by the random walk [14].2 The neigh-borhood of u is then defined as the top T nodes with the highest normalized visit counts with respect to node u.</p>
<p>The advantages of this importance-based neighborhood defi-nition are two-fold. First, selecting a fixed number of nodes to aggregate from allows us to control the memory footprint of the algorithm during training [18]. Second, it allows Algorithm 1 to take into account the importance of neighbors when aggregating the vector representations of neighbors. In particular, we imple-ment γ in Algorithm 1 as a weighted-mean, with weights defined according to the L1 normalized visit counts. We refer to this new approach as importance pooling.</p>
<p>Stacking convolutions. Each time we apply the convolve opera-tion (Algorithm 1) we get a new representation for a node, and we can stack multiple such convolutions on top of each other in order to gain more information about the local graph structure around node u. In particular, we use multiple layers of convolutions, where the inputs to the convolutions at layer k depend on the representa-tions output from layer k − 1 (Figure 1) and where the initial (i.e., “layer 0”) representations are equal to the input node features. Note that the model parameters in Algorithm 1 (Q, q, W, and w) are shared across the nodes but differ between layers.<br /> 我们的PinSage算法的核心是局部卷积运算，我们学习如何从u的邻域聚合信息（图1）。此过程在算法1卷入中详细说明。基本思想是我们通过密集神经网络转换u邻居的表示zv，∀v∈N（u）然后应用聚合器/池化函数（例如，元素方式或加权和，表示为γ）在得到的矢量集上（第1行）。该聚合步骤提供了u的局部邻域N（u）的向量表示nu。然后，我们将ag-gregated邻域向量nu与u的当前表示hu连接，并将连接的向量转换为另一个密集的神经网络层（第2行）。根据经验，我们在使用串联操作而不是[21]中的平均操作时观察到显着的性能提升。此外，第3行中的归一化使训练更加稳定，并且对归一化嵌入执行近似最近邻搜索更有效（第3.5节）。算法的输出是u的表示，其包含关于其自身及其局部图邻域的信息。</p>
<p>基于重要性的社区。我们方法的一个重要创新是我们如何定义节点邻域N（u），即我们如何选择在算法1中进行卷积的邻居集合。而以前的GCN方法只是检查k-hop图形邻域，在PinSage中我们定义重要性基于邻居的邻居，其中节点u的邻域被定义为对节点u施加最大影响的T节点。具体地说，我们模拟从节点u开始的随机游走并计算L1标准化</p>
<p>访问随机游走所访问的节点数[14] .2然后将u的邻域定义为相对于节点u具有最高归一化访问计数的前T个节点。</p>
<p>这种基于重要性的邻域定义的优势是双重的。首先，选择要聚合的固定数量的节点允许我们在训练期间控制算法的内存占用[18]。其次，它允许算法1在聚合邻居的向量表示时考虑邻居的重要性。特别地，我们在算法1中实现γ作为加权平均值，其中权重根据L1归一化访问计数来定义。我们将这种新方法称为重要性池。</p>
<p>堆叠卷积。每次我们应用卷积运算（算法1）时，我们得到一个节点的新表示，并且我们可以将多个这样的卷积堆叠在彼此之上，以便获得关于节点u周围的局部图结构的更多信息。特别地，我们使用多层卷积，其中层k处的卷积的输入取决于层k-1（图1）的表示和初始（即“层0”）表示相等的表示。到输入节点功能。请注意，算法1（Q，q，W和w）中的模型参数在节点之间共享，但层之间不同。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707092102872.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070709213627.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>We first describe our margin-based loss function in detail. Follow-ing this, we give an overview of several techniques we developed that lead to the computation efficiency and fast convergence rate of PinSage, allowing us to train on billion node graphs and billions training examples. And finally, we describe our curriculum-training scheme, which improves the overall quality of the recommendations.</p>
<p>我们首先详细描述基于保证金的损失函数。 接下来，我们概述了我们开发的几种技术，这些技术可以提高PinSage的计算效率和快速收敛速度，使我们能够训练数十亿个节点图和数十亿个训练样例。 最后，我们描述了我们的课程培训计划，该计划提高了建议的整体质量。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070709224222.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Loss function. In order to train the parameters of the model, we use a max-margin-based loss function. The basic idea is that we want to maximize the inner product of positive examples, i.e., the embedding of the query item and the corresponding related item. At the same time we want to ensure that the inner product of negative examples—i.e., the inner product between the embedding of the query item and an unrelated item—is smaller than that of the positive sample by some pre-defined margin. The loss function for a<br /> 损失功能。 为了训练模型的参数，我们使用基于最大边际的损失函数。 基本思想是我们希望最大化正例的内积，即嵌入查询项和相应的相关项。 同时，我们希望确保否定示例的内积 &#8211; 即，查询项的嵌入与不相关项之间的内积 &#8211; 比正样本的内积小一些预定义的余量。 a的损失函数<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070709234097.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Multi-GPU training with large minibatches. To make full use of multiple GPUs on a single machine for training, we run the for-ward and backward propagation in a multi-tower fashion. With multiple GPUs, we first divide each minibatch (Figure 1 bottom) into equal-sized portions. Each GPU takes one portion of the minibatch and performs the computations using the same set of parameters. Af-ter backward propagation, the gradients for each parameter across all GPUs are aggregated together, and a single step of synchronous SGD is performed. Due to the need to train on extremely large number of examples (on the scale of billions), we run our system with large batch sizes, ranging from 512 to 4096.</p>
<p>We use techniques similar to those proposed by Goyal et al. [16] to ensure fast convergence and maintain training and generalization accuracy when dealing with large batch sizes. We use a gradual warmup procedure that increases learning rate from small to a peak value in the first epoch according to the linear scaling rule. Afterwards the learning rate is decreased exponentially.</p>
<p>Producer-consumer minibatch construction. During training, the adjacency list and the feature matrix for billions of nodes are placed in CPU memory due to their large size. However, during the convolve step of PinSage, each GPU process needs access to the neighborhood and feature information of nodes in the neighbor-hood. Accessing the data in CPU memory from GPU is not efficient. To solve this problem, we use a re-indexing technique to create a sub-graph G′ = (V ′, E′) containing nodes and their neighborhood, which will be involved in the computation of the current minibatch. A small feature matrix containing only node features relevant to computation of the current minibatch is also extracted such that the order is consistent with the index of nodes in G′. The adjacency list of G′ and the small feature matrix are fed into GPUs at the start of each minibatch iteration, so that no communication between the GPU and CPU is needed during the convolve step, greatly improving GPU utilization.</p>
<p>The training procedure has alternating usage of CPUs and GPUs. The model computations are in GPUs, whereas extracting features, re-indexing, and negative sampling are computed on CPUs. In ad-dition to parallelizing GPU computation with multi-tower training, and CPU computation using OpenMP [25], we design a producer-consumer pattern to run GPU computation at the current iteration and CPU computation at the next iteration in parallel. This further reduces the training time by almost a half.</p>
<p>Sampling negative items. Negative sampling is used in our loss function (Equation 1) as an approximation of the normalization factor of edge likelihood [23]. To improve efficiency when training with large batch sizes, we sample a set of 500 negative items to be shared by all training examples in each minibatch. This drastically saves the number of embeddings that need to be computed during each training step, compared to running negative sampling for each node independently. Empirically, we do not observe a difference between the performance of the two sampling schemes.</p>
<p>In the simplest case, we could just uniformly sample negative examples from the entire set of items. However, ensuring that the inner product of the positive example (pair of items (q, i )) is larger than that of the q and each of the 500 negative items is too “easy” and does not provide fine enough “resolution” for the system to learn. In particular, our recommendation algorithm should be capable of finding 1,000 most relevant items to q among the catalog of over 2 billion items. In other words, our model should be able to distinguish/identify 1 item out of 2 million items. But with 500 random negative items, the model’s resolution is only 1 out of</p>
<p>500.Thus, if we sample 500 random negative items out of 2 billion items, the chance of any of these items being even slightly related to the query item is small. Therefore, with large probability the learning will not make good parameter updates and will not be able to differentiate slightly related items from the very related ones.</p>
<p>To solve the above problem, for each positive training example (i.e., item pair (q, i)), we add “hard” negative examples, i.e., items that are somewhat related to the query item q, but not as related as the positive item i. We call these “hard negative items”. They are generated by ranking items in a graph according to their Per-sonalized PageRank scores with respect to query item q [14]. Items ranked at 2000-5000 are randomly sampled as hard negative items. As illustrated in Figure 2, the hard negative examples are more similar to the query than random negative examples, and are thus challenging for the model to rank, forcing the model to learn to distinguish items at a finer granularity.<br /> 使用大型微型计算机进行多GPU培训。为了在一台机器上充分利用多个GPU进行培训，我们以多塔式方式进行前向和后向传播。对于多个GPU，我们首先将每个小批量（图1底部）划分为相等大小的部分。每个GPU获取一部分小批量并使用相同的参数集执行计算。在后向传播之后，所有GPU上的每个参数的梯度被聚合在一起，并且执行同步SGD的单个步骤。由于需要训练大量的例子（数十亿的规模），我们运行我们的系统，批量大，从512到4096。</p>
<p>我们使用类似于Goyal等人提出的技术。 [16]确保在处理大批量时快速收敛并保持培训和泛化准确性。我们使用渐进的预热程序，根据线性缩放规则，在第一个时期内将学习率从小值提高到峰值。之后学习率呈指数下降。</p>
<p>生产者 &#8211; 消费者小批量建设。在训练期间，由于数据量大，数十亿个节点的邻接列表和特征矩阵被放置在CPU内存中。但是，在PinSage的卷积步骤中，每个GPU进程都需要访问邻居和邻居中节点的特征信息。从GPU访问CPU内存中的数据效率不高。为了解决这个问题，我们使用重新索引技术来创建包含节点及其邻域的子图G’=（V’，E’），其将参与当前小批量的计算。还提取仅包含与当前小批量的计算相关的节点特征的小特征矩阵，使得该顺序与G’中的节点索引一致。 G’的邻接列表和小特征矩阵在每个小批量迭代开始时被馈送到GPU中，因此在卷积步骤期间不需要GPU和CPU之间的通信，从而大大提高了GPU利用率。</p>
<p>训练过程交替使用CPU和GPU。模型计算在GPU中，而在CPU上计算提取特征，重新索引和负抽样。除了使用多塔培训并行GPU计算和使用OpenMP进行CPU计算[25]之外，我们还设计了一个生产者 &#8211; 消费者模式，以便在当前迭代中运行GPU计算，并在下一次迭代中并行运行CPU计算。这进一步将训练时间减少了近一半。</p>
<p>采样负面项目。在我们的损失函数（等式1）中使用负采样作为边缘似然归一化因子的近似值[23]。为了提高大批量培训的效率，我们对每个小批量的所有培训示例共享500个负面项目。与每个节点独立运行负采样相比，这大大节省了每个训练步骤中需要计算的嵌入数量。根据经验，我们没有观察到两种抽样方案的性能差异。</p>
<p>在最简单的情况下，我们可以从整个项目集合中统一采样负面示例。但是，确保正例（项目对（q，i））的内积大于q的内积，并且500个负项中的每一个都太“容易”并且不能提供足够精细的“分辨率”。系统要学习。特别是，我们的推荐算法应该能够在超过20亿个项目的目录中找到1000个最相关的项目。换句话说，我们的模型应该能够区分/识别200万个项目中的1个项目。但是有500个随机负面项目，模型的分辨率只有1个</p>
<p>500.因此，如果我们从20亿个项目中抽取500个随机负面项目，那么这些项目中任何一个与查询项目稍微相关的可能性就很小。因此，很有可能学习不会进行良好的参数更新，并且无法区分略微相关的项目与非常相关的项目。</p>
<p>为了解决上述问题，对于每个积极的训练样例（即项目对（q，i）），我们添加“硬”否定示例，即与查询项目q有些相关的项目，但不是与积极的项目我我们将这些称为“硬性负面物品”。它们是通过根据查询项q [14]的Per-sonalized PageRank分数对图表中的项目进行排名而生成的。排名为2000-5000的项目被随机抽样为硬阴性项目。如图2所示，硬否定示例与查询比随机否定示例更相似，因此难以对模型进行排名，迫使模型学习以更精细的粒度区分项目。</p>
<p>Using hard negative items throughout the training procedure doubles the number of epochs needed for the training to con-verge. To help with convergence, we develop a curriculum training scheme [4]. In the first epoch of training, no hard negative items are used, so that the algorithm quickly finds an area in the parameter space where the loss is relatively small. We then add hard negative items in subsequent epochs, focusing the model to learn how to distinguish highly related pins from only slightly related ones. At epoch n of the training, we add n − 1 hard negative items to the set of negative items for each item.<br /> 在整个训练过程中使用硬阴性项目会使训练所需的时期数量增加一倍。 为了帮助融合，我们制定了课程培训计划[4]。 在第一个训练时期，没有使用硬负项，因此算法快速找到参数空间中损失相对较小的区域。 然后，我们在后续时期添加硬阴性项，重点关注模型，以了解如何区分高度相关的引脚和仅略微相关的引脚。 在训练的时期，我们将n &#8211; 1个硬阴性项添加到每个项目的负项目集中。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707092622891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> Figure 2: Random negative examples and hard negative ex-amples. Notice that the hard negative example is signifi-cantly more similar to the query, than the random negative example, though not as similar as the positive example.<br /> 图2：随机负面例子和硬性负面例子。 请注意，与负面示例相比，硬负面示例显着更类似于查询，尽管与正面示例不太相似。</p>
<h5><a id="34	Node_Embeddings_via_MapReduce_223"></a>3.4 Node Embeddings via MapReduce</h5>
<p>After the model is trained, it is still challenging to directly apply the trained model to generate embeddings for all items, including those that were not seen during training. Naively computing embeddings for nodes using Algorithm 2 leads to repeated computations caused by the overlap between K-hop neighborhoods of nodes. As illus-trated in Figure 1, many nodes are repeatedly computed at multiple layers when generating the embeddings for different target nodes. To ensure efficient inference, we develop a MapReduce approach that runs model inference without repeated computations.</p>
<p>We observe that inference of node embeddings very nicely lends itself to MapReduce computational model. Figure 3 details the data flow on the bipartite pin-to-board Pinterest graph, where we assume the input (i.e., “layer-0”) nodes are pins/items (and the layer-1 nodes are boards/contexts). The MapReduce pipeline has two key parts:</p>
<p>(1)One MapReduce job is used to project all pins to a low-dimensional latent space, where the aggregation operation will be performed (Algorithm 1, Line 1).</p>
<p>(2)Another MapReduce job is then used to join the resulting pin representations with the ids of the boards they occur in, and the board embedding is computed by pooling the features of its (sampled) neighbors.</p>
<p>Note that our approach avoids redundant computations and that the latent vector for each node is computed only once. After the em-beddings of the boards are obtained, we use two more MapReduce jobs to compute the second-layer embeddings of pins, in a similar fashion as above, and this process can be iterated as necessary (up to K convolutional layers).3</p>
<p>在训练模型之后，直接应用训练模型来生成所有项目的嵌入仍然具有挑战性，包括在训练期间未看到的项目。使用算法2对节点进行初始计算嵌入导致由节点的K跳邻域之间的重叠引起的重复计算。如图1所示，当为不同的目标节点生成嵌入时，在多个层重复计算许多节点。为了确保有效的推理，我们开发了一种MapReduce方法，该方法运行模型推理而无需重复计算。</p>
<p>我们观察到节点嵌入的推断非常好地适用于MapReduce计算模型。图3详述了二分针对板Pinterest图的数据流，其中我们假设输入（即“第0层”）节点是引脚/项（并且第1层节点是板/上下文）。 MapReduce管道有两个关键部分：</p>
<p>（1）一个MapReduce作业用于将所有引脚投影到低维潜在空间，其中将执行聚合操作（算法1，第1行）。</p>
<p>（2）然后使用另一个MapReduce作业将得到的引脚表示与它们出现的板的ID相连，并且通过汇集其（采样的）邻居的特征来计算板嵌入。</p>
<p>请注意，我们的方法避免了冗余计算，并且每个节点的潜在向量仅计算一次。在获得电路板的em-bedding之后，我们使用另外两个MapReduce作业以与上面类似的方式计算引脚的第二层嵌入，并且可以根据需要迭代该过程（直到K个卷积层）。</p>
<h6><a id="35	Efficient_nearestneighbor_lookups_249"></a>3.5 Efficient nearest-neighbor lookups</h6>
<p>The embeddings generated by PinSage can be used for a wide range of downstream recommendation tasks, and in many settings we can directly use these embeddings to make recommendations by performing nearest-neighbor lookups in the learned embedding space. That is, given a query item q, the we can recommend items whose embeddings are the K-nearest neighbors of the query item’s embedding. Approximate KNN can be obtained efficiently via lo-cality sensitive hashing [2]. After the hash function is computed, retrieval of items can be implemented with a two-level retrieval pro-cess based on the Weak AND operator [5]. Given that the PinSage model is trained offline and all node embeddings are computed via MapReduce and saved in a database, the efficient nearest-neighbor lookup operation enables the system to serve recommendations in an online fashion,<br /> PinSage生成的嵌入可用于各种下游推荐任务，在许多设置中，我们可以通过在学习的嵌入空间中执行最近邻查找来直接使用这些嵌入来提出建议。 也就是说，给定查询项q，我们可以推荐其嵌入是查询项嵌入的K最近邻居的项。 通过物理敏感散列可以有效地获得近似KNN [2]。 在计算散列函数之后，可以使用基于Weak AND运算符的两级检索过程来实现项目的检索[5]。 鉴于PinSage模型是离线训练的，并且所有节点嵌入都是通过MapReduce计算并保存在数据库中，有效的最近邻查找操作使系统能够以在线方式提供建议，</p>
<h3><a id="4_EXPERIMENTS_253"></a>4 EXPERIMENTS</h3>
<p>To demonstrate the efficiency of PinSage and the quality of the embeddings it generates, we conduct a comprehensive suite of experiments on the entire Pinterest object graph, including offline experiments, production A/B tests as well as user studies.<br /> 为了证明PinSage的效率及其产生的嵌入质量，我们对整个Pinterest对象图进行了一整套实验，包括离线实验，生产A / B测试以及用户研究。</p>
<h6><a id="41	Experimental_Setup_258"></a>4.1 Experimental Setup</h6>
<p>We evaluate the embeddings generated by PinSage in two tasks: recommending related pins and recommending pins in a user’s home/news feed. To recommend related pins, we select the K near-est neighbors to the query pin in the embedding space. We evaluate performance on this related-pin recommendation task using both offline ranking measures as well as a controlled user study. For the homefeed recommendation task, we select the pins that are closest in the embedding space to one of the most recently pinned items by the user. We evaluate performance of a fully-deployed production system on this task using A/B tests to measure the overall impact on user engagement.</p>
<p>Training details and data preparation. We define the set, L, of positive training examples (Equation (1)) using historical user engagement data. In particular, we use historical user engagement data to identify pairs of pins (q, i), where a user interacted with pin</p>
<p>iimmediately after she interacted with pin q. We use all other pins as negative items (and sample them as described in Section 3.3). Overall, we use 1.2 billion pairs of positive training examples (in addition to 500 negative examples per batch and 6 hard negative examples per pin). Thus in total we use 7.5 billion training examples.<br /> Since PinSage can efficiently generate embeddings for unseen data, we only train on a subset of the Pinterest graph and then generate embeddings for the entire graph using the MapReduce pipeline described in Section 3.4. In particular, for training we use a randomly sampled subgraph of the entire graph, containing 20% of all boards (and all the pins touched by those boards) and 70% of the labeled examples. During hyperparameter tuning, a remaining 10% of the labeled examples are used. And, when testing, we run inference on the entire graph to compute embeddings for all 2 billion pins, and the remaining 20% of the labeled examples are used to test the recommendation performance of our PinSage in the offline evaluations. Note that training on a subset of the full graph drastically decreased training time, with a negligible impact on final performance. In total, the full datasets for training and evaluation are approximately 18TB in size with the full output embeddings being 4TB.<br /> 我们评估PinSage在两个任务中生成的嵌入：推荐相关引脚并在用户的主页/新闻源中推荐引脚。为了推荐相关引脚，我们选择嵌入空间中查询引脚的K近邻。我们使用离线排名度量和受控用户研究来评估此相关引脚推荐任务的性能。对于主页提交推荐任务，我们选择嵌入空间中最接近用户最近固定项目之一的引脚。我们使用A / B测试来评估完全部署的生产系统在此任务上的性能，以衡量对用户参与的总体影响。</p>
<p>培训细节和数据准备。我们使用历史用户参与数据定义积极训练样例（等式（1））的集合L.特别地，我们使用历史用户参与数据来识别用户与引脚交互的引脚对（q，i）</p>
<p>她与pin q交互后立刻。我们使用所有其他引脚作为负项（并按照第3.3节中的描述对它们进行采样）。总的来说，我们使用了12亿对正面训练示例（除了每批500个负面示例和每个针脚6个硬负面示例）。因此，我们总共使用了75亿个培训示例。<br /> 由于PinSage可以有效地为看不见的数据生成嵌入，因此我们只训练Pinterest图的子集，然后使用3.4节中描述的MapReduce管道为整个图生成嵌入。特别是，对于训练，我们使用整个图形的随机采样子图，包含20％的所有板（以及这些板触及的所有引脚）和70％的标记示例。在超参数调整期间，使用剩余的10％的标记示例。并且，在测试时，我们对整个图形进行推断，以计算所有20亿个引脚的嵌入，其余20％的标记示例用于测试我们的PinSage在离线评估中的推荐性能。请注意，对完整图表子集的培训大大减少了培训时间，对最终性能的影响可以忽略不计。总的来说，用于训练和评估的完整数据集大小约为18TB，完整输出嵌入为4TB。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707092854376.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>Features used for learning. Each pin at Pinterest is associated with an image and a set of textual annotations (title, description). To generate feature representation xq for each pin q, we concatenate visual embeddings (4,096 dimensions), textual annotation embed-dings (256 dimensions), and the log degree of the node/pin in the graph. The visual embeddings are the 6-th fully connected layer of a classification network using the VGG-16 architecture [28]. Tex-tual annotation embeddings are trained using a Word2Vec-based model [23], where the context of an annotation consists of other annotations that are associated with each pin.</p>
<p>Baselines for comparison. We evaluate the performance of Pin-Sage against the following state-of-the-art content-based, graph-based and deep learning baselines that generate embeddings of pins:</p>
<p>(1)Visual embeddings (Visual): Uses nearest neighbors of deep visual embeddings for recommendations. The visual features are described above.</p>
<p>(2)Annotation embeddings (Annotation): Recommends based on nearest neighbors in terms of annotation embeddings. The annotation embeddings are described above.<br /> (3)Combined embeddings (Combined): Recommends based on concatenating visual and annotation embeddings, and using a 2-layer multi-layer perceptron to compute embeddings that capture both visual and annotation features.</p>
<p>(4)Graph-based method (Pixie): This random-walk-based method [14] uses biased random walks to generate ranking scores by simulating random walks starting at query pin q. Items with top K scores are retrieved as recommendations. While this approach does not generate pin embeddings, it is currently the state-of-the-art at Pinterest for certain recommendation tasks [14] and thus an informative baseline.</p>
<p>The visual and annotation embeddings are state-of-the-art deep learning content-based systems currently deployed at Pinterest to generate representations of pins. Note that we do not compare against other deep learning baselines from the literature simply due to the scale of our problem. We also do not consider non-deep learning approaches for generating item/content embeddings, since other works have already proven state-of-the-art performance of deep learning approaches for generating such embeddings [9, 12, 24].</p>
<p>用于学习的功能。 Pinterest中的每个引脚都与一个图像和一组文本注释（标题，描述）相关联。为了生成每个引脚q的特征表示xq，我们连接了视觉嵌入（4,096维），文本注释嵌入（256维）以及图中节点/引脚的对数度。视觉嵌入是使用VGG-16架构的分类网络的第6个完全连接层[28]。使用基于Word2Vec的模型[23]训练文本注释嵌入，其中注释的上下文包含与每个引脚相关联的其他注释。</p>
<p>比较基线。我们针对以下基于内容的，基于图形的深度学习基线评估了Pin-Sage的性能，这些基线生成了引脚的嵌入：</p>
<p>（1）视觉嵌入（Visual）：使用深度视觉嵌入的最近邻居作为推荐。视觉特征如上所述。</p>
<p>（2）注释嵌入（注释）：根据注释嵌入推荐基于最近邻居。注释嵌入如上所述。<br /> （3）组合嵌入（组合）：推荐基于连接视觉和注释嵌入，并使用2层多层感知器计算捕获视觉和注释特征的嵌入。</p>
<p>（4）基于图的方法（Pixie）：这种基于随机游走的方法[14]通过模拟从查询引脚q开始的随机游走来使用偏向随机游走来生成排名分数。具有最高K分数的项目被检索作为推荐。虽然这种方法不会产生引脚嵌入，但它目前是Pinterest中针对某些推荐任务[14]的最新技术，因此也是一个信息基线。</p>
<p>视觉和注释嵌入是目前在Pinterest部署的最先进的基于深度学习内容的系统，用于生成引脚的表示。请注意，由于问题的严重性，我们不会与文献中的其他深度学习基线进行比较。我们也不考虑用于生成项目/内容嵌入的非深度学习方法，因为其他工作已经证明了用于生成这种嵌入的深度学习方法的最先进性能[9,12,24]。</p>
<p>We also conduct ablation studies and consider several variants of PinSage when evaluating performance:</p>
<ul>
<li>
<p>max-pooling uses the element-wise max as a symmetric aggre-gation function (i.e., γ = max) without hard negative samples;</p>
</li>
<li>
<p>mean-pooling uses the element-wise mean as a symmetric aggregation function (i.e., γ = mean);</p>
</li>
<li>
<p>mean-pooling-xent is the same as mean-pooling but uses the cross-entropy loss introduced in [18].</p>
</li>
<li>
<p>mean-pooling-hard is the same as mean-pooling, except that it incorporates hard negative samples as detailed in Section 3.3.</p>
</li>
<li>
<p>PinSage uses all optimizations presented in this paper, includ-ing the use of importance pooling in the convolution step.</p>
</li>
</ul>
<p>The max-pooling and cross-entropy settings are extensions of the best-performing GCN model from Hamilton et al. [18]—other vari-ants (e.g., based on Kipf et al. [21]) performed significantly worse in development tests and are omitted for brevity.4 For all the above variants, we used K = 2, hidden dimension size m = 2048, and set the embedding dimension d to be 1024.</p>
<p>Computation resources. Training of PinSage is implemented in TensorFlow [1] and run on a single machine with 32 cores and 16 Tesla K80 GPUs. To ensure fast fetching of item’s visual and annotation features, we store them in main memory, together with the graph, using Linux HugePages to increase the size of virtual memory pages from 4KB to 2MB. The total amount of memory used in training is 500GB. Our MapReduce inference pipeline is run on a Hadoop2 cluster with 378 d2.8xlarge Amazon AWS nodes.</p>
<p>我们还进行消融研究，并在评估性能时考虑PinSage的几种变体：</p>
<ul>
<li>
<p>max-pooling使用逐元素max作为对称聚合函数（即γ= max）而没有硬阴性样本;</p>
</li>
<li>
<p>均值池使用元素均值作为对称聚合函数（即，γ=均值）;</p>
</li>
<li>
<p>mean-pooling-xent与均值池相同，但使用[18]中引入的交叉熵损失。</p>
</li>
<li>
<p>mean-pooling-hard与mean-pooling相同，除了它包含第3.3节中详述的硬阴性样本。</p>
</li>
<li>
<p>PinSage使用本文中介绍的所有优化，包括在卷积步骤中使用重要性池。</p>
</li>
</ul>
<p>最大池和交叉熵设置是Hamilton等人的最佳性能GCN模型的扩展。 [18]其他变量（例如，基于Kipf等人[21]）在开发测试中表现更差，为简洁省略.4对于所有上述变体，我们使用K = 2，隐藏尺寸大小为m = 2048，并将嵌入维度d设置为1024。</p>
<p>计算资源。 PinSage的培训在TensorFlow [1]中实施，并在具有32个核心和16个Tesla K80 GPU的单台机器上运行。为了确保快速获取项目的视觉和注释功能，我们将它们与图形一起存储在主存储器中，使用Linux HugePages将虚拟内存页面的大小从4KB增加到2MB。培训中使用的内存总量为500GB。我们的MapReduce推理管道在具有378个d2.8xlarge Amazon AWS节点的Hadoop2集群上运行。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707092944123.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>表1：PinSage和基于内容的深度学习基线的命中率和MRR。 总体而言，PinSage在最佳基线上的命中率提高了150％，MRR提高了60％</p>
<h4><a id="42	Offline_Evaluation_333"></a>4.2 Offline Evaluation</h4>
<p>To evaluate performance on the related pin recommendation task, we define the notion of hit-rate. For each positive pair of pins (q, i) in the test set, we use q as a query pin and then compute its top<br /> Knearest neighbors NNq from a sample of 5 million test pins. We then define the hit-rate as the fraction of queries q where i was ranked among the top K of the test sample (i.e., where i ∈ NNq ). This metric directly measures the probability that recommendations made by the algorithm contain the items related to the query pin q. In our experiments K is set to be 500.</p>
<p>We also evaluate the methods using Mean Reciprocal Rank (MRR), which takes into account of the rank of the item j among recommended items for query item q:<br /> 为了评估相关引脚推荐任务的性能，我们定义了命中率的概念。 对于测试集中的每个正对引脚（q，i），我们使用q作为查询引脚，然后计算其顶部<br /> 来自500万个测试引脚样本的最近邻NNq。 然后，我们将命中率定义为查询q的分数，其中i在测试样本的前K中排名（即，其中i∈NNq）。 该度量直接测量算法所做推荐包含与查询引脚q相关的项目的概率。 在我们的实验中，K设定为500。</p>
<p>我们还使用均值倒数等级（MRR）来评估方法，其考虑了查询项目q的推荐项目中的项目j的等级：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093036909.png" alt="在这里插入图片描述"><br /> Due to the large pool of candidates (more than 2 billion), we use a scaled version of the MRR in Equation (2), where Ri, q is the rank of item i among recommended items for query q, and n is the total number of labeled item pairs. The scaling factor 100 ensures that, for example, the difference between rank at 1, 000 and rank at 2, 000 is still noticeable, instead of being very close to 0.</p>
<p>Table 1 compares the performance of the various approaches using the hit rate as well as the MRR.5 PinSage with our new importance-pooling aggregation and hard negative examples achieves the best performance at 67% hit-rate and 0.59 MRR, outperforming the top baseline by 40% absolute (150% relative) in terms of the hit rate and also 22% absolute (60% relative) in terms of MRR. We also observe that combining visual and textual information works much better than using either one alone (60% improvement of the combined approach over visual/annotation only).</p>
<p>Embedding similarity distribution. Another indication of the effectiveness of the learned embeddings is that the distances be-tween random pairs of item embeddings are widely distributed. If all items are at about the same distance (i.e., the distances are tightly clustered) then the embedding space does not have enough “resolu-tion” to distinguish between items of different relevance. Figure 4 plots the distribution of cosine similarities between pairs of items using annotation, visual, and PinSage embeddings. This distribution of cosine similarity between random pairs of items demonstrates the effectiveness of PinSage, which has the most spread out distribu-tion. In particular, the kurtosis of the cosine similarities of PinSage embeddings is 0.43, compared to 2.49 for annotation embeddings and 1.20 for visual embeddings.<br /> Another important advantage of having such a wide-spread in the embeddings is that it reduces the collision probability of the subsequent LSH algorithm, thus increasing the efficiency of serving the nearest neighbor pins during recommendation.<br /> 由于候选人数量大（超过20亿），我们在等式（2）中使用MRR的缩放版本，其中Ri，q是查询q的推荐项目中项目i的等级，n是总数标记的项目对的数量。缩放因子100确保例如，等级为1,000和等级为2,000的差异仍然是显着的，而不是非常接近于0。</p>
<p>表1比较了使用命中率的各种方法的性能以及MRR.5 PinSage与我们新的重要性汇总聚合和硬阴性示例在67％命中率和0.59 MRR下达到最佳性能，优于最高基线就命中率而言绝对值为40％（相对于150％），就MRR而言绝对值为22％（相对于60％）。我们还观察到，将视觉和文本信息结合起来比单独使用任何一个都要好得多（组合方法仅比视觉/注释提高60％）。</p>
<p>嵌入相似度分布。学习嵌入的有效性的另一个指示是随机对项目嵌入之间的距离被广泛分布。如果所有项目都处于大约相同的距离（即，距离紧密聚集），则嵌入空间没有足够的“分辨率”来区分不同相关的项目。图4绘制了使用注释，视觉和PinSage嵌入的项目对之间余弦相似度的分布。随机对项之间的余弦相似性的这种分布证明了PinSage的有效性，其具有最大的分布。特别是，PinSage嵌入的余弦相似度的峰度为0.43，而注释嵌入为2.49，视觉嵌入为1.20。<br /> 在嵌入中具有如此广泛的扩展的另一个重要优点是它降低了后续LSH算法的冲突概率，从而提高了在推荐期间服务最近邻居引脚的效率。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093124819.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 图4：视觉嵌入，注释嵌入和Pin-Sage嵌入的成对余弦相似度的概率密度。</p>
<h4><a id="43	User_Studies_359"></a>4.3 User Studies</h4>
<p>We also investigate the effectiveness of PinSage by performing head-to-head comparison between different learned representations. In the user study, a user is presented with an image of the query pin, together with two pins retrieved by two different recommendation algorithms. The user is then asked to choose which of the two candidate pins is more related to the query pin. Users are instructed to find various correlations between the recommended items and the query item, in aspects such as visual appearance, object category and personal identity. If both recommended items seem equally related, users have the option to choose “equal”. If no consensus is reached among 2/3 of users who rate the same question, we deem the result as inconclusive.</p>
<p>Table 2 shows the results of the head-to-head comparison be-tween PinSage and the 4 baselines. Among items for which the user has an opinion of which is more related, around 60% of the pre-ferred items are recommended by PinSage. Figure 5 gives examples of recommendations and illustrates strengths and weaknesses of the different methods. The image to the left represents the query item. Each row to the right corresponds to the top recommendations made by the visual embedding baseline, annotation embedding baseline, Pixie, and PinSage. Although visual embeddings gener-ally predict categories and visual similarity well, they occasionally make large mistakes in terms of image semantics. In this example, visual information confused plants with food, and tree logging with war photos, due to similar image style and appearance. The graph-based Pixie method, which uses the graph of pin-to-board relations, correctly understands that the category of query is “plants” and it recommends items in that general category. However, it does not find the most relevant items. Combining both visual/textual and graph information, PinSage is able to find relevant items that are both visually and topically similar to the query item.</p>
<p>我们还通过在不同的学习表示之间进行头对头比较来研究PinSage的有效性。在用户研究中，向用户呈现查询引脚的图像，以及由两个不同推荐算法检索的两个引脚。然后要求用户选择两个候选引脚中的哪一个与查询引脚更相关。指示用户在视觉外观，对象类别和个人身份等方面找到推荐项目和查询项目之间的各种相关性。如果两个推荐项似乎相同，则用户可以选择“相等”。如果对同一问题评分的2/3的用户未达成共识，我们认为结果不确定。</p>
<p>表2显示了PinSage与4个基线之间的头对头比较结果。在用户具有更多相关意见的项目中，PinSage推荐约60％的优先项目。图5给出了建议的示例，并说明了不同方法的优缺点。左侧的图像表示查询项。右侧的每一行对应于可视嵌入基线，注释嵌入基线，Pixie和PinSage所做的最高建议。虽然视觉嵌入通常可以很好地预测类别和视觉相似性，但它们偶尔会在图像语义方面犯大错误。在这个例子中，由于类似的图像样式和外观，视觉信息使植物与食物混淆，并且树木记录与战争照片。基于图形的Pixie方法使用引脚到板的关系图，正确地理解查询的类别是“植物”，并且它推荐该一般类别中的项目。但是，它没有找到最相关的项目。结合视觉/文本和图形信息，PinSage能够找到与查询项目在视觉上和主题上相似的相关项目。</p>
<p>In addition, we visualize the embedding space by randomly choosing 1000 items and compute the 2D t-SNE coordinates from the PinSage embedding, as shown in Figure 6.6 We observe that the proximity of the item embeddings corresponds well with the simi-larity of content, and that items of the same category are embedded into the same part of the space. Note that items that are visually different but have the same theme are also close to each other in the embedding space, as seen by the items depicting different fashion-related items on the bottom side of the plot.</p>
<p>此外，我们通过随机选择1000个项目来可视化嵌入空间，并从PinSage嵌入计算2D t-SNE坐标，如图6.6所示。我们观察到项目嵌入的接近度与内容的相似性很好地对应， 并且相同类别的项目嵌入到空间的相同部分中。 请注意，视觉上不同但具有相同主题的项目在嵌入空间中也彼此接近，如在绘图底部描绘不同时尚相关项目的项目所示。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093244394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4><a id="44	Production_AB_Test_375"></a>4.4 Production A/B Test</h4>
<p>Lastly, we also report on the production A/B test experiments, which compared the performance of PinSage to other deep learning content-based recommender systems at Pinterest on the task of homefeed recommendations. We evaluate the performance by ob-serving the lift in user engagement. The metric of interest is repin rate, which measures the percentage of homefeed recommendations that have been saved by the users. A user saving a pin to a board is a high-value action that signifies deep engagement of the user. It means that a given pin presented to a user at a given time was relevant enough for the user to save that pin to one of their boards so that they can retrieve it later.</p>
<p>We find that PinSage consistently recommends pins that are more likely to be re-pinned by the user than the alternative methods. Depending on the particular setting, we observe 10-30% improve-ments in repin rate over the Annotation and Visual embedding based recommendations.<br /> 最后，我们还报告了生产A / B测试实验，该实验比较了PinSage与Pinterest上其他基于深度学习内容的推荐系统在家庭饲料建议任务上的表现。 我们通过观察用户参与的提升来评估性能。 感兴趣的度量标准是repin rate，用于衡量用户保存的家庭馈送建议的百分比。 用户将引脚保存到电路板是一种高价值的操作，表示用户的深度参与。 这意味着在给定时间呈现给用户的给定引脚足够相关，用户可以将该引脚保存到其中一个板上，以便以后可以检索它。</p>
<p>我们发现PinSage始终推荐比其他方法更有可能被用户重新固定的引脚。 根据特定设置，我们观察到基于Annotation和Visual embedding建议的重复率提高了10-30％。</p>
<h5><a id="45	Training_and_Inference_Runtime_Analysis_383"></a>4.5 Training and Inference Runtime Analysis</h5>
<p>One advantage of GCNs is that they can be made inductive [19]: at the inference (i.e., embedding generation) step, we are able to compute embeddings for items that were not in the training set. This allows us to train on a subgraph to obtain model parameters, and then make embed nodes that have not been observed during training. Also note that it is easy to compute embeddings of new nodes that get added into the graph over time. This means that recommendations can be made on the full (and constantly grow-ing) graph. Experiments on development data demonstrated that training on a subgraph containing 300 million items could achieve the best performance in terms of hit-rate (i.e., further increases in the training set size did not seem to help), reducing the runtime by a factor of 6 compared to training on the full graph.<br /> Table 3 shows the the effect of batch size of the minibatch SGD on the runtime of PinSage training procedure, using the mean-pooling-hard variant. For varying batch sizes, the table shows: (1) the computation time, in milliseconds, for each minibatch, when varying batch size; (2) the number of iterations needed for the model to converge; and (3) the total estimated time for the training proce-dure. Experiments show that a batch size of 2048 makes training most efficient.</p>
<p>When training the PinSage variant with importance pooling, another trade-off comes from choosing the size of neighborhood T . Table 3 shows the runtime and performance of PinSage when</p>
<p>T= 10, 20 and 50. We observe a diminishing return as T increases, and find that a two-layer GCN with neighborhood size 50 can best capture the neighborhood information of nodes, while still being computationally efficient.<br /> After training completes, due to the highly efficient MapReduce inference pipeline, the whole inference procedure to generate em-beddings for 3 billion items can finish in less than 24 hours.</p>
<p>GCN的一个优点是可以使它们具有归纳性[19]：在推理（即嵌入生成）步骤中，我们能够计算不在训练集中的项目的嵌入。这允许我们在子图上训练以获得模型参数，然后制作在训练期间未观察到的嵌入节点。另请注意，计算新节点的嵌入很容易随着时间的推移而添加到图形中。这意味着可以在完整（并且不断增长）的图表上进行推荐。对开发数据的实验表明，对包含3亿个项目的子图的训练可以在命中率方面达到最佳性能（即，训练集大小的进一步增加似乎没有帮助），将运行时间缩短了6倍与完整图表上的培训相比。<br /> 表3显示了使用mean-pooling-hard变体，miniatch SGD的批量大小对PinSage训练过程的运行时间的影响。对于不同的批量大小，该表显示：（1）当批量大小变化时，每个小批量的计算时间（以毫秒为单位）; （2）模型收敛所需的迭代次数; （3）培训程序的总预计时间。实验表明，2048的批量大小使培训效率最高。</p>
<p>在训练具有重要性汇集的PinSage变体时，另一个权衡取决于选择邻域T的大小。表3显示了PinSage的运行时和性能</p>
<p>T = 10,20和50.随着T的增加，我们观察到收益递减，并发现邻域大小为50的双层GCN可以最好地捕获节点的邻域信息，同时仍然具有计算效率。<br /> 培训完成后，由于高效的MapReduce推理管道，生成30亿件物品的整体推理程序可以在不到24小时内完成。<br /> <img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093352552.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br /> 图5：不同算法推荐的Pinterest引脚示例。 左边的图像是查询引脚。 使用Visual em-beddings，Annotation embeddings，基于图形的Pixie和PinSage计算右侧的推荐项目。</p>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093445155.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4><a id="Figure_6_tSNE_plot_of_item_embeddings_in_2_dimensions_405"></a>Figure 6: t-SNE plot of item embeddings in 2 dimensions.</h4>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2019070709351932.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h6><a id="Table_3_Runtime_comparisons_for_different_batch_sizes_409"></a>Table 3: Runtime comparisons for different batch sizes.</h6>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190707093557957.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTY5NzUwNw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h4><a id="Table_4_Performance_tradeoffs_for_importance_pooling_412"></a>Table 4: Performance tradeoffs for importance pooling.</h4>
<h4><a id="5	CONCLUSION_415"></a>5 CONCLUSION</h4>
<p>We proposed PinSage, a random-walk graph convolutional network (GCN). PinSage is a highly-scalable GCN algorithm capable of learn-ing embeddings for nodes in web-scale graphs containing billions of objects. In addition to new techniques that ensure scalability, we introduced the use of importance pooling and curriculum training that drastically improved embedding performance. We deployed PinSage at Pinterest and comprehensively evaluated the quality of the learned embeddings on a number of recommendation tasks, with offline metrics, user studies and A/B tests all demonstrating a substantial improvement in recommendation performance. Our work demonstrates the impact that graph convolutional methods can have in a production recommender system, and we believe that PinSage can be further extended in the future to tackle other graph representation learning problems at large scale, including knowledge graph reasoning and graph clustering.<br /> 我们提出了PinSage，一种随机游走图卷积网络（GCN）。 PinSage是一种高度可扩展的GCN算法，能够在包含数十亿个对象的Web级图形中学习节点的嵌入。 除了确保可扩展性的新技术之外，我们还引入了重要性池和课程训练，大大提高了嵌入性能。 我们在Pinterest部署了PinSage，并在一系列推荐任务中全面评估了学习嵌入的质量，离线指标，用户研究和A / B测试都证明了推荐性能的实质性改进。 我们的工作展示了图卷积方法在生产推荐系统中可能产生的影响，我们相信PinSage可以在未来进一步扩展，以解决大规模的其他图表表示学习问题，包括知识图推理和图聚类。</p>
<h2><a id="Acknowledgments_419"></a>Acknowledgments</h2>
<p>The authors acknowledge Raymond Hsu, Andrei Curelea and Ali Altaf for performing various A/B tests in production system, Jerry</p>
<p>Zitao Liu for providing data used by Pixie[14], and Vitaliy Kulikov for help in nearest neighbor query of the item embeddings.</p>
<h2><a id="REFERENCES_429"></a>REFERENCES</h2>
<p>[1]M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, et al. 2016. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 (2016).</p>
<p>[2]A. Andoni and P. Indyk. 2006. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. In FOCS.<br /> [3]T. Bansal, D. Belanger, and A. McCallum. 2016. Ask the GRU: Multi-task learning for deep text recommendations. In RecSys. ACM.<br /> [4]Y. Bengio, J. Louradour, R. Collobert, and J. Weston. 2009. Curriculum learning. In ICML.<br /> [5]A. Z. Broder, D. Carmel, M. Herscovici, A. Soffer, and J. Zien. 2003. Efficient query evaluation using a two-level retrieval process. In CIKM.<br /> [6]M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, and P. Vandergheynst. 2017. Geometric deep learning: Going beyond euclidean data. IEEE Signal Processing Magazine 34, 4 (2017).</p>
<p>[7]J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun. 2014. Spectral networks and locally connected networks on graphs. In ICLR.<br /> [8]J. Chen, T. Ma, and C. Xiao. 2018. FastGCN: Fast Learning with Graph Convolu-tional Networks via Importance Sampling. ICLR (2018).<br /> [9]P. Covington, J. Adams, and E. Sargin. 2016. Deep neural networks for youtube recommendations. In RecSys. ACM.<br /> [10]H. Dai, B. Dai, and L. Song. 2016. Discriminative Embeddings of Latent Variable Models for Structured Data. In ICML.<br /> [11]M. Defferrard, X. Bresson, and P. Vandergheynst. 2016. Convolutional neural networks on graphs with fast localized spectral filtering. In NIPS.<br /> [12]A. Van den Oord, S. Dieleman, and B. Schrauwen. 2013. Deep content-based music recommendation. In NIPS.<br /> [13]D. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams. 2015. Convolutional networks on graphs for learning molecular fingerprints. In NIPS.</p>
<p>[14]C. Eksombatchai, P. Jindal, J. Z. Liu, Y. Liu, R. Sharma, C. Sugnet, M. Ulrich, and</p>
<p>J.Leskovec. 2018. Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time. WWW (2018).<br /> [15]M. Gori, G. Monfardini, and F. Scarselli. 2005. A new model for learning in graph domains. In IEEE International Joint Conference on Neural Networks.</p>
<p>[16]P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch,</p>
<p>Y.Jia, and K. He. 2017. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. arXiv preprint arXiv:1706.02677 (2017).<br /> [17]A. Grover and J. Leskovec. 2016. node2vec: Scalable feature learning for networks. In KDD.<br /> [18]W. L. Hamilton, R. Ying, and J. Leskovec. 2017. Inductive Representation Learning on Large Graphs. In NIPS.<br /> [19]W. L. Hamilton, R. Ying, and J. Leskovec. 2017. Representation Learning on Graphs: Methods and Applications. IEEE Data Engineering Bulletin (2017).</p>
<p>[20]S. Kearnes, K. McCloskey, M. Berndl, V. Pande, and P. Riley. 2016. Molecular graph convolutions: moving beyond fingerprints. CAMD 30, 8.<br /> [21]T. N. Kipf and M. Welling. 2017. Semi-supervised classification with graph convolutional networks. In ICLR.<br /> [22]Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. 2015. Gated graph sequence neural networks. In ICLR.<br /> [23]T. Mikolov, I Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013. Distributed representations of words and phrases and their compositionality. In NIPS.<br /> [24]F. Monti, M. M. Bronstein, and X. Bresson. 2017. Geometric matrix completion with recurrent multi-graph neural networks. In NIPS.<br /> [25]OpenMP Architecture Review Board. 2015. OpenMP Application Program Inter-face Version 4.5. (2015).<br /> [26]B. Perozzi, R. Al-Rfou, and S. Skiena. 2014. DeepWalk: Online learning of social representations. In KDD.<br /> [27]F. Scarselli, M. Gori, A.C. Tsoi, M. Hagenbuchner, and G. Monfardini. 2009. The graph neural network model. IEEE Transactions on Neural Networks 20, 1 (2009), 61–80.</p>
<p>[28]K. Simonyan and A. Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).<br /> [29]R. van den Berg, T. N. Kipf, and M. Welling. 2017. Graph Convolutional Matrix Completion. arXiv preprint arXiv:1706.02263 (2017).<br /> [30]J. You, R. Ying, X. Ren, W. L. Hamilton, and J. Leskovec. 2018. GraphRNN: Generating Realistic Graphs using Deep Auto-regressive Models. ICML (2018).<br /> [31]M. Zitnik, M. Agrawal, and J. Leskovec. 2018. Modeling polypharmacy side effects with graph convolutional networks. Bioinformatics (2018).</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>比特币背后的数据秘密，币价涨跌规律暗藏玄机</title>
		<link>https://uzzz.org/article/2281.html</link>
				<pubDate>Wed, 06 Mar 2019 10:01:26 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[区块链]]></category>
		<category><![CDATA[大数据]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2281.html</guid>
				<description><![CDATA[币圈历经7个月的寒冬之后，比特币价格在今年2月首次出现连续上涨，涨幅超过11%，一度突破4000美元。 &#160; 近期，据Gikee.com链上数据监测显示：今年2月的交易量也是自去年3月以来的最高月度交易量，BTC交易量超过2000亿美元，ETH交易量超过1000亿美元。 &#160; &#160; BTC &#160; 由于比特币在2月中下旬突破4000美元，交易量环比增长34%，这是BTC交易量是一年内的最高，BTC交易在一个月内交易量近2063.7亿美元。 &#160; &#160; ETH &#160; 以太坊的交易量也达到了一年来的高位，在2月份，以太坊交易了价值1044.6亿美元的以太坊，ETH交易环比增长34.40％。 &#160; EOS &#160; 在2月份，EOS交易量为332.2亿美元，交易量增长54.75％，连续第5个月成为交易量第4的加密货币，是自2018年5月以来EOS交易量最高的月份。 &#160; &#160; LTC &#160; 莱特币在2月份的交易量排名第5，约有328.5亿美元的交易量，交易量环比增长93.58％，这是自2018年第一季度以来交易量最高的月份。 &#160; &#160; 2月比特币价格飙升11％鼓舞了市场情绪，比特币链上交易量和交易所总交易量都达到了7个月以来的高位，但它还没有真正走出熊市困境，市场触底在即，投资者在等待市场下一次大幅调整。 &#160; 目前，整个加密货币市场市值徘徊在1290亿美元左右，交易量开始略有减少，过去24小时内交易量为249亿美元，市场持续横盘，也许下一轮大幅调整马上要来了。 &#160; 今日，据Gikee.com排行榜统计显示，目前交易量TOP 3是USDT、BTC、ETH。 &#160; &#160; &#160; 3月6日市值TOP 10币种 &#160; &#160; &#160; &#160; &#160; 而在今年，加密货币可能会触及市场行情的底部，并且今年大部分时间比特币价格可能徘徊在3000到5000美元之间。 也许直到2020年比特币产量减半的时候，我们才会迎来下一轮真正的牛市。]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
<p style="margin-left:0pt;"><span style="color:#000000;">币圈历经7个月的寒冬之后，比特币价格在今年2月首次出现连续上涨，涨幅超过11%，一度突破4000美元。</span></p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;"><span style="color:#000000;">近期，据Gikee.com链上数据监测显示：今年2月的交易量也是自去年3月以来的最高月度交易量，BTC交易量超过2000亿美元，ETH交易量超过1000亿美元。</span></p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;"><span style="color:#000000;">BTC</span></p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;"><span style="color:#000000;">由于比特币在2月中下旬突破4000美元，交易量环比增长34%，这是BTC交易量是一年内的最高，BTC交易在一个月内交易量近2063.7亿美元。</span></p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;"><span style="color:#000000;">ETH</span></p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;"><span style="color:#000000;">以太坊的交易量也达到了一年来的高位，在2月份，以太坊交易了价值1044.6亿美元的以太坊，ETH交易环比增长34.40％。</span></p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;"><span style="color:#000000;">EOS</span></p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;"><span style="color:#000000;">在2月份，EOS交易量为332.2亿美元，交易量增长54.75％，连续第5个月成为交易量第4的加密货币，是自2018年5月以来EOS交易量最高的月份。</span></p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;"><span style="color:#000000;">LTC</span></p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;"><span style="color:#000000;">莱特币在2月份的交易量排名第5，约有328.5亿美元的交易量，交易量环比增长93.58％，这是自2018年第一季度以来交易量最高的月份。</span></p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;"><span style="color:#000000;">2月比特币价格飙升11％鼓舞了市场情绪，比特币链上交易量和交易所总交易量都达到了7个月以来的高位，但它还没有真正走出熊市困境，市场触底在即，投资者在等待市场下一次大幅调整。</span></p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;"><span style="color:#000000;">目前，整个加密货币市场市值徘徊在1290亿美元左右，交易量开始略有减少，过去24小时内交易量为249亿美元，市场持续横盘，也许下一轮大幅调整马上要来了。</span></p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;"><span style="color:#000000;">今日，据Gikee.com排行榜统计显示，目前交易量TOP 3是USDT、BTC、ETH。</span></p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;"><span style="color:#000000;">3月6日市值TOP 10币种</span></p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;">&nbsp;</p>
<p style="margin-left:0pt;"><span style="color:#000000;">而在今年，加密货币可能会触及市场行情的底部，并且今年大部分时间比特币价格可能徘徊在3000到5000美元之间。</span></p>
<p style="margin-left:0pt;"><span style="color:#000000;">也许直到2020年比特币产量减半的时候，我们才会迎来下一轮真正的牛市。</span></p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>Breeze安装常见错误锦集</title>
		<link>https://uzzz.org/article/1392.html</link>
				<pubDate>Fri, 04 Jan 2019 04:49:16 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[大数据]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1392.html</guid>
				<description><![CDATA[前端Web UI的日志如果还不能判断出具体问题所在，可以在部署机上输入命令 docker logs -f deploy-main 来获取更详细的日志 docker、registry、etcd、k8s这四个角色是缺一不可的，不能缺少组件。 节点主机内存不能太低，建议最少4G配置，否则kubeadm部署过程中master节点可能会卡死在等待kubelet服务启动的过程中而导致最终部署失败。 Breeze部署工具底层是调用ansible执行playbook脚本，所以对宿主机环境而言，python的版本兼容性是相关联的，如果在部署中看见了Failed to import docker-py &#8211; No module named &#8216;requests.packages.urllib3&#8217;. Try pip install docker-py这样的信息，请修正您宿主机的python依赖问题后再进行部署。 Breeze暂不支持非root账号的环境部署，因此请确保您部署机到各个服务器节点是直接root ssh免密的。 部署好之后，dashboard的端口是30300，但是谷歌浏览器是不可以访问的，火狐可以，这个是浏览器安全设置问题，和部署没有关系。 由于CentOS的特性，部署之后内核并未启动ipvs，因此kube-proxy服务中会看见警告日志，退回iptables方式，这个只需要将所有节点重启一次即可解决。 在部署机上，一定不要忘记执行“（1）对部署机取消SELINUX设定及放开防火墙”，否则会导致selinux的限制而无法创建数据库文件cluster.db，页面提示“unable to open database file”。 不要这样去关闭防火墙 systemctl stop firewalld 或 systemctl disable firewalld，我们的部署过程中已经做了正确的防火墙规则设定，服务是必须开启的，只是设定为可信任模式，也就是放开所有访问策略，如果你需要设定严格的防火墙规则，请自行学习研究清楚firewall-cmd的用法。 详细注解： iptables与firewalld都不是真正的防火墙，它们都只是用来定义防火墙策略的防火墙管理工具而已，或者说，它们只是一种服务。iptables服务会把配置好的防火墙策略交由内核层面的netfilter网络过滤器来处理，而firewalld服务则是把配置好的防火墙策略交由内核层面的nftables包过滤框架来处理。对于RHEL/CentOS 7系列，我们推荐的做法就是删掉iptables服务启用firewalld服务，注意不是删掉iptables命令。然后用命令firewall-cmd &#8211;set-default-zone=trusted来“关闭”防火墙。这样docker和kubernetes运行时才不会出故障。 docker最终还是要调用iptables命令的，它不在乎你的系统底层究竟是iptables服务还是firewalld服务，总之要么转换成netfilter模块执行要么转换成nftables模块执行。我们的部署程序，在安装docker的环节中，已经为您的主机做了这样的设置。也就是防火墙服务是active的，但是policy是trusted，这样是最佳方法。当然如果您实际生产环境不允许过于宽松的防火墙，可以手动再去使用firewall-cmd命令控制严格的具体ACL条目。 所有被部署的服务器在部署工作开始之前请使用命令： hostnamectl set-hostname 主机名]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<ol>
<li>前端Web UI的日志如果还不能判断出具体问题所在，可以在部署机上输入命令 docker logs -f deploy-main 来获取更详细的日志</li>
<li>docker、registry、etcd、k8s这四个角色是缺一不可的，不能缺少组件。</li>
<li>节点主机内存不能太低，建议最少4G配置，否则kubeadm部署过程中master节点可能会卡死在等待kubelet服务启动的过程中而导致最终部署失败。</li>
<li>Breeze部署工具底层是调用ansible执行playbook脚本，所以对宿主机环境而言，python的版本兼容性是相关联的，如果在部署中看见了Failed to import docker-py &#8211; No module named &#8216;requests.packages.urllib3&#8217;. Try pip install docker-py这样的信息，请修正您宿主机的python依赖问题后再进行部署。</li>
<li>Breeze暂不支持非root账号的环境部署，因此请确保您部署机到各个服务器节点是直接root ssh免密的。</li>
<li>部署好之后，dashboard的端口是30300，但是谷歌浏览器是不可以访问的，火狐可以，这个是浏览器安全设置问题，和部署没有关系。</li>
<li>由于CentOS的特性，部署之后内核并未启动ipvs，因此kube-proxy服务中会看见警告日志，退回iptables方式，这个只需要将所有节点重启一次即可解决。</li>
<li>在部署机上，一定不要忘记执行“（1）对部署机取消SELINUX设定及放开防火墙”，否则会导致selinux的限制而无法创建数据库文件cluster.db，页面提示“unable to open database file”。</li>
<li>不要这样去关闭防火墙 systemctl stop firewalld 或 systemctl disable firewalld，我们的部署过程中已经做了正确的防火墙规则设定，服务是必须开启的，只是设定为可信任模式，也就是放开所有访问策略，如果你需要设定严格的防火墙规则，请自行学习研究清楚firewall-cmd的用法。</li>
</ol>
<p>详细注解： iptables与firewalld都不是真正的防火墙，它们都只是用来定义防火墙策略的防火墙管理工具而已，或者说，它们只是一种服务。iptables服务会把配置好的防火墙策略交由内核层面的netfilter网络过滤器来处理，而firewalld服务则是把配置好的防火墙策略交由内核层面的nftables包过滤框架来处理。对于RHEL/CentOS 7系列，我们推荐的做法就是删掉iptables服务启用firewalld服务，注意不是删掉iptables命令。然后用命令firewall-cmd &#8211;set-default-zone=trusted来“关闭”防火墙。这样docker和kubernetes运行时才不会出故障。 docker最终还是要调用iptables命令的，它不在乎你的系统底层究竟是iptables服务还是firewalld服务，总之要么转换成netfilter模块执行要么转换成nftables模块执行。我们的部署程序，在安装docker的环节中，已经为您的主机做了这样的设置。也就是防火墙服务是active的，但是policy是trusted，这样是最佳方法。当然如果您实际生产环境不允许过于宽松的防火墙，可以手动再去使用firewall-cmd命令控制严格的具体ACL条目。</p>
<ol>
<li>所有被部署的服务器在部署工作开始之前请使用命令：
<pre class="has">
<code>hostnamectl set-hostname 主机名 
</code></pre>
</li>
</ol></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>基于VirtualBox安装Ubuntu 18.04.1 LTS</title>
		<link>https://uzzz.org/article/3469.html</link>
				<pubDate>Thu, 27 Dec 2018 16:55:44 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[大数据]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/3469.html</guid>
				<description><![CDATA[大家好，本文将带大家使用VirtualBox安装Ubuntu 18.04.1 LTS。 （0）镜像地址下载：Ubuntu Server 18.04.1 LTS 安装步骤： （1）virtualbox新建虚拟电脑，依次点击下一步 （2）选择虚拟机内存 （3）为虚拟机创建一块虚拟硬盘。选择“现在创建虚拟硬盘”，并单击“创建”。 &#160; &#160; &#160; &#160; &#160;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;&#160;&#160; &#160; （4）选择虚拟硬盘文件类型。选择默认的VDI（VirtualBox磁盘映像），并单击“下一步”。 （5）选择“动态分配”，并单击“下一步”。 （6）输入虚拟硬盘文件的名称，并选择保存位置 （7）基础配置完毕，显示如下界面。 &#160; &#160; &#160; &#160; &#160; （8）设置&#8211;&#62;存储&#8211;&#62;控制器&#8211;&#62;属性，选择已经下载的镜像 &#160; &#160; （9）右键启动虚拟机，即将进入安装流程 （10）安装界面 &#160; &#160; （11）安装成功界面如下 （12）安装完毕，进行网络配置 注：大多数朋友选择配置静态编辑网络配置文件进行静态IP配置，但由于操作复杂，而且配置难度比较大， 下面教大家用一个简便的操作。 注：网卡1配置&#8217;网络地址转换(NAT)&#8217; &#160; 注：网卡2配置&#8217;仅主机(Host-Only)网络&#8217; （13）管理&#8211;&#62;主机网络管理器，设置网卡 设置DHCP服务器 配置了主机网络管理器之后，我们需要打开虚拟机，点击图中的配置 配置Ethernet，第一个默认不需要配置，只需要配置第二个，例如我的是enp0s8 &#160; 配置IPv4结束后，请点击APPLY应用，然后关闭窗口，打开终端，查看配置好的IP：ifconfig 到此，&#160;Ubuntu安装完毕，网络配置完成！ &#160; &#160; 下一步就是使用XShell远程连接虚拟机 安装Ubuntu18.04教程结束，谢谢大家，共同进步！ &#160;]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
<h3>大家好，本文将带大家使用VirtualBox安装Ubuntu 18.04.1 LTS。</h3>
<hr>
<p>（0）镜像地址下载：<a href="https://www.ubuntu.com/download/desktop" rel="nofollow" data-token="9d28b0996dd5f47f49d454707bc42080">Ubuntu Server 18.04.1 LTS</a></p>
<p><img alt="" class="has" height="547" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181227221142855.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="1083"></p>
<p>安装步骤：</p>
<p>（1）virtualbox新建虚拟电脑，依次点击下一步</p>
<p><img alt="" class="has" height="568" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181227222921868.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="799"></p>
<p>（2）选择虚拟机内存</p>
<p><img alt="memory" class="has" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181228005544517"></p>
<p>（3）为虚拟机创建一块虚拟硬盘。选择“现在创建虚拟硬盘”，并单击“创建”。</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;<img alt="" class="has" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181228005544560"></p>
<h1>&nbsp;</h1>
<p>（4）选择虚拟硬盘文件类型。选择默认的VDI（VirtualBox磁盘映像），并单击“下一步”。</p>
<p><img alt="disktype" class="has" height="583" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181228005544598" width="813"></p>
<p>（5）选择“动态分配”，并单击“下一步”。</p>
<p><img alt="" class="has" height="585" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181228005544639" width="815"></p>
<p>（6）输入虚拟硬盘文件的名称，并选择保存位置</p>
<p><img alt="" class="has" height="585" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181227224944795.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="816"></p>
<p>（7）基础配置完毕，显示如下界面。</p>
<p><img alt="" class="has" height="550" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181227225208961.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="815"></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>（8）设置&#8211;&gt;存储&#8211;&gt;控制器&#8211;&gt;属性，选择已经下载的镜像</p>
<p><img alt="" class="has" height="484" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181227225653453.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="707"></p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>（9）右键启动虚拟机，即将进入安装流程</p>
<p><img alt="" class="has" height="333" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181227230057265.png" width="199"></p>
<p>（10）安装界面</p>
<p><img alt="" class="has" height="601" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2018122723045611.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="802"></p>
<p><img alt="" class="has" height="600" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181227230813777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="799"></p>
<p><img alt="" class="has" height="602" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181227230854615.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="801"></p>
<p>&nbsp;</p>
<p><img alt="" class="has" height="601" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181227230932887.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="801"></p>
<p><img alt="" class="has" height="602" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2018122723122338.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="799"></p>
<p><img alt="" class="has" height="598" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181227231400529.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="800"></p>
<p>&nbsp;</p>
<p>（11）安装成功界面如下</p>
<p><img alt="" class="has" height="675" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181227231654650.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="799"></p>
<p>（12）安装完毕，进行网络配置</p>
<p>注：大多数朋友选择配置静态编辑网络配置文件进行静态IP配置，但由于操作复杂，而且配置难度比较大，</p>
<p>下面教大家用一个简便的操作。</p>
<p>注：网卡1配置&#8217;网络地址转换(NAT)&#8217;</p>
<p><img alt="" class="has" height="484" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181227231523620.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="711"></p>
<p>&nbsp;</p>
<p>注：网卡2配置&#8217;仅主机(Host-Only)网络&#8217;</p>
<p><img alt="" class="has" height="486" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181227231601887.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="711"></p>
<p>（13）管理&#8211;&gt;主机网络管理器，设置网卡</p>
<p><img alt="" class="has" height="421" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181228002818480.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="639"></p>
<p>设置DHCP服务器</p>
<p><img alt="" class="has" height="421" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181228002849684.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="641"></p>
<p>配置了主机网络管理器之后，我们需要打开虚拟机，点击图中的配置</p>
<p><img alt="" class="has" height="594" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181228003259412.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="706"></p>
<p>配置Ethernet，第一个默认不需要配置，只需要配置第二个，例如我的是enp0s8</p>
<p><img alt="" class="has" height="457" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181228003416255.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="698"></p>
<p>&nbsp;</p>
<p><img alt="" class="has" height="498" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2018122800370047.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="697"></p>
<p><img alt="" class="has" height="499" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181228004032740.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="697"></p>
<p>配置IPv4结束后，请点击APPLY应用，然后关闭窗口，打开终端，查看配置好的IP：ifconfig</p>
<p><img alt="" class="has" height="460" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181228004248740.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="695"></p>
<p>到此，&nbsp;Ubuntu安装完毕，网络配置完成！</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p><strong>下一步就是使用XShell远程连接虚拟机</strong></p>
<p><img alt="" class="has" height="514" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181228004543828.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="592"></p>
<p><img alt="" class="has" height="396" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181228004717616.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0ppbmdtaW5nQ2hlbg==,size_16,color_FFFFFF,t_70" width="596"></p>
<p>安装Ubuntu18.04教程结束，谢谢大家，共同进步！</p>
<p>&nbsp;</p>
<hr>
 </div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>python数据分析：关联规则学习（Association rule learning）</title>
		<link>https://uzzz.org/article/2522.html</link>
				<pubDate>Sat, 01 Dec 2018 06:58:03 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[大数据]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2522.html</guid>
				<description><![CDATA[何为关联规则学习 关联规则学习是一种基于规则的机器学习方法，用于发现大型数据库中变量之间的有趣关系。它旨在使用一些有趣的度量来识别在数据库中发现的强规则。这种基于规则的方法在分析更多数据时也会生成新规则。假设数据集足够大，最终目标是帮助机器模拟人类大脑的特征提取和新未分类数据的抽象关联能力。 基于强有力规则的概念，Rakesh Agrawal，TomaszImieliński和Arun Swami介绍了关联规则，用于发现超市中销售点（POS）系统记录的大规模交易数据中产品之间的规律性。例如，在超市的销售数据中发现，如果顾客一起购买洋葱和土豆，他们也可能会购买汉堡肉。这些信息可以用作关于营销活动的决策的基础，例如促销定价或产品放置。 除了市场购物篮分析的上述示例之外，目前在许多应用领域中使用关联规则，包括Web使用挖掘，入侵检测，连续生产和生物信息学。与序列挖掘相反，关联规则学习通常不考虑事务内或跨事务的项目顺序。 常用的关联算法包括 Apriori、FP-Growth、PrefixSpan、SPADE、AprioriAll、Apriori-Some等。 关联算法评估规则 频繁规律与有效规则： 频繁规则指的是关联结果中支持度和置信度都比较高的规则 有效规则指关联规则真正能促进规则中的前后项提升 假如，数据集有1000条事务数据用来显示购买苹果和香蕉的订单记录，其中有600个客户的订单记录中包含了苹果，有800个客户的订单记录中包含了香蕉，而有400个客户同时购买了苹果和香蕉。假如我们产生了一条关联规则，用来表示购买了苹果的客户中还有很多人购买香蕉，那么该规则可以表示为：苹果→香蕉。 支持度：support = 400/1000= 40% 置信度：confidence = 400/600=67% 如果只是看支持度和置信度，这个规则似乎非常显著的说明了苹果和香蕉之间的频繁关系，买了苹果的客户中有67%的人也会一起购买香蕉。但是，如果忽略购买苹果的事实，只购买香蕉的客户比例会高达是80%（800/1000）！这显示了购买苹果这种条件不会对购买香蕉产生积极的促进作用，反而会阻碍其销售，苹果和香蕉之间是一种负相关的关系。因此，只看支持度和置信度将无法完整体现规则的有效性，这里我们使用提升度来有效应对该问题。 提升度（Lift）指的是应用关联规则和不应用产生结果的比例。在本示例中，Lift=(400)/(800)=0.5（有关联规则的前提下只有400个客户会购买香蕉，没有关联规则的前提下会有800个购买香蕉）。当提升度为1时，说明应用关联规则和不应用关联规则产生相同的结果；当提升度大于1时，说明应用关联规则比不应用关联规则能产生更好的结果；当提升度小于1时，关联规则具有负相关的作用，该规则是无效规则。 做关联规则评估时，需要综合考虑支持度、置信度和提升度三个指标，支持度和置信度的值越大越好。 提升度低的负相关关联其实也是一种关联模式，也是可以通过避免的方式利用这种关联：不将互斥商品放在一个组合中，不将互斥广告投放整合投放，不将互斥关键字提供个客户，不将互斥信息流展现给客户。 运营分析中关联分析的使用 网站页面关联分析：帮助我们找到用户在不同页面之间的频繁访问关系，以分析用户特定的页面浏览方式，这样可以帮助了解不同页面之间的分流和引流情况，可用于不同页面间的推荐已达到提高转化率。 广告流量关联分析：针对站外广告投放渠道用户浏览或点击的行为分析，该分析主要用于了解用户的浏览和点击广告的规则。这种站外广告曝光和点击的关联分析可以为广告客户的精准投放提供参考。 用户关键字搜索：通过分析用户在站内的搜索关键字了解用户真实需求的。通过对用户搜索的关键字的关联分析，可以得到类似于搜索了iPhone又搜索了三星，这种关联可以用于搜索推荐、搜索关联等场景，有助于改善搜索体验，提高客户的目标转化率。 不同场景发生，这种模式可以广泛用于分析运营中关注的要素，例如用户浏览商品和购买商品的关联，关注产品价格和购买产品价格的关联，加入购物车与购买的关联。这种关联可以找到用户在一个事件中不同行为之间的关联，可以用来挖掘用户的真实需求，有针对性的对当前用户进行个性化推荐，同时也对定价策略有参考价值。 相同场景发生，用户在同一个页面中点击不同功能、选择不同的应用。这种关联可以用于分析用户使用功能的先后顺序，有利于做产品优化和用户体验提升；对于不同产品功能组合、开发和升级有了更加明确地参考方向，便于针对永辉习惯性操作模式做功能迭代；同时针对用户频繁查看和点击的内容，可以采用打包、组合、轮转等策略，帮助客户查找，同时增加内容曝光度和用户体验。 python代码实现 apriori模块： # -*- coding: utf-8 -*- from numpy import * import re def createData(fileName): mat = [] req = re.compile(r',') fr = open(fileName) content = fr.readlines() for line in content: tem = line.replace('\n','').split(',') mat.append(tem) return mat def loadDataSet(): return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]] # C1 是大小为1的所有候选项集的集合 def createC1(dataSet): C1 = [] for transaction in dataSet: for item in transaction: if not [item] in C1: C1.append([item]) #store all the item unrepeatly C1.sort() #return map(frozenset, C1)#frozen set, user can't change it. return list(map(frozenset, C1)) def scanD(D,Ck,minSupport): ssCnt={} for tid in D: for can in Ck: if can.issubset(tid): #if not ssCnt.has_key(can): if not can in ssCnt: ssCnt[can]=1 else: ssCnt[can]+=1 numItems=float(len(D)) retList = [] supportData = {} for key in ssCnt: support = ssCnt[key]/numItems #compute support if support &#62;= minSupport: retList.insert(0,key) supportData[key] = support return retList, supportData #total apriori def aprioriGen(Lk, k): #组合，向上合并 #creates Ck]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<h2><a id="_0"></a>何为关联规则学习</h2>
<p>关联规则学习是一种基于规则的机器学习方法，用于发现大型数据库中变量之间的有趣关系。它旨在使用一些有趣的度量来识别在数据库中发现的强规则。这种基于规则的方法在分析更多数据时也会生成新规则。假设数据集足够大，最终目标是帮助机器模拟人类大脑的特征提取和新未分类数据的抽象关联能力。</p>
<p>基于强有力规则的概念，Rakesh Agrawal，TomaszImieliński和Arun Swami介绍了关联规则，用于发现超市中销售点（POS）系统记录的大规模交易数据中产品之间的规律性。例如，在超市的销售数据中发现，如果顾客一起购买洋葱和土豆，他们也可能会购买汉堡肉。这些信息可以用作关于营销活动的决策的基础，例如促销定价或产品放置。</p>
<p>除了市场购物篮分析的上述示例之外，目前在许多应用领域中使用关联规则，包括Web使用挖掘，入侵检测，连续生产和生物信息学。与序列挖掘相反，关联规则学习通常不考虑事务内或跨事务的项目顺序。</p>
<h2><a id="_8"></a>常用的关联算法包括</h2>
<p>Apriori、FP-Growth、PrefixSpan、SPADE、AprioriAll、Apriori-Some等。</p>
<h2><a id="_12"></a>关联算法评估规则</h2>
<p>频繁规律与有效规则：</p>
<ul>
<li>频繁规则指的是关联结果中支持度和置信度都比较高的规则</li>
<li>有效规则指关联规则真正能促进规则中的前后项提升</li>
</ul>
<p>假如，数据集有1000条事务数据用来显示购买苹果和香蕉的订单记录，其中有600个客户的订单记录中包含了苹果，有800个客户的订单记录中包含了香蕉，而有400个客户同时购买了苹果和香蕉。假如我们产生了一条关联规则，用来表示购买了苹果的客户中还有很多人购买香蕉，那么该规则可以表示为：苹果→香蕉。<br /> 支持度：support = 400/1000= 40%<br /> 置信度：confidence = 400/600=67%<br /> 如果只是看支持度和置信度，这个规则似乎非常显著的说明了苹果和香蕉之间的频繁关系，买了苹果的客户中有67%的人也会一起购买香蕉。但是，如果忽略购买苹果的事实，只购买香蕉的客户比例会高达是80%（800/1000）！这显示了购买苹果这种条件不会对购买香蕉产生积极的促进作用，反而会阻碍其销售，苹果和香蕉之间是一种负相关的关系。因此，只看支持度和置信度将无法完整体现规则的有效性，这里我们使用提升度来有效应对该问题。</p>
<p>提升度（Lift）指的是应用关联规则和不应用产生结果的比例。在本示例中，Lift=(400)/(800)=0.5（有关联规则的前提下只有400个客户会购买香蕉，没有关联规则的前提下会有800个购买香蕉）。当提升度为1时，说明应用关联规则和不应用关联规则产生相同的结果；当提升度大于1时，说明应用关联规则比不应用关联规则能产生更好的结果；当提升度小于1时，关联规则具有负相关的作用，该规则是无效规则。</p>
<p>做关联规则评估时，需要综合考虑支持度、置信度和提升度三个指标，支持度和置信度的值越大越好。</p>
<p>提升度低的负相关关联其实也是一种关联模式，也是可以通过避免的方式利用这种关联：不将互斥商品放在一个组合中，不将互斥广告投放整合投放，不将互斥关键字提供个客户，不将互斥信息流展现给客户。</p>
<h2><a id="_30"></a>运营分析中关联分析的使用</h2>
<ul>
<li>网站页面关联分析：帮助我们找到用户在不同页面之间的频繁访问关系，以分析用户特定的页面浏览方式，这样可以帮助了解不同页面之间的分流和引流情况，可用于不同页面间的推荐已达到提高转化率。</li>
<li>广告流量关联分析：针对站外广告投放渠道用户浏览或点击的行为分析，该分析主要用于了解用户的浏览和点击广告的规则。这种站外广告曝光和点击的关联分析可以为广告客户的精准投放提供参考。</li>
<li>用户关键字搜索：通过分析用户在站内的搜索关键字了解用户真实需求的。通过对用户搜索的关键字的关联分析，可以得到类似于搜索了iPhone又搜索了三星，这种关联可以用于搜索推荐、搜索关联等场景，有助于改善搜索体验，提高客户的目标转化率。</li>
<li>不同场景发生，这种模式可以广泛用于分析运营中关注的要素，例如用户浏览商品和购买商品的关联，关注产品价格和购买产品价格的关联，加入购物车与购买的关联。这种关联可以找到用户在一个事件中不同行为之间的关联，可以用来挖掘用户的真实需求，有针对性的对当前用户进行个性化推荐，同时也对定价策略有参考价值。</li>
<li>相同场景发生，用户在同一个页面中点击不同功能、选择不同的应用。这种关联可以用于分析用户使用功能的先后顺序，有利于做产品优化和用户体验提升；对于不同产品功能组合、开发和升级有了更加明确地参考方向，便于针对永辉习惯性操作模式做功能迭代；同时针对用户频繁查看和点击的内容，可以采用打包、组合、轮转等策略，帮助客户查找，同时增加内容曝光度和用户体验。</li>
</ul>
<h2><a id="python_38"></a>python代码实现</h2>
<p>apriori模块：</p>
<pre><code class="prism language-python"><span class="token comment"># -*- coding: utf-8 -*-</span>

<span class="token keyword">from</span> numpy <span class="token keyword">import</span> <span class="token operator">*</span>
<span class="token keyword">import</span> re

<span class="token keyword">def</span> <span class="token function">createData</span><span class="token punctuation">(</span>fileName<span class="token punctuation">)</span><span class="token punctuation">:</span>
    mat <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    req <span class="token operator">=</span> re<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>r<span class="token string">','</span><span class="token punctuation">)</span>
    fr <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span>fileName<span class="token punctuation">)</span>
    content <span class="token operator">=</span> fr<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> line <span class="token keyword">in</span> content<span class="token punctuation">:</span>
        tem <span class="token operator">=</span> line<span class="token punctuation">.</span>replace<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">,</span><span class="token string">''</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">','</span><span class="token punctuation">)</span>
        mat<span class="token punctuation">.</span>append<span class="token punctuation">(</span>tem<span class="token punctuation">)</span>
    <span class="token keyword">return</span> mat

<span class="token keyword">def</span> <span class="token function">loadDataSet</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">]</span>

<span class="token comment"># C1 是大小为1的所有候选项集的集合</span>
<span class="token keyword">def</span> <span class="token function">createC1</span><span class="token punctuation">(</span>dataSet<span class="token punctuation">)</span><span class="token punctuation">:</span>
    C1 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> transaction <span class="token keyword">in</span> dataSet<span class="token punctuation">:</span>
        <span class="token keyword">for</span> item <span class="token keyword">in</span> transaction<span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token operator">not</span> <span class="token punctuation">[</span>item<span class="token punctuation">]</span> <span class="token keyword">in</span> C1<span class="token punctuation">:</span>
                C1<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span>item<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment">#store all the item unrepeatly</span>

    C1<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment">#return map(frozenset, C1)#frozen set, user can't change it.</span>
    <span class="token keyword">return</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token builtin">frozenset</span><span class="token punctuation">,</span> C1<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">scanD</span><span class="token punctuation">(</span>D<span class="token punctuation">,</span>Ck<span class="token punctuation">,</span>minSupport<span class="token punctuation">)</span><span class="token punctuation">:</span>
    ssCnt<span class="token operator">=</span><span class="token punctuation">{</span><span class="token punctuation">}</span>
    <span class="token keyword">for</span> tid <span class="token keyword">in</span> D<span class="token punctuation">:</span>
        <span class="token keyword">for</span> can <span class="token keyword">in</span> Ck<span class="token punctuation">:</span>
            <span class="token keyword">if</span> can<span class="token punctuation">.</span>issubset<span class="token punctuation">(</span>tid<span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token comment">#if not ssCnt.has_key(can):</span>
                <span class="token keyword">if</span> <span class="token operator">not</span> can <span class="token keyword">in</span> ssCnt<span class="token punctuation">:</span>
                    ssCnt<span class="token punctuation">[</span>can<span class="token punctuation">]</span><span class="token operator">=</span><span class="token number">1</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span> ssCnt<span class="token punctuation">[</span>can<span class="token punctuation">]</span><span class="token operator">+=</span><span class="token number">1</span>
    numItems<span class="token operator">=</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>D<span class="token punctuation">)</span><span class="token punctuation">)</span>
    retList <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    supportData <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
    <span class="token keyword">for</span> key <span class="token keyword">in</span> ssCnt<span class="token punctuation">:</span>
        support <span class="token operator">=</span> ssCnt<span class="token punctuation">[</span>key<span class="token punctuation">]</span><span class="token operator">/</span>numItems   <span class="token comment">#compute support</span>
        <span class="token keyword">if</span> support <span class="token operator">&gt;=</span> minSupport<span class="token punctuation">:</span>
            retList<span class="token punctuation">.</span>insert<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span>key<span class="token punctuation">)</span>
        supportData<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">=</span> support
    <span class="token keyword">return</span> retList<span class="token punctuation">,</span> supportData

<span class="token comment">#total apriori</span>
<span class="token keyword">def</span> <span class="token function">aprioriGen</span><span class="token punctuation">(</span>Lk<span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#组合，向上合并</span>
    <span class="token comment">#creates Ck 参数：频繁项集列表 Lk 与项集元素个数 k</span>
    retList <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    lenLk <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>Lk<span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>lenLk<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> lenLk<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#两两组合遍历</span>
            L1 <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>Lk<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span>k<span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">;</span> L2 <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>Lk<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span>k<span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span>
            L1<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> L2<span class="token punctuation">.</span>sort<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> L1<span class="token operator">==</span>L2<span class="token punctuation">:</span> <span class="token comment">#若两个集合的前k-2个项相同时,则将两个集合合并</span>
                retList<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Lk<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">|</span> Lk<span class="token punctuation">[</span>j<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment">#set union</span>
    <span class="token keyword">return</span> retList

<span class="token comment">#apriori</span>
<span class="token keyword">def</span> <span class="token function">apriori</span><span class="token punctuation">(</span>dataSet<span class="token punctuation">,</span> minSupport <span class="token operator">=</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    C1 <span class="token operator">=</span> createC1<span class="token punctuation">(</span>dataSet<span class="token punctuation">)</span>
    D <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">map</span><span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">,</span> dataSet<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#python3</span>
    L1<span class="token punctuation">,</span> supportData <span class="token operator">=</span> scanD<span class="token punctuation">(</span>D<span class="token punctuation">,</span> C1<span class="token punctuation">,</span> minSupport<span class="token punctuation">)</span><span class="token comment">#单项最小支持度判断 0.5，生成L1</span>
    L <span class="token operator">=</span> <span class="token punctuation">[</span>L1<span class="token punctuation">]</span>
    k <span class="token operator">=</span> <span class="token number">2</span>
    <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>L<span class="token punctuation">[</span>k<span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment">#创建包含更大项集的更大列表,直到下一个大的项集为空</span>
        Ck <span class="token operator">=</span> aprioriGen<span class="token punctuation">(</span>L<span class="token punctuation">[</span>k<span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token comment">#Ck</span>
        Lk<span class="token punctuation">,</span> supK <span class="token operator">=</span> scanD<span class="token punctuation">(</span>D<span class="token punctuation">,</span> Ck<span class="token punctuation">,</span> minSupport<span class="token punctuation">)</span><span class="token comment">#get Lk</span>
        supportData<span class="token punctuation">.</span>update<span class="token punctuation">(</span>supK<span class="token punctuation">)</span>
        L<span class="token punctuation">.</span>append<span class="token punctuation">(</span>Lk<span class="token punctuation">)</span>
        k <span class="token operator">+=</span> <span class="token number">1</span> <span class="token comment">#继续向上合并 生成项集个数更多的</span>
    <span class="token keyword">return</span> L<span class="token punctuation">,</span> supportData

<span class="token comment">#生成关联规则</span>
<span class="token comment"># 创建关联规则</span>
<span class="token keyword">def</span> <span class="token function">generateRules</span><span class="token punctuation">(</span>fileName<span class="token punctuation">,</span> L<span class="token punctuation">,</span> supportData<span class="token punctuation">,</span> minConf<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># supportData是从scanD获得的字段</span>
    bigRuleList <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>L<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 只获得又有2个或以上的项目的集合</span>
        <span class="token keyword">for</span> freqSet <span class="token keyword">in</span> L<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">:</span>
            H1 <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token builtin">frozenset</span><span class="token punctuation">(</span><span class="token punctuation">[</span>item<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">for</span> item <span class="token keyword">in</span> freqSet<span class="token punctuation">]</span>
            <span class="token keyword">if</span> <span class="token punctuation">(</span>i <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                rulesFromConseq<span class="token punctuation">(</span>fileName<span class="token punctuation">,</span> freqSet<span class="token punctuation">,</span> H1<span class="token punctuation">,</span> supportData<span class="token punctuation">,</span> bigRuleList<span class="token punctuation">,</span> minConf<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                calcConf<span class="token punctuation">(</span>fileName<span class="token punctuation">,</span> freqSet<span class="token punctuation">,</span> H1<span class="token punctuation">,</span> supportData<span class="token punctuation">,</span> bigRuleList<span class="token punctuation">,</span> minConf<span class="token punctuation">)</span>
    <span class="token keyword">return</span> bigRuleList

<span class="token comment"># 实例数、支持度、置信度和提升度评估</span>
<span class="token keyword">def</span> <span class="token function">calcConf</span><span class="token punctuation">(</span>fileName<span class="token punctuation">,</span> freqSet<span class="token punctuation">,</span> H<span class="token punctuation">,</span> supportData<span class="token punctuation">,</span> brl<span class="token punctuation">,</span> minConf<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    prunedH <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    D <span class="token operator">=</span> fileName
    numItems <span class="token operator">=</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>D<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> conseq <span class="token keyword">in</span> H<span class="token punctuation">:</span>
        conf <span class="token operator">=</span> supportData<span class="token punctuation">[</span>freqSet<span class="token punctuation">]</span> <span class="token operator">/</span> supportData<span class="token punctuation">[</span>freqSet <span class="token operator">-</span> conseq<span class="token punctuation">]</span>  <span class="token comment"># 计算置信度</span>
        <span class="token keyword">if</span> conf <span class="token operator">&gt;=</span> minConf<span class="token punctuation">:</span>
            instances <span class="token operator">=</span> numItems <span class="token operator">*</span> supportData<span class="token punctuation">[</span>freqSet<span class="token punctuation">]</span>  <span class="token comment"># 计算实例数</span>
            liftvalue <span class="token operator">=</span> conf <span class="token operator">/</span> supportData<span class="token punctuation">[</span>conseq<span class="token punctuation">]</span>  <span class="token comment"># 计算提升度</span>
            brl<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">(</span>freqSet <span class="token operator">-</span> conseq<span class="token punctuation">,</span> conseq<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">(</span>instances<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">round</span><span class="token punctuation">(</span>supportData<span class="token punctuation">[</span>freqSet<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">round</span><span class="token punctuation">(</span>conf<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                        <span class="token builtin">round</span><span class="token punctuation">(</span>liftvalue<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 支持度已经在SCAND中计算得出</span>
            prunedH<span class="token punctuation">.</span>append<span class="token punctuation">(</span>conseq<span class="token punctuation">)</span>
    <span class="token keyword">return</span> prunedH

<span class="token comment"># 生成候选规则集</span>
<span class="token keyword">def</span> <span class="token function">rulesFromConseq</span><span class="token punctuation">(</span>fileName<span class="token punctuation">,</span> freqSet<span class="token punctuation">,</span> H<span class="token punctuation">,</span> supportData<span class="token punctuation">,</span> brl<span class="token punctuation">,</span> minConf<span class="token operator">=</span><span class="token number">0.7</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    m <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>H<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>freqSet<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token punctuation">(</span>m <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        Hmp1 <span class="token operator">=</span> aprioriGen<span class="token punctuation">(</span>H<span class="token punctuation">,</span> m <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span>
        Hmp1 <span class="token operator">=</span> calcConf<span class="token punctuation">(</span>fileName<span class="token punctuation">,</span> freqSet<span class="token punctuation">,</span> Hmp1<span class="token punctuation">,</span> supportData<span class="token punctuation">,</span> brl<span class="token punctuation">,</span> minConf<span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>Hmp1<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            rulesFromConseq<span class="token punctuation">(</span>fileName<span class="token punctuation">,</span> freqSet<span class="token punctuation">,</span> Hmp1<span class="token punctuation">,</span> supportData<span class="token punctuation">,</span> brl<span class="token punctuation">,</span> minConf<span class="token punctuation">)</span>

</code></pre>
<p>apriori算法实现:</p>
<pre><code class="prism language-python"><span class="token keyword">import</span> apriori
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> graphviz <span class="token keyword">import</span> Digraph

<span class="token comment"># 设置最小支持度阈值</span>
minS <span class="token operator">=</span> <span class="token number">0.5</span>
<span class="token comment"># 设置最小置信度阈值</span>
minC <span class="token operator">=</span> <span class="token number">0.7</span>

data <span class="token operator">=</span> apriori<span class="token punctuation">.</span>loadDataSet<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token comment"># 计算符合最小支持度的规则</span>
L<span class="token punctuation">,</span> suppdata <span class="token operator">=</span> apriori<span class="token punctuation">.</span>apriori<span class="token punctuation">(</span>data<span class="token punctuation">,</span> minSupport<span class="token operator">=</span>minS<span class="token punctuation">)</span>

<span class="token comment"># 计算满足最小置信度规则</span>
rules <span class="token operator">=</span> apriori<span class="token punctuation">.</span>generateRules<span class="token punctuation">(</span>data<span class="token punctuation">,</span> L<span class="token punctuation">,</span> suppdata<span class="token punctuation">,</span> minConf<span class="token operator">=</span>minC<span class="token punctuation">)</span>

<span class="token comment">### 关联结果评估###</span>
model_summary <span class="token operator">=</span> <span class="token string">'data record: {1} \nassociation rules count: {0}'</span>  <span class="token comment"># 展示数据集记录数和满足阈值定义的规则数量</span>
<span class="token keyword">print</span> <span class="token punctuation">(</span>model_summary<span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>rules<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 使用str.format做格式化输出</span>
df <span class="token operator">=</span>  pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>rules<span class="token punctuation">,</span>  columns<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'item1'</span><span class="token punctuation">,</span>  <span class="token string">'itme2'</span><span class="token punctuation">,</span>  <span class="token string">'instance'</span><span class="token punctuation">,</span>  <span class="token string">'support'</span><span class="token punctuation">,</span>
    <span class="token string">'confidence'</span><span class="token punctuation">,</span> <span class="token string">'lift'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 创建频繁规则数据框</span>
df_lift <span class="token operator">=</span> df<span class="token punctuation">[</span>df<span class="token punctuation">[</span><span class="token string">'lift'</span><span class="token punctuation">]</span> <span class="token operator">&gt;</span> <span class="token number">1.0</span><span class="token punctuation">]</span>  <span class="token comment"># 只选择提升度&gt;1的规则</span>
df_lift<span class="token punctuation">.</span>sort_values<span class="token punctuation">(</span><span class="token string">'instance'</span><span class="token punctuation">,</span> ascending<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181201143702923.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RvbnlkejA1MjM=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<pre><code class="prism language-python"><span class="token comment"># 关联结果图形展示</span>
dot <span class="token operator">=</span> Digraph<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 创建有向图</span>
graph_data <span class="token operator">=</span> df_lift<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'item1'</span><span class="token punctuation">,</span> <span class="token string">'itme2'</span><span class="token punctuation">,</span> <span class="token string">'instance'</span><span class="token punctuation">]</span><span class="token punctuation">]</span>   <span class="token comment"># 切分画图用的前项、后项和实例数数据</span>
<span class="token keyword">for</span> each_data <span class="token keyword">in</span> graph_data<span class="token punctuation">.</span>values<span class="token punctuation">:</span>  <span class="token comment"># 循环读出每条规则</span>
    node1<span class="token punctuation">,</span> node2<span class="token punctuation">,</span> weight <span class="token operator">=</span> each_data  <span class="token comment"># 分割每条数据画图用的前项、后项和实例数</span>
    node1 <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">(</span>node1<span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">'frozenset({})'</span><span class="token punctuation">)</span>  <span class="token comment"># 转化为字符串</span>
    node2 <span class="token operator">=</span> <span class="token builtin">str</span><span class="token punctuation">(</span>node2<span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">'frozenset({})'</span><span class="token punctuation">)</span>  <span class="token comment"># 转化为字符串</span>
    label <span class="token operator">=</span> <span class="token string">'%s'</span> <span class="token operator">%</span> weight  <span class="token comment"># 创建一个标签用于展示实例数</span>
    dot<span class="token punctuation">.</span>node<span class="token punctuation">(</span>node1<span class="token punctuation">,</span> node1<span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token string">'record'</span><span class="token punctuation">)</span>  <span class="token comment"># 增加节点（规则中的前项）</span>
    dot<span class="token punctuation">.</span>edge<span class="token punctuation">(</span>node1<span class="token punctuation">,</span> node2<span class="token punctuation">,</span> label<span class="token operator">=</span>label<span class="token punctuation">,</span> constraint<span class="token operator">=</span><span class="token string">'true'</span><span class="token punctuation">)</span>  <span class="token comment"># 增加有向边</span>
dot<span class="token punctuation">.</span>render<span class="token punctuation">(</span><span class="token string">'apriori'</span><span class="token punctuation">,</span> view<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># 保存规则为pdf文件</span>
</code></pre>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181201143745524.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RvbnlkejA1MjM=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>使用pyfpgrowth模块实现FP-Growth：</p>
<pre><code class="prism language-python"><span class="token keyword">import</span> pyfpgrowth
<span class="token comment"># 数据</span>
data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
<span class="token comment"># 设置支持度和置信度</span>
minS <span class="token operator">=</span> <span class="token number">0.2</span>
minC <span class="token operator">=</span> <span class="token number">0.7</span>
<span class="token comment"># 计算支持值</span>
support <span class="token operator">=</span> minS<span class="token operator">*</span><span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span>
<span class="token comment"># 获取符合支持度规则数据</span>
patterns <span class="token operator">=</span> pyfpgrowth<span class="token punctuation">.</span>find_frequent_patterns<span class="token punctuation">(</span>data<span class="token punctuation">,</span> support<span class="token punctuation">)</span>
<span class="token comment"># 获取符合置信度规则数据</span>
rules <span class="token operator">=</span> pyfpgrowth<span class="token punctuation">.</span>generate_association_rules<span class="token punctuation">(</span>patterns<span class="token punctuation">,</span> minC<span class="token punctuation">)</span>

rules
</code></pre>
<p>结果如下：</p>
<p>{(5,): ((1, 2), 1.0),<br /> (1, 5): ((2,), 1.0),<br /> (2, 5): ((1,), 1.0),<br /> (4,): ((2,), 1.0)}</p>
<h2><a id="_238"></a>补充</h2>
<p>除了打包组合的思维方式之外，还可以这样考虑应用：既然用户具有较强的发生关联事件关系（例如购买、查看等）的可能性，那么可以基于用户的这种习惯，将前后项内容故意分离开，利用用户主动查找的时机来产生更多价值或完成特定转化目标。<br /> 例如：用户经常一起购买啤酒和尿布，我们可以分别将啤酒和尿布陈列在展柜的两端（或者隔开一段距离），然后在用户购买啤酒又去购买尿布的途中，也许会发现别的商品进而产生兴趣，从而实现更多的销售。但是需要注意这种关联需求的效用能否支撑这种搜索过程，则刚需最好，强有效规则其次，不可过多降低用户体验。</p>
<h2><a id="_243"></a>序列模式的关联规则：</h2>
<p>序列模式相较于普通关联模式最大的区别是不同的事件之间具有明显的时间区隔，以及先后的序列发生关系，能得到类似于“完成某个事件之后会在特定的时间周期内完成其他事件”的结论，例如购买了冰箱的客户会在3个月内购买洗衣机的结论。这是一种预测性分析的模式，能够将事件发生的时间和对象提取出来，使用与基于时间序列的数据化运营需求。<br /> 应用场景：</p>
<ul>
<li>基于用户上一次购买时间和商品信息，推断用户下一次购物时间和购买的商品，如果用户脱离周期过久需要实施挽留措施</li>
<li>基于用户上一次浏览页面的时间和页面信息，推断用户下一次可能浏览的页面</li>
<li>通过上衣此关键字的搜索预测下一次最可能搜索的关键字</li>
</ul>
<p>能实现序列模式的关联算法包括：</p>
<ul>
<li>AprioriAll：基于哈希树的序列关联算法，它与Apriori算法的执行过程是一样的，不同点在于候选集的产生，需要区分最后两个元素的前后顺序。</li>
<li>AprioriSome：该算法是对AprioriAll算法的改进。</li>
<li>CARMA（Continuous Association Rule Mining Algorithm）:CARMA是一种比较新的关联规则算法，能够处理在线连续交易流数据。</li>
<li>GSP（Generalized Sequential Patterns）：基于水平存储结构和哈希树遍历操作的序列关联算法，它具有类似于Apriori算法的实现步骤，主要区别在于产生候选序列模式。</li>
<li>SPADE（Sequential PAttern Discovery using Equivalence classes）：基于垂直存储结构和格理论连接操作的序列关联算法，它是一种改进的GSP算法。</li>
<li>FreeSpan：频繁模式投影的序列关联算法，利用频繁项递归地将序列数据库投影到更小的投影数据库集中，在每个投影数据库中生成子序列片断，是一种分治思想的算法。</li>
<li>PrefixSpan（Prefix-Projected Pattern Growth）：基于前缀树的序列关联算法，从Free-Span中推导演化而来</li>
</ul>
<p><strong>参考：<br /> 《python数据分析与数据化运营》 宋天龙</strong></p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-526ced5128.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>网络安全概述</title>
		<link>https://uzzz.org/article/865.html</link>
				<pubDate>Wed, 28 Nov 2018 11:03:23 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[大数据]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/865.html</guid>
				<description><![CDATA[网络安全概述 网络安全概述主要是讲解一些在网络层级容易发生的安全问题，一些常见的攻击手段及防御措施。主要的关注点在于网络协议中的一些漏洞和问题。 网络安全概述 暗网 暗网介绍 暗网与深网 ARP欺骗 ARP协议 1. ARP缓存表 2. ARP欺骗原理 3. 常用ARP攻击工具 DNS欺骗 DNS协议 DNS欺骗原理 DNS欺骗防范 中间人攻击 中间人攻击原理 VPN中的中间人攻击 SSL中的中间人攻击 暗网 暗网介绍 暗网（又称深网，不可见网）是指那些存储在网络数据库里，不能通过超链接访问而需要通过动态网页技术访问的资源集合，不属于那些可以被标准搜索引擎索引的表面网络。可以说，我们正常访问的网站只属于整个网络中的一小部分，只占到了整个网络的4%-20%，更多的部分由暗网组成。 暗网与深网 网络其实一共有三层： 表层网络： 表层网络就是人们所熟知的可见网络，人们平时访问的网站都属于这类，通过链接抓取技术就可以轻松访问这些网站。 深网： 表层网络之外的所有网络都称为深网，搜索引擎无法对其进行抓取。它们并没有完全隐藏起来，只是普通搜索引擎无法发现它的踪迹，不过使用一些工具也可以访问这些网络。 暗网： 暗网是深网的一部分，但被人为隐藏起来。如果不是精通技术，很难进入这个网络，他的域名数量甚至是表层网络的400-500倍。 ARP欺骗 ARP协议 ARP（Address Resolution Protocol）称为地址解析协议，就是在主机发出数据帧之前，先将目标IP地址转化成目的MAC地址的过程，它被收录在RFC826标准中。 1. ARP缓存表 如果每次使用IP地址访问目标主机时都需要使用ARP协议来获得MAC地址，会造成局域网内流量负载增加，传输效率降低。为了避免这种情况的发生，ARP协议规定每台主机都有一个本地的ARP高速缓存表，用来存放IP地址和MAC地址的映射，如果表中有目的主机的MAC地址，则不会再向局域网内广播请求，直接使用本地的IP-MAC映射。如果一段时间内不与某IP通讯，系统会删除相关的映射条目。 2. ARP欺骗原理 在以太局域网内，数据分组传输依靠的是MAC地址，而IP地址与MAC地址的映射依靠ARP表，每台主机都有一个ARP缓存表。正常情况下主机的ARP缓存表可以保证数据正确传输到目的主机。但是在ARP缓存表的实现机制中有一个漏洞，当主机收到一个ARP应答报文后，它不会去验证自己是否发送过这个ARP请求，而是直接用应答报文中的数据更新ARP缓存表。利用这一漏洞可以通过发送虚假的ARP应答报文，达到替换靶机ARP缓存中的映射关系。 3. 常用ARP攻击工具 Cain &#38; Abel： 该组件是一个针对Microsoft操作系统的免费口令恢复工具，号称穷人使用的L0phtcrack。它功能十分强大，可以网络嗅探，网络欺骗，破解加密口令，解码被打乱的口令，显示缓存口令和分析路由协议。 BetterCap： 该组件是一个功能强大，模块化，轻便的MiTM框架，可以用来对网络开展各种类型的中间人攻击。他也可以实时操作HTTP和HTTPS流量，以及其他更多功能。 Ettercap： Ettercap是一个全面的套件，用于中间人攻击。它具有嗅探活动连接，内容过滤和许多其他的技巧。同时支持多种协议的主动和被动消除包括许多网络和主机分析的功能。 Netfuke： 该组件是一款Windows下常使用的ARP攻击工具。 DNS欺骗 DNS协议 DNS（Domain Name System）域名系统是域名与IP地址相互映射的一个分布式数据库，能够使用户更加方便的访问互联网，省去了记住要访问的主机IP地址的麻烦，可以直接使用更加语义化的域名来访问。通过主机名（域名）最终得到对应IP地址的过程叫做域名解析。 DNS欺骗原理 DNS欺骗攻击就是攻击者冒充域名服务器，吧用户查询的域名地址更换成攻击者的IP地址，然后攻击者将自己的主页取代用户的主页，这样访问用户主页的时候只会显示攻击者的主页，进而可以诱导用户点击或下载恶意程序。 DNS欺骗的实现利用了DNS设计时的一个安全缺陷。在一个局域网内，攻击者首先使用ARP欺骗，使目的主机的所有网络流量都通过攻击者的主机，之后通过嗅探目标主机发出的DNS请求分组，分析数据分组的ID和端口号，向目标主机发送构造好的DNS返回分组，目标主机确认分组ID和端口号都正确后，将返回分组中的域名和对应的IP地址保存进DSN缓存，而后当真正DNS应答报文返回时则被丢弃。 DNS欺骗防范 在DNS欺骗之前一般都需要使用ARP攻击来配合，因此可以首先做好对ARP欺骗的防御工作，例如设置静态的ARP映射，安装ARP防火墙 等。 使用代理服务器进行网络通信，本地主机与代理服务器的全部流量都可以加密，包括DNS信息 尽量访问带有https标识的站点 使用DNSSrypt等工具，DNSCrypt时OpenDNS发布的加密DNS工具，可以加密DNS流量，阻止最常见的DNS攻击 中间人攻击 中间人攻击原理 中间人攻击（Man-In-the-MiddleAttack）简称MITM攻击。是一种由来已久的网络入侵手段。所谓的中间人攻击，就是通过拦截正常的网络通信数据，进行数据的篡改和嗅探，而通信双方毫不知情。 VPN中的中间人攻击 对于采用PPTP方式的VPN，可以实现通过中间人攻击来进行渗透，只需要截获VPN客户端登录VPN服务器/设备的数据报文后，对VPN报文的密码进行破解就可以完成攻击。 SSL中的中间人攻击 攻击者通过监听被转发到本地的流量，从中得知目标主机要连接的服务器地址，然后分别与目的主机和真正的服务器建立SSL连接，攻击者在这两个连接之间转发数据，这样便可以得到受害者和服务器交互的数据]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-light">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<h1><a id="_0"></a>网络安全概述</h1>
<p>网络安全概述主要是讲解一些在网络层级容易发生的安全问题，一些常见的攻击手段及防御措施。主要的关注点在于网络协议中的一些漏洞和问题。</p>
<p>  <!-- TOC --> </p>
<ul>
<li><a href="#%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E6%A6%82%E8%BF%B0" rel="nofollow" data-token="bf355906041f863ae376e93c877a02d2">网络安全概述</a>
<ul>
<li><a href="#%E6%9A%97%E7%BD%91" rel="nofollow" data-token="3b5c6ea770afd2c9f4608fb69babdee2">暗网</a>
<ul>
<li><a href="#%E6%9A%97%E7%BD%91%E4%BB%8B%E7%BB%8D" rel="nofollow" data-token="9c1e46bcd19c58e269ca011f434e7afe">暗网介绍</a></li>
<li><a href="#%E6%9A%97%E7%BD%91%E4%B8%8E%E6%B7%B1%E7%BD%91" rel="nofollow" data-token="6bde74bbd5d8f95064c3c822bfa8a5b8">暗网与深网</a></li>
</ul>
</li>
<li><a href="#arp%E6%AC%BA%E9%AA%97" rel="nofollow" data-token="3f51457811e0f27063852cefa3f08a39">ARP欺骗</a>
<ul>
<li><a href="#arp%E5%8D%8F%E8%AE%AE" rel="nofollow" data-token="d0536d5b1d947673b9753424a363c67b">ARP协议</a>
<ul>
<li><a href="#1-arp%E7%BC%93%E5%AD%98%E8%A1%A8" rel="nofollow" data-token="867e7ca775ff1469a5925b90732aa6fe">1. ARP缓存表</a></li>
<li><a href="#2-arp%E6%AC%BA%E9%AA%97%E5%8E%9F%E7%90%86" rel="nofollow" data-token="9d3b0affc967527a26b5f663ace225cd">2. ARP欺骗原理</a></li>
<li><a href="#3-%E5%B8%B8%E7%94%A8arp%E6%94%BB%E5%87%BB%E5%B7%A5%E5%85%B7" rel="nofollow" data-token="81fddf33b699077c00487dec28401862">3. 常用ARP攻击工具</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#dns%E6%AC%BA%E9%AA%97" rel="nofollow" data-token="02a8854a290c7a3414d628fdfb1a8c56">DNS欺骗</a>
<ul>
<li><a href="#dns%E5%8D%8F%E8%AE%AE" rel="nofollow" data-token="74e586ad10730453c15cbe341bdb28d4">DNS协议</a>
<ul>
<li><a href="#dns%E6%AC%BA%E9%AA%97%E5%8E%9F%E7%90%86" rel="nofollow" data-token="b17437130ba8ea3e088621af976cd404">DNS欺骗原理</a></li>
<li><a href="#dns%E6%AC%BA%E9%AA%97%E9%98%B2%E8%8C%83" rel="nofollow" data-token="0764d05cfd53ca49867e825af0b41432">DNS欺骗防范</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E4%B8%AD%E9%97%B4%E4%BA%BA%E6%94%BB%E5%87%BB" rel="nofollow" data-token="fcc3ebff3f52234ec0027cf78dfea807">中间人攻击</a>
<ul>
<li><a href="#%E4%B8%AD%E9%97%B4%E4%BA%BA%E6%94%BB%E5%87%BB%E5%8E%9F%E7%90%86" rel="nofollow" data-token="55aafd327b034e7c03f8db8e6538e201">中间人攻击原理</a></li>
<li><a href="#vpn%E4%B8%AD%E7%9A%84%E4%B8%AD%E9%97%B4%E4%BA%BA%E6%94%BB%E5%87%BB" rel="nofollow" data-token="b40c22296e6005344a04b16b0ee5cd19">VPN中的中间人攻击</a></li>
<li><a href="#ssl%E4%B8%AD%E7%9A%84%E4%B8%AD%E9%97%B4%E4%BA%BA%E6%94%BB%E5%87%BB" rel="nofollow" data-token="6f237d512d218517d4dda65182408ff4">SSL中的中间人攻击</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>  <!-- /TOC --> </p>
<h2><a id="_26"></a>暗网</h2>
<h3><a id="_28"></a>暗网介绍</h3>
<p>暗网（又称深网，不可见网）是指那些存储在网络数据库里，不能通过超链接访问而需要通过动态网页技术访问的资源集合，不属于那些可以被标准搜索引擎索引的表面网络。可以说，我们正常访问的网站只属于整个网络中的一小部分，只占到了整个网络的4%-20%，更多的部分由暗网组成。</p>
<h3><a id="_32"></a>暗网与深网</h3>
<p>网络其实一共有三层：</p>
<ol>
<li>表层网络：<br /> 表层网络就是人们所熟知的可见网络，人们平时访问的网站都属于这类，通过链接抓取技术就可以轻松访问这些网站。</li>
<li>深网：<br /> 表层网络之外的所有网络都称为深网，搜索引擎无法对其进行抓取。它们并没有完全隐藏起来，只是普通搜索引擎无法发现它的踪迹，不过使用一些工具也可以访问这些网络。</li>
<li>暗网：<br /> 暗网是深网的一部分，但被人为隐藏起来。如果不是精通技术，很难进入这个网络，他的域名数量甚至是表层网络的400-500倍。</li>
</ol>
<h2><a id="ARP_43"></a>ARP欺骗</h2>
<h3><a id="ARP_45"></a>ARP协议</h3>
<p>ARP（Address Resolution Protocol）称为地址解析协议，就是在主机发出数据帧之前，先将目标IP地址转化成目的MAC地址的过程，它被收录在RFC826标准中。</p>
<h4><a id="1_ARP_49"></a>1. ARP缓存表</h4>
<p>如果每次使用IP地址访问目标主机时都需要使用ARP协议来获得MAC地址，会造成局域网内流量负载增加，传输效率降低。为了避免这种情况的发生，ARP协议规定每台主机都有一个本地的ARP高速缓存表，用来存放IP地址和MAC地址的映射，如果表中有目的主机的MAC地址，则不会再向局域网内广播请求，直接使用本地的IP-MAC映射。如果一段时间内不与某IP通讯，系统会删除相关的映射条目。</p>
<h4><a id="2_ARP_53"></a>2. ARP欺骗原理</h4>
<p>在以太局域网内，数据分组传输依靠的是MAC地址，而IP地址与MAC地址的映射依靠ARP表，每台主机都有一个ARP缓存表。正常情况下主机的ARP缓存表可以保证数据正确传输到目的主机。但是在ARP缓存表的实现机制中有一个漏洞，当主机收到一个ARP应答报文后，它不会去验证自己是否发送过这个ARP请求，而是直接用应答报文中的数据更新ARP缓存表。利用这一漏洞可以通过发送虚假的ARP应答报文，达到替换靶机ARP缓存中的映射关系。</p>
<h4><a id="3_ARP_57"></a>3. 常用ARP攻击工具</h4>
<ol>
<li>
<p><strong>Cain &amp; Abel：</strong> 该组件是一个针对Microsoft操作系统的免费口令恢复工具，号称穷人使用的L0phtcrack。它功能十分强大，可以网络嗅探，网络欺骗，破解加密口令，解码被打乱的口令，显示缓存口令和分析路由协议。</p>
</li>
<li>
<p><strong>BetterCap：</strong> 该组件是一个功能强大，模块化，轻便的MiTM框架，可以用来对网络开展各种类型的中间人攻击。他也可以实时操作HTTP和HTTPS流量，以及其他更多功能。</p>
</li>
<li>
<p><strong>Ettercap：</strong> Ettercap是一个全面的套件，用于中间人攻击。它具有嗅探活动连接，内容过滤和许多其他的技巧。同时支持多种协议的主动和被动消除包括许多网络和主机分析的功能。</p>
</li>
<li>
<p><strong>Netfuke：</strong> 该组件是一款Windows下常使用的ARP攻击工具。</p>
</li>
</ol>
<h2><a id="DNS_67"></a>DNS欺骗</h2>
<h3><a id="DNS_69"></a>DNS协议</h3>
<p>DNS（Domain Name System）域名系统是域名与IP地址相互映射的一个分布式数据库，能够使用户更加方便的访问互联网，省去了记住要访问的主机IP地址的麻烦，可以直接使用更加语义化的域名来访问。通过主机名（域名）最终得到对应IP地址的过程叫做域名解析。</p>
<h4><a id="DNS_73"></a>DNS欺骗原理</h4>
<p>DNS欺骗攻击就是攻击者冒充域名服务器，吧用户查询的域名地址更换成攻击者的IP地址，然后攻击者将自己的主页取代用户的主页，这样访问用户主页的时候只会显示攻击者的主页，进而可以诱导用户点击或下载恶意程序。</p>
<p>DNS欺骗的实现利用了DNS设计时的一个安全缺陷。在一个局域网内，攻击者首先使用ARP欺骗，使目的主机的所有网络流量都通过攻击者的主机，之后通过嗅探目标主机发出的DNS请求分组，分析数据分组的ID和端口号，向目标主机发送构造好的DNS返回分组，目标主机确认分组ID和端口号都正确后，将返回分组中的域名和对应的IP地址保存进DSN缓存，而后当真正DNS应答报文返回时则被丢弃。</p>
<h4><a id="DNS_79"></a>DNS欺骗防范</h4>
<ol>
<li>在DNS欺骗之前一般都需要使用ARP攻击来配合，因此可以首先做好对ARP欺骗的防御工作，例如设置静态的ARP映射，安装ARP防火墙<br /> 等。</li>
<li>使用代理服务器进行网络通信，本地主机与代理服务器的全部流量都可以加密，包括DNS信息</li>
<li>尽量访问带有https标识的站点</li>
<li>使用DNSSrypt等工具，DNSCrypt时OpenDNS发布的加密DNS工具，可以加密DNS流量，阻止最常见的DNS攻击</li>
</ol>
<h2><a id="_87"></a>中间人攻击</h2>
<h3><a id="_89"></a>中间人攻击原理</h3>
<p>中间人攻击（Man-In-the-MiddleAttack）简称MITM攻击。是一种由来已久的网络入侵手段。所谓的中间人攻击，就是通过拦截正常的网络通信数据，进行数据的篡改和嗅探，而通信双方毫不知情。</p>
<h3><a id="VPN_93"></a>VPN中的中间人攻击</h3>
<p>对于采用PPTP方式的VPN，可以实现通过中间人攻击来进行渗透，只需要截获VPN客户端登录VPN服务器/设备的数据报文后，对VPN报文的密码进行破解就可以完成攻击。</p>
<h3><a id="SSL_97"></a>SSL中的中间人攻击</h3>
<p>攻击者通过监听被转发到本地的流量，从中得知目标主机要连接的服务器地址，然后分别与目的主机和真正的服务器建立SSL连接，攻击者在这两个连接之间转发数据，这样便可以得到受害者和服务器交互的数据</p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>深度 &#124; IBM长文解读人工智能、机器学习和认知计算</title>
		<link>https://uzzz.org/article/2569.html</link>
				<pubDate>Fri, 26 Oct 2018 02:33:56 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[人工智能]]></category>
		<category><![CDATA[区块链]]></category>
		<category><![CDATA[大数据]]></category>
		<category><![CDATA[程序员]]></category>
		<category><![CDATA[资讯]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2569.html</guid>
				<description><![CDATA[人工智能的发展曾经经历过几次起起伏伏，近来在深度学习技术的推动下又迎来了一波新的前所未有的高潮。近日，IBM 官网发表了一篇概述文章，对人工智能技术的发展过程进行了简单梳理，同时还图文并茂地介绍了感知器、聚类算法、基于规则的系统、机器学习、深度学习、神经网络等技术的概念和原理。 &#160; 人类对如何创造智能机器的思考从来没有中断过。期间，人工智能的发展起起伏伏，有成功，也有失败，以及其中暗藏的潜力。今天，有太多的新闻报道是关于机器学习算法的应用问题，从癌症检查预测到图像理解、自然语言处理，人工智能正在赋能并改变着这个世界。 &#160; 现代人工智能的历史具备成为一部伟大戏剧的所有元素。在最开始的 1950 年代，人工智能的发展紧紧围绕着思考机器和焦点人物比如艾伦·图灵、冯·诺伊曼，迎来了其第一次春天。经过数十年的繁荣与衰败，以及难以置信的高期望，人工智能及其先驱们再次携手来到一个新境界。现在，人工智能正展现着其真正的潜力，深度学习、认知计算等新技术不断涌现，且不乏应用指向。 &#160; 本文探讨了人工智能及其子领域的一些重要方面。下面就先从人工智能发展的时间线开始，并逐个剖析其中的所有元素。 &#160; &#160; 现代人工智能的时间线 &#160; 1950 年代初期，人工智能聚焦在所谓的强人工智能，希望机器可以像人一样完成任何智力任务。强人工智能的发展止步不前，导致了弱人工智能的出现，即把人工智能技术应用于更窄领域的问题。1980 年代之前，人工智能的研究一直被这两种范式分割着，两营相对。但是，1980 年左右，机器学习开始成为主流，它的目的是让计算机具备学习和构建模型的能力，从而它们可在特定领域做出预测等行为。很多初学者，对大数据的概念都是模糊不清的，大数据是什么，能做什么，学的时候，该按照什么线路去学习，学完往哪方面发展，想深入了解，想学习的同学欢迎加入大数据学习扣群：805127855，有大量干货（零基础以及进阶的经典实战）分享给大家，并且有清华大学毕业的资深大数据讲师给大家免费授课，给大家分享目前国内最完整的大数据高端实战实用学习流程体系 &#160; &#160; 图 1：现代人工智能发展的时间线 &#160; 在人工智能和机器学习研究的基础之上，深度学习在 2000 年左右应运而生。计算机科学家在多层神经网络之中使用了新的拓扑学和学习方法。最终，神经网络的进化成功解决了多个领域的棘手问题。 &#160; 在过去的十年中，认知计算（Cognitive computing）也出现了，其目标是打造可以学习并与人类自然交互的系统。通过成功地击败 Jeopardy 游戏的世界级选手，IBM Watson 证明了认知计算的价值。 &#160; 在本文中，我将逐一探索上述的所有领域，并对一些关键算法作出解释。 基础性人工智能 &#160; 1950 年之前的研究提出了大脑是由电脉冲网络组成的想法，正是脉冲之间的交互产生了人类思想与意识。艾伦·图灵表明一切计算皆是数字，那么，打造一台能够模拟人脑的机器也就并非遥不可及。 &#160; 上文说过，早期的研究很多是强人工智能，但是也提出了一些基本概念，被机器学习和深度学习沿用至今。 &#160; 图 2：1950 &#8211; 1980 年间人工智能方法的时间线 &#160; &#160; 人工智能搜索引擎 &#160; 人工智能中的很多问题可以通过强力搜索（brute-force search）得到解决。然而，考虑到中等问题的搜索空间，基本搜索很快就受影响。人工智能搜索的最早期例子之一是跳棋程序的开发。亚瑟·塞缪尔（Arthur Samuel）在 IBM 701 电子数据处理机器上打造了第一款跳棋程序，实现了对搜索树（alpha-beta 剪枝）的优化；这个程序也记录并奖励具体行动，允许应用学习每一个玩过的游戏（这是首个自我学习的程序）。为了提升程序的学习率，塞缪尔将其编程为自我游戏，以提升其游戏和学习的能力。 &#160; 尽管你可以成功地把搜索应用到很多简单问题上，但是当选择的数量增加时，这一方法很快就会失效。以简单的一字棋游戏为例，游戏一开始，有 9 步可能的走棋，每 1 个走棋有 8 个可能的相反走棋，依次类推。一字棋的完整走棋树包含 362,880 个节点。如果你继续将这一想法扩展到国际象棋或者围棋，很快你就会发展搜索的劣势。 &#160; &#160; 感知器 &#160; 感知器是单层神经网络的一个早期监督学习算法。给定一个输入特征向量，感知器可对输入进行具体分类。通过使用训练集，网络的权重和偏差可为线性分类而更新。感知器的首次实现是 IBM 704，接着在自定义硬件上用于图像识别。 &#160; 图 3：感知器与线性分类 &#160; 作为一个线性分类器，感知器有能力解决线性分离问题。感知器局限性的典型实例是它无法学习专属的 OR （XOR） 函数。多层感知器解决了这一问题，并为更复杂的算法、网络拓扑学、深度学习奠定了基础。 &#160; &#160; 聚类算法 &#160; 使用感知器的方法是有监督的。用户提供数据来训练网络，然后在新数据上对该网络进行测试。聚类算法则是一种无监督学习（unsupervised learning）方法。在这种模型中，算法会根据数据的一个或多个属性将一组特征向量组织成聚类。 &#160; 图 4：在一个二维特征空间中的聚类 &#160; 你可以使用少量代码就能实现的最简单的聚类算法是 k-均值（k-means）。其中，k 表示你为样本分配的聚类的数量。你可以使用一个随机特征向量来对一个聚类进行初始化，然后将其它样本添加到其最近邻的聚类（假定每个样本都能表示一个特征向量，并且可以使用 Euclidean distance 来确定「距离」）。随着你往一个聚类添加的样本越来越多，其形心（centroid，即聚类的中心）就会重新计算。然后该算法会重新检查一次样本，以确保它们都在最近邻的聚类中，最后直到没有样本需要改变所属聚类。 &#160; 尽管 k-均值聚类相对有效，但你必须事先确定 k 的大小。根据数据的不同，其它方法可能会更加有效，比如分层聚类（hierarchical clustering）或基于分布的聚类（distribution-based clustering）。 &#160; &#160; 决策树 &#160; 决策树和聚类很相近。决策树是一种关于观察（observation）的预测模型，可以得到一些结论。结论在决策树上被表示成树叶，而节点则是观察分叉的决策点。决策树来自决策树学习算法，其中数据集会根据属性值测试（attribute value tests）而被分成不同的子集，这个分割过程被称为递归分区（recursive partitioning）。 &#160; 考虑下图中的示例。在这个数据集中，我可以基于三个因素观察到某人是否有生产力。使用一个决策树学习算法，我可以通过一个指标来识别属性（其中一个例子是信息增益）。在这个例子中，心情（mood）是生产力的主要影响因素，所以我根据 Good Mood 一项是 Yes 或 No 而对这个数据集进行了分割。但是，在 Yes 这边，还需要我根据其它两个属性再次对该数据集进行切分。表中不同的颜色对应右侧中不同颜色的叶节点。 &#160; 图 5：一个简单的数据集及其得到的决策树 &#160; 决策树的一个重要性质在于它们的内在的组织能力，这能让你轻松地（图形化地）解释你分类一个项的方式。流行的决策树学习算法包括 C4.5 以及分类与回归树（Classification and Regression Tree）。 &#160; &#160; 基于规则的系统 &#160; 最早的基于规则和推理的系统是 Dendral，于 1965 年被开发出来，但直到 1970 年代，所谓的专家系统（expert systems）才开始大行其道。基于规则的系统会同时存有所需的知识的规则，并会使用一个推理系统（reasoning system）来得出结论。 &#160; 基于规则的系统通常由一个规则集合、一个知识库、一个推理引擎（使用前向或反向规则链）和一个用户接口组成。下图中，我使用了知识「苏格拉底是人」、规则「如果是人，就会死」以及一个交互「谁会死？」 &#160; 图 6：基于规则的系统 &#160; 基于规则的系统已经在语音识别、规划和控制以及疾病识别等领域得到了应用。上世纪 90 年代人们开发的一个监控和诊断大坝稳定性的系统 Kaleidos 至今仍在使用。 &#160; &#160; 机器学习 &#160; 机器学习是人工智能和计算机科学的一个子领域，也有统计学和数学优化方面的根基。机器学习涵盖了有监督学习和无监督学习领域的技术，可用于预测、分析和数据挖掘。机器学习不限于深度学习这一种。但在这一节，我会介绍几种使得深度学习变得如此高效的算法。 &#160; 图 7：机器学习方法的时间线 &#160; &#160; 反向传播 &#160; 神经网络的强大力量源于其多层的结构。单层感知器的训练是很直接的，但得到的网络并不强大。那问题就来了：我们如何训练多层网络呢？这就是反向传播的用武之地。 &#160; 反向传播是一种用于训练多层神经网络的算法。它的工作过程分为两个阶段。第一阶段是将输入传播通过整个神经网络直到最后一层（称为前馈）。第二阶段，该算法会计算一个误差，然后从最后一层到第一层反向传播该误差（调整权重）。 &#160; 图 8：反向传播示意图 &#160; 在训练过程中，该网络的中间层会自己进行组织，将输入空间的部分映射到输出空间。反向传播，使用监督学习，可以识别出输入到输出映射的误差，然后可以据此调整权重（使用一个学习率）来矫正这个误差。反向传播现在仍然是神经网络学习的一个重要方面。随着计算资源越来越快、越来越便宜，它还将继续在更大和更密集的网络中得到应用。 &#160; &#160;]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
<p>人工智能的发展曾经经历过几次起起伏伏，近来在深度学习技术的推动下又迎来了一波新的前所未有的高潮。近日，IBM 官网发表了一篇概述文章，对人工智能技术的发展过程进行了简单梳理，同时还图文并茂地介绍了感知器、聚类算法、基于规则的系统、机器学习、深度学习、神经网络等技术的概念和原理。</p>
<p>&nbsp;</p>
<p>人类对如何创造智能机器的思考从来没有中断过。期间，人工智能的发展起起伏伏，有成功，也有失败，以及其中暗藏的潜力。今天，有太多的新闻报道是关于机器学习算法的应用问题，从癌症检查预测到图像理解、自然语言处理，人工智能正在赋能并改变着这个世界。</p>
<p>&nbsp;</p>
<p>现代人工智能的历史具备成为一部伟大戏剧的所有元素。在最开始的 1950 年代，人工智能的发展紧紧围绕着思考机器和焦点人物比如艾伦·图灵、冯·诺伊曼，迎来了其第一次春天。经过数十年的繁荣与衰败，以及难以置信的高期望，人工智能及其先驱们再次携手来到一个新境界。现在，人工智能正展现着其真正的潜力，深度学习、认知计算等新技术不断涌现，且不乏应用指向。</p>
<p>&nbsp;</p>
<p>本文探讨了人工智能及其子领域的一些重要方面。下面就先从人工智能发展的时间线开始，并逐个剖析其中的所有元素。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>现代人工智能的时间线</p>
<p>&nbsp;</p>
<p>1950 年代初期，人工智能聚焦在所谓的强人工智能，希望机器可以像人一样完成任何智力任务。强人工智能的发展止步不前，导致了弱人工智能的出现，即把人工智能技术应用于更窄领域的问题。1980 年代之前，人工智能的研究一直被这两种范式分割着，两营相对。但是，1980 年左右，机器学习开始成为主流，它的目的是让计算机具备学习和构建模型的能力，从而它们可在特定领域做出预测等行为。很多初学者，对大数据的概念都是模糊不清的，大数据是什么，能做什么，学的时候，该按照什么线路去学习，学完往哪方面发展，想深入了解，想学习的同学欢迎加入大数据学习扣群：805127855，有大量干货（零基础以及进阶的经典实战）分享给大家，并且有清华大学毕业的资深大数据讲师给大家免费授课，给大家分享目前国内最完整的大数据高端实战实用学习流程体系<br /> &nbsp;</p>
<p>&nbsp;</p>
<p><img alt="" class="has" height="151" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181026102433240.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NxYWNyeTI3OTg=,size_27,color_FFFFFF,t_70" width="691"></p>
<p>图 1：现代人工智能发展的时间线</p>
<p>&nbsp;</p>
<p>在人工智能和机器学习研究的基础之上，深度学习在 2000 年左右应运而生。计算机科学家在多层神经网络之中使用了新的拓扑学和学习方法。最终，神经网络的进化成功解决了多个领域的棘手问题。</p>
<p>&nbsp;</p>
<p>在过去的十年中，认知计算（Cognitive computing）也出现了，其目标是打造可以学习并与人类自然交互的系统。通过成功地击败 Jeopardy 游戏的世界级选手，IBM Watson 证明了认知计算的价值。</p>
<p>&nbsp;</p>
<p>在本文中，我将逐一探索上述的所有领域，并对一些关键算法作出解释。</p>
<p>基础性人工智能</p>
<p>&nbsp;</p>
<p>1950 年之前的研究提出了大脑是由电脉冲网络组成的想法，正是脉冲之间的交互产生了人类思想与意识。艾伦·图灵表明一切计算皆是数字，那么，打造一台能够模拟人脑的机器也就并非遥不可及。</p>
<p>&nbsp;</p>
<p>上文说过，早期的研究很多是强人工智能，但是也提出了一些基本概念，被机器学习和深度学习沿用至今。</p>
<p>&nbsp;</p>
<p><img alt="" class="has" height="184" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181026102459645.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NxYWNyeTI3OTg=,size_27,color_FFFFFF,t_70" width="667"></p>
<p>图 2：1950 &#8211; 1980 年间人工智能方法的时间线</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>人工智能搜索引擎</p>
<p>&nbsp;</p>
<p>人工智能中的很多问题可以通过强力搜索（brute-force search）得到解决。然而，考虑到中等问题的搜索空间，基本搜索很快就受影响。人工智能搜索的最早期例子之一是跳棋程序的开发。亚瑟·塞缪尔（Arthur Samuel）在 IBM 701 电子数据处理机器上打造了第一款跳棋程序，实现了对搜索树（alpha-beta 剪枝）的优化；这个程序也记录并奖励具体行动，允许应用学习每一个玩过的游戏（这是首个自我学习的程序）。为了提升程序的学习率，塞缪尔将其编程为自我游戏，以提升其游戏和学习的能力。</p>
<p>&nbsp;</p>
<p>尽管你可以成功地把搜索应用到很多简单问题上，但是当选择的数量增加时，这一方法很快就会失效。以简单的一字棋游戏为例，游戏一开始，有 9 步可能的走棋，每 1 个走棋有 8 个可能的相反走棋，依次类推。一字棋的完整走棋树包含 362,880 个节点。如果你继续将这一想法扩展到国际象棋或者围棋，很快你就会发展搜索的劣势。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>感知器</p>
<p>&nbsp;</p>
<p>感知器是单层神经网络的一个早期监督学习算法。给定一个输入特征向量，感知器可对输入进行具体分类。通过使用训练集，网络的权重和偏差可为线性分类而更新。感知器的首次实现是 IBM 704，接着在自定义硬件上用于图像识别。</p>
<p>&nbsp;</p>
<p><img alt="" class="has" height="273" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181026102530403.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NxYWNyeTI3OTg=,size_27,color_FFFFFF,t_70" width="653"></p>
<p>图 3：感知器与线性分类</p>
<p>&nbsp;</p>
<p>作为一个线性分类器，感知器有能力解决线性分离问题。感知器局限性的典型实例是它无法学习专属的 OR （XOR） 函数。多层感知器解决了这一问题，并为更复杂的算法、网络拓扑学、深度学习奠定了基础。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>聚类算法</p>
<p>&nbsp;</p>
<p>使用感知器的方法是有监督的。用户提供数据来训练网络，然后在新数据上对该网络进行测试。聚类算法则是一种无监督学习（unsupervised learning）方法。在这种模型中，算法会根据数据的一个或多个属性将一组特征向量组织成聚类。</p>
<p>&nbsp;</p>
<p><img alt="" class="has" height="344" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2018102610255187.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NxYWNyeTI3OTg=,size_27,color_FFFFFF,t_70" width="696"></p>
<p>图 4：在一个二维特征空间中的聚类</p>
<p>&nbsp;</p>
<p>你可以使用少量代码就能实现的最简单的聚类算法是 k-均值（k-means）。其中，k 表示你为样本分配的聚类的数量。你可以使用一个随机特征向量来对一个聚类进行初始化，然后将其它样本添加到其最近邻的聚类（假定每个样本都能表示一个特征向量，并且可以使用 Euclidean distance 来确定「距离」）。随着你往一个聚类添加的样本越来越多，其形心（centroid，即聚类的中心）就会重新计算。然后该算法会重新检查一次样本，以确保它们都在最近邻的聚类中，最后直到没有样本需要改变所属聚类。</p>
<p>&nbsp;</p>
<p>尽管 k-均值聚类相对有效，但你必须事先确定 k 的大小。根据数据的不同，其它方法可能会更加有效，比如分层聚类（hierarchical clustering）或基于分布的聚类（distribution-based clustering）。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>决策树</p>
<p>&nbsp;</p>
<p>决策树和聚类很相近。决策树是一种关于观察（observation）的预测模型，可以得到一些结论。结论在决策树上被表示成树叶，而节点则是观察分叉的决策点。决策树来自决策树学习算法，其中数据集会根据属性值测试（attribute value tests）而被分成不同的子集，这个分割过程被称为递归分区（recursive partitioning）。</p>
<p>&nbsp;</p>
<p>考虑下图中的示例。在这个数据集中，我可以基于三个因素观察到某人是否有生产力。使用一个决策树学习算法，我可以通过一个指标来识别属性（其中一个例子是信息增益）。在这个例子中，心情（mood）是生产力的主要影响因素，所以我根据 Good Mood 一项是 Yes 或 No 而对这个数据集进行了分割。但是，在 Yes 这边，还需要我根据其它两个属性再次对该数据集进行切分。表中不同的颜色对应右侧中不同颜色的叶节点。</p>
<p>&nbsp;</p>
<p><img alt="" class="has" height="184" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181026102620416.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NxYWNyeTI3OTg=,size_27,color_FFFFFF,t_70" width="683"></p>
<p>图 5：一个简单的数据集及其得到的决策树</p>
<p>&nbsp;</p>
<p>决策树的一个重要性质在于它们的内在的组织能力，这能让你轻松地（图形化地）解释你分类一个项的方式。流行的决策树学习算法包括 C4.5 以及分类与回归树（Classification and Regression Tree）。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>基于规则的系统</p>
<p>&nbsp;</p>
<p>最早的基于规则和推理的系统是 Dendral，于 1965 年被开发出来，但直到 1970 年代，所谓的专家系统（expert systems）才开始大行其道。基于规则的系统会同时存有所需的知识的规则，并会使用一个推理系统（reasoning system）来得出结论。</p>
<p>&nbsp;</p>
<p>基于规则的系统通常由一个规则集合、一个知识库、一个推理引擎（使用前向或反向规则链）和一个用户接口组成。下图中，我使用了知识「苏格拉底是人」、规则「如果是人，就会死」以及一个交互「谁会死？」</p>
<p>&nbsp;</p>
<p><img alt="" class="has" height="377" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2018102610264788.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NxYWNyeTI3OTg=,size_27,color_FFFFFF,t_70" width="674"></p>
<p>图 6：基于规则的系统</p>
<p>&nbsp;</p>
<p>基于规则的系统已经在语音识别、规划和控制以及疾病识别等领域得到了应用。上世纪 90 年代人们开发的一个监控和诊断大坝稳定性的系统 Kaleidos 至今仍在使用。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>机器学习</p>
<p>&nbsp;</p>
<p>机器学习是人工智能和计算机科学的一个子领域，也有统计学和数学优化方面的根基。机器学习涵盖了有监督学习和无监督学习领域的技术，可用于预测、分析和数据挖掘。机器学习不限于深度学习这一种。但在这一节，我会介绍几种使得深度学习变得如此高效的算法。</p>
<p>&nbsp;</p>
<p><img alt="" class="has" height="193" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181026102726685.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NxYWNyeTI3OTg=,size_27,color_FFFFFF,t_70" width="674"></p>
<p>图 7：机器学习方法的时间线</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>反向传播</p>
<p>&nbsp;</p>
<p>神经网络的强大力量源于其多层的结构。单层感知器的训练是很直接的，但得到的网络并不强大。那问题就来了：我们如何训练多层网络呢？这就是反向传播的用武之地。</p>
<p>&nbsp;</p>
<p>反向传播是一种用于训练多层神经网络的算法。它的工作过程分为两个阶段。第一阶段是将输入传播通过整个神经网络直到最后一层（称为前馈）。第二阶段，该算法会计算一个误差，然后从最后一层到第一层反向传播该误差（调整权重）。</p>
<p>&nbsp;</p>
<p><img alt="" class="has" height="521" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181026102749384.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NxYWNyeTI3OTg=,size_27,color_FFFFFF,t_70" width="669"></p>
<p>图 8：反向传播示意图</p>
<p>&nbsp;</p>
<p>在训练过程中，该网络的中间层会自己进行组织，将输入空间的部分映射到输出空间。反向传播，使用监督学习，可以识别出输入到输出映射的误差，然后可以据此调整权重（使用一个学习率）来矫正这个误差。反向传播现在仍然是神经网络学习的一个重要方面。随着计算资源越来越快、越来越便宜，它还将继续在更大和更密集的网络中得到应用。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>卷积神经网络</p>
<p>&nbsp;</p>
<p>卷积神经网络（CNN）是受动物视觉皮层启发的多层神经网络。这种架构在包括图像处理的很多应用中都有用。第一个 CNN 是由 Yann LeCun 创建的，当时 CNN 架构主要用于手写字符识别任务，例如读取邮政编码。</p>
<p>LeNet CNN 由好几层能够分别实现特征提取和分类的神经网络组成。图像被分为多个可以被接受的区域，这些子区域进入到一个能够从输入图像提取特征的卷积层。下一步就是池化，这个过程降低了卷积层提取到的特征的维度（通过下采样的方法），同时保留了最重要的信息（通常通过最大池化的方法）。然后这个算法又执行另一次卷积和池化，池化之后便进入一个全连接的多层感知器。卷积神经网络的最终输出是一组能够识别图像特征的节点（在这个例子中，每个被识别的数字都是一个节点）。使用者可以通过反向传播的方法来训练网络。</p>
<p>&nbsp;</p>
<p><img alt="" class="has" height="150" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181026102828234.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NxYWNyeTI3OTg=,size_27,color_FFFFFF,t_70" width="641"></p>
<p>图 9.LeNet 卷积神经网络架构</p>
<p>&nbsp;</p>
<p>对深层处理、卷积、池化以及全连接分类层的使用打开了神经网络的各种新型应用的大门。除了图像处理之外，卷积神经网络已经被成功地应用在了视频识别以及自然语言处理等多种任务中。卷积神经网络也已经在 GPU 上被有效地实现，这极大地提升了卷积神经网络的性能。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>长短期记忆（LSTM）</p>
<p>&nbsp;</p>
<p>记得前面反向传播中的讨论吗？网络是前馈式的训练的。在这种架构中，我们将输入送到网络并且通过隐藏层将它们向前传播到输出层。但是，还存在其他的拓扑结构。我在这里要研究的一个架构允许节点之间形成直接的回路。这些神经网络被称为循环神经网络（RNN），它们可以向前面的层或者同一层的后续节点馈送内容。这一特性使得这些网络对时序数据而言是理想化的。</p>
<p>&nbsp;</p>
<p>在 1997 年，一种叫做长短期记忆（LSTM）的特殊的循环网络被发明了。LSTM 包含网络中能够长时间或者短时间记忆数值的记忆单元。</p>
<p>&nbsp;</p>
<p><img alt="" class="has" height="327" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/2018102610285835.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NxYWNyeTI3OTg=,size_27,color_FFFFFF,t_70" width="670"></p>
<p>图 10. 长短期记忆网络和记忆单元</p>
<p>&nbsp;</p>
<p>记忆单元包含了能够控制信息流入或者流出该单元的一些门。输入门（input gate）控制什么时候新的信息可以流入记忆单元。遗忘门（forget gate）控制一段信息在记忆单元中存留的时间。最后，输出门（output gate）控制输出何时使用记忆单元中包含的信息。记忆单元还包括控制每一个门的权重。训练算法（通常是通过时间的反向传播（backpropagation-through-time），反向传播算法的一种变体）基于所得到的误差来优化这些权重。</p>
<p>LSTM 已经被应用在语音识别、手写识别、语音合成、图像描述等各种任务中。下面我还会谈到 LSTM。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>深度学习</p>
<p>&nbsp;</p>
<p>深度学习是一组相对新颖的方法集合，它们从根本上改变了机器学习。深度学习本身不是一种算法，但是它是一系列可以用无监督学习实现深度网络的算法。这些网络是非常深层的，所以需要新的计算方法来构建它们，例如 GPU，除此之外还有计算机集群。</p>
<p>&nbsp;</p>
<p>本文目前已经介绍了两种深度学习的算法：卷积神经网络和长短期记忆网络。这些算法已经被结合起来实现了一些令人惊讶的智能任务。如下图所示，卷积神经网络和长短期记忆已经被用来识别并用自然语言描述图片或者视频中的物体。</p>
<p>&nbsp;</p>
<p><img alt="" class="has" height="120" src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20181026102924996.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2NxYWNyeTI3OTg=,size_27,color_FFFFFF,t_70" width="675"></p>
<p>图 11. 结合卷积神经网络和长短期记忆来进行图像描述</p>
<p>&nbsp;</p>
<p>深度学习算法也已经被用在了人脸识别中，也能够以 96% 的准确率来识别结核病，还被用在自动驾驶和其他复杂的问题中。</p>
<p>&nbsp;</p>
<p>然而，尽管运用深度学习算法有着很多结果，但是仍然存在问题需要我们去解决。一个最近的将深度学习用于皮肤癌检测的应用发现，这个算法比经过认证的皮肤科医生具有更高的准确率。但是，医生可以列举出导致其诊断结果的因素，却没有办法知道深度学习程序在分类的时候所用的因素。这被称为深度学习的黑箱问题。</p>
<p>&nbsp;</p>
<p>另一个被称为 Deep Patient 的应用，在提供病人的病例时能够成功地预测疾病。该应用被证明在疾病预测方面比医生还做得好——即使是众所周知的难以预测的精神分裂症。所以，即便模型效果良好，也没人能够深入到这些大型神经网络去找到原因。</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>认知计算</p>
<p>&nbsp;</p>
<p>人工智能和机器学习充满了生物启示的案例。尽管早期的人工智能专注于建立模仿人脑的机器这一宏伟目标，而现在，是认知计算正在朝着这个目标迈进。</p>
<p>&nbsp;</p>
<p>认知计算建立在神经网络和深度学习之上，运用认知科学中的知识来构建能够模拟人类思维过程的系统。然而，认知计算覆盖了好多学科，例如机器学习、自然语言处理、视觉以及人机交互，而不仅仅是聚焦于某个单独的技术。</p>
<p>&nbsp;</p>
<p>认知学习的一个例子就是 IBM 的 Waston，它在 Jeopardy 上展示了当时最先进的问答交互。IBM 已经将其扩展在了一系列的 web 服务上了。这些服务提供了用于一些列应用的编程接口来构建强大的虚拟代理，这些接口有：视觉识别、语音文本转换（语音识别）、文本语音转换（语音合成）、语言理解和翻译、以及对话引擎。</p>
<p>很多初学者，对大数据的概念都是模糊不清的，大数据是什么，能做什么，学的时候，该按照什么线路去学习，学完往哪方面发展，想深入了解，想学习的同学欢迎加入大数据学习扣群：805127855，有大量干货（零基础以及进阶的经典实战）分享给大家，并且有清华大学毕业的资深大数据讲师给大家免费授课，给大家分享目前国内最完整的大数据高端实战实用学习流程体系<br /> &nbsp;</p>
<p>&nbsp;</p>
<p>继续前进</p>
<p>&nbsp;</p>
<p>本文仅仅涵盖了关于人工智能历史以及最新的神经网络和深度学习方法的一小部分。尽管人工智能和机器学习经历了很多起起伏伏，但是像深度学习和认知计算这样的新方法已经明显地提升了这些学科的水平。虽然可能还无法实现一个具有意识的机器，但是今天确实有着能够改善人类生活的人工智能系统。</p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>Mybatis查询时，区分大小写</title>
		<link>https://uzzz.org/article/1735.html</link>
				<pubDate>Fri, 21 Sep 2018 07:42:02 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[大数据]]></category>
		<category><![CDATA[数据库]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1735.html</guid>
				<description><![CDATA[登陆的时候，发现输入账号的不同大小写竟然能够登陆。Mybatis查询代码如下 &#60;select id="selectById" parameterType="java.lang.String" resultType="com.deep.web.func.entity.UserData"&#62; select &#60;include refid="sql_columns" /&#62; from &#60;include refid="table_name" /&#62; &#60;where&#62; userName=#{userName} &#60;/where&#62; &#60;/select&#62; 通过多次断点查询后，发现是上面的xml文件没写对。 userName=#{userName} 这样写不匹配大小写。 解决办法，加个BINARY &#60;select id="selectById" parameterType="java.lang.String" resultType="com.deep.web.func.entity.UserData"&#62; select &#60;include refid="sql_columns" /&#62; from &#60;include refid="table_name" /&#62; &#60;where&#62; BINARY userId=#{uid} &#60;/where&#62; &#60;/select&#62;]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<p>登陆的时候，发现输入账号的不同大小写竟然能够登陆。Mybatis查询代码如下</p>
<pre><code>&lt;select id="selectById" parameterType="java.lang.String" resultType="com.deep.web.func.entity.UserData"&gt;  
        select &lt;include refid="sql_columns" /&gt; from &lt;include refid="table_name" /&gt; 
        &lt;where&gt;
            userName=#{userName}  
        &lt;/where&gt;
    &lt;/select&gt;
</code></pre>
<p>通过多次断点查询后，发现是上面的xml文件没写对。</p>
<pre><code> userName=#{userName}  
</code></pre>
<p>这样写不匹配大小写。<br /> 解决办法，加个BINARY</p>
<pre><code> &lt;select id="selectById" parameterType="java.lang.String" resultType="com.deep.web.func.entity.UserData"&gt;  
        select &lt;include refid="sql_columns" /&gt; from &lt;include refid="table_name" /&gt; 
        &lt;where&gt;
           BINARY userId=#{uid}  
        &lt;/where&gt;
    &lt;/select&gt;
</code></pre>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-e9f16cbbc2.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
		<item>
		<title>数据增强——基本方法</title>
		<link>https://uzzz.org/article/3162.html</link>
				<pubDate>Sun, 15 Jul 2018 01:51:57 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[大数据]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/3162.html</guid>
				<description><![CDATA[基本数据增强主要包含如下方式： 1.旋转： 可通过在原图上先放大图像，然后剪切图像得到。 2.平移：先放大图像，然后水平或垂直偏移位置剪切 3.缩放：缩放图像 4.随机遮挡：对图像进行小区域遮挡 5.水平翻转：以过图像中心的竖直轴为对称轴，将左、右两边像素交换 6.颜色色差（饱和度、亮度、对比度、 锐度等） 7.噪声扰动: 对图像的每个像素RGB进行随机扰动, 常用的噪声模式是椒盐噪声和高斯噪声; Tensorflow代码实现： 参数可根据需求进行相应调整。 # -*- coding: utf-8 -*- """ # 数据增强实现 """ import tensorflow as tf import cv2 import numpy as np from scipy import misc import random def random_rotate_image(image): interb = ['nearest','bilinear','cubic','bicubic'] angle = np.random.uniform(low=-10.0, high=10.0) key = random.randint(0,3) return misc.imrotate(image, angle, interb[key]) def random_occlusion(image): b_ratio = 1./10 #遮挡比例 M1 = np.ones((320,250)) b_H = random.randint(10,320*(1-b_ratio)-10) b_W = random.randint(10,250*(1-b_ratio)-10) M1[b_H:int(b_H+320*b_ratio),b_W:int(b_W+250*b_ratio)] = 0 M1 = np.expand_dims(M1, 2) image = image*M1 image = image.astype(np.uint8) return image def data_augumrntation(image): image = tf.py_func(random_occlusion, [image], tf.uint8) #随机遮挡 image = tf.py_func(random_rotate_image, [image], tf.uint8) #旋转 ratio = [0.9,1.1] #缩放比例 new_H = random.randint(320*ratio[0], 320*ratio[1]) new_W = random.randint(250*ratio[0], 250*ratio[1]) print(new_H,new_W) image.set_shape((320, 250,3)) image = tf.image.resize_images(image,[new_H, new_W]) image = tf.cast(image,tf.uint8) image = tf.image.resize_image_with_crop_or_pad(image, 320, 250 )#缩放 image = tf.random_crop(image, [299, 235, 3]) #随机裁剪 image = tf.image.random_flip_left_right(image)#镜像 N_key = random.randint(0,10) if N_key == 8: image = tf.image.per_image_standardization(image)#标准化 image = tf.cast(image, tf.float32) image = tf.minimum(255.0, tf.maximum(0.0,tf.image.random_brightness(image,25.0)))#光照 image = tf.minimum(255.0, tf.maximum(0.0,tf.image.random_contrast(image,0.8,1.2)))#对比度 noise = tf.random_normal((299, 235, 3), mean=0.0, stddev=1.0, dtype=tf.float32) image = tf.minimum(255.0, tf.maximum(0.0,image+noise))#随机噪声 image = tf.subtract(image,127.5) image = tf.multiply(image,0.0078125) return image if]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div id="content_views" class="markdown_views prism-atom-one-dark">
  <!-- flowchart 箭头图标 勿删 --><br />
  <svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
   <path stroke-linecap="round" d="M5,0 0,2.5 5,5z" id="raphael-marker-block" style="-webkit-tap-highlight-color: rgba(0, 0, 0, 0);"></path>
  </svg> </p>
<p><strong>基本数据增强主要包含如下方式：</strong><br /> <strong>1.旋转</strong>： 可通过在原图上先放大图像，然后剪切图像得到。<br /> <strong>2.平移</strong>：先放大图像，然后水平或垂直偏移位置剪切<br /> <strong>3.缩放</strong>：缩放图像<br /> <strong>4.随机遮挡</strong>：对图像进行小区域遮挡<br /> <strong>5.水平翻转</strong>：以过图像中心的竖直轴为对称轴，将左、右两边像素交换<br /> <strong>6.颜色色差</strong>（饱和度、亮度、对比度、 锐度等）<br /> <strong>7.噪声扰动</strong>: 对图像的每个像素RGB进行随机扰动, 常用的噪声模式是椒盐噪声和高斯噪声;</p>
<hr>
<p><strong>Tensorflow代码实现：</strong><br /> 参数可根据需求进行相应调整。</p>
<pre><code># -*- coding: utf-8 -*-
"""
# 数据增强实现
"""
import tensorflow as tf
import cv2
import numpy as np
from scipy import misc
import random

def random_rotate_image(image):
    interb = ['nearest','bilinear','cubic','bicubic']
    angle = np.random.uniform(low=-10.0, high=10.0)
    key = random.randint(0,3)
    return misc.imrotate(image, angle, interb[key])

def random_occlusion(image):
    b_ratio = 1./10 #遮挡比例
    M1 = np.ones((320,250))
    b_H = random.randint(10,320*(1-b_ratio)-10)  
    b_W = random.randint(10,250*(1-b_ratio)-10)
    M1[b_H:int(b_H+320*b_ratio),b_W:int(b_W+250*b_ratio)] = 0
    M1 = np.expand_dims(M1, 2)
    image = image*M1
    image = image.astype(np.uint8)
    return image

def data_augumrntation(image):
    image = tf.py_func(random_occlusion, [image], tf.uint8) #随机遮挡
    image = tf.py_func(random_rotate_image, [image], tf.uint8) #旋转
    ratio = [0.9,1.1] #缩放比例
    new_H = random.randint(320*ratio[0], 320*ratio[1])
    new_W = random.randint(250*ratio[0], 250*ratio[1])
    print(new_H,new_W)
    image.set_shape((320, 250,3))
    image = tf.image.resize_images(image,[new_H, new_W])
    image = tf.cast(image,tf.uint8)
    image = tf.image.resize_image_with_crop_or_pad(image, 320, 250 )#缩放
    image = tf.random_crop(image, [299, 235, 3]) #随机裁剪
    image = tf.image.random_flip_left_right(image)#镜像
    N_key = random.randint(0,10)
    if N_key == 8:
        image = tf.image.per_image_standardization(image)#标准化
    image = tf.cast(image, tf.float32)
    image = tf.minimum(255.0, tf.maximum(0.0,tf.image.random_brightness(image,25.0)))#光照
    image = tf.minimum(255.0, tf.maximum(0.0,tf.image.random_contrast(image,0.8,1.2)))#对比度
    noise = tf.random_normal((299, 235, 3), mean=0.0, stddev=1.0, dtype=tf.float32)
    image = tf.minimum(255.0, tf.maximum(0.0,image+noise))#随机噪声    
    image = tf.subtract(image,127.5)
    image = tf.multiply(image,0.0078125)    
    return image

if __name__ == '__main__':
    pic = r"bb.jpg"
    file_contents = tf.read_file(pic)
    image = tf.image.decode_jpeg(file_contents, dct_method="INTEGER_ACCURATE")
    R,G,B=tf.unstack(image, num=3, axis=2)
    image=tf.stack([B,G,R], axis=2) #通道转换
    image = data_augumrntation(image)

    #image = tf.cast(image,tf.uint8)
    sess = tf.Session()
    img = sess.run(image)
    cv2.imshow('img',img)
    cv2.waitKey()
</code></pre>
<p>原图：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180715094703154?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="这里写图片描述"><br /> 增强后图像（图像做了归一化操作）：<br /> <img src="https://uzshare.com/_p?https://img-blog.csdn.net/2018071509474914?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" width="30%" alt=""></p>
<hr>
<h6><a id="_88"></a>注：博众家之所长，集群英之荟萃。</h6>
<p><img src="https://uzshare.com/_p?https://img-blog.csdnimg.cn/20190714110533194.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTM4NDExOTY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述" width="300"></p>
</p></div>
<link href="https://csdnimg.cn/release/phoenix/mdeditor/markdown_views-526ced5128.css" rel="stylesheet">
</div>
]]></content:encoded>
										</item>
	</channel>
</rss>
