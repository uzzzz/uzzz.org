<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>搜索引擎 &#8211; 有组织在!</title>
	<atom:link href="https://uzzz.org/tag/sousuoyinqing/feed" rel="self" type="application/rss+xml" />
	<link>https://uzzz.org/</link>
	<description></description>
	<lastBuildDate>Wed, 04 Apr 2012 15:26:46 +0000</lastBuildDate>
	<language>en-US</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=5.2.4</generator>

<image>
	<url>https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png</url>
	<title>搜索引擎 &#8211; 有组织在!</title>
	<link>https://uzzz.org/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>搜索引擎duckduckgo</title>
		<link>https://uzzz.org/article/2364.html</link>
				<pubDate>Wed, 04 Apr 2012 15:26:46 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[bing]]></category>
		<category><![CDATA[google]]></category>
		<category><![CDATA[存储]]></category>
		<category><![CDATA[搜索引擎]]></category>
		<category><![CDATA[活动]]></category>
		<category><![CDATA[算法]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2364.html</guid>
				<description><![CDATA[北京时间3月31日消息，DuckDuckGo搜索引擎近来发展迅猛，3个月来搜索请求以平均每天227％的速度高速增长。虽然现在它尚不能撼动Google搜索霸主地位，但谷歌亦也应该提起重视。 报道称，不少国外企业家在其个人电脑上使用的是另类的搜索引擎DuckDuckGo，而不是谷歌或者Bing。另外据ycombinator报道称，使用这个搜索引擎的人也不是一个两个。 从下面的图表中可以看出，到今年为止，DuckDuckGo每天的搜索量如同曲棍球球棍一样快速增长，平均每天搜索请求增速达227％。自去年年底到现在为止，用户数已接近150万。 这巨大的上升势头，一方面是来自去年一月推动的视觉界面设计，另一方面也是数据隐私日活动的鞭策。正是因为DuckDuckGo对数据隐私的保护，黑客甚至都集体使用它。 因此有人提出建议，谷歌应该对此感到重视。不过分析人士指出，DuckDuckGo目前对谷歌而言还不是最大的威胁，谷歌现在还没精力关注它，这对DuckDuckGo来说无异是一件好事。 就目前许多人而言，DuckDuckGo已经被定位成一个有友好隐私保护的搜索引擎。因此它不会在众多竞争中被淘汰掉，也更不用说非要和其他的搜索引擎一决高下，因为DuckDuckgo推动发展方向和战略是对的。 此外在早些访谈中，DuckDuckGo创始人Gabriel Weinberg称，目前的工作重点将是对搜索算法进行改进和加快相应速度。 据悉DuckDuckGo是一个综合性搜索引擎，它索引的搜索结果包括了Google、Bing、维基百科、亚马逊等。另外DuckDuckGo把隐私放在第一位，它不存储用户IP地址、也不记录用户信息，同时DuckDuckGo搜索结果更加实时化，Spam也更少。http://duckduckgo.com/]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
  </p>
<p>北京时间3月31日消息，DuckDuckGo搜索引擎近来发展迅猛，3个月来搜索请求以平均每天227％的速度高速增长。虽然现在它尚不能撼动Google搜索霸主地位，但谷歌亦也应该提起重视。</p>
<p>报道称，不少国外企业家在其个人电脑上使用的是另类的搜索引擎DuckDuckGo，而不是谷歌或者Bing。另外据ycombinator报道称，使用这个搜索引擎的人也不是一个两个。</p>
<p>从下面的图表中可以看出，到今年为止，DuckDuckGo每天的搜索量如同曲棍球球棍一样快速增长，平均每天搜索请求增速达227％。自去年年底到现在为止，用户数已接近150万。</p>
<p style="text-align:center;"><img alt="" src="http://articles.csdn.net/uploads/allimg/120331/119_120331080149_1.png" border="0" height="309" width="520"></p>
<p>这巨大的上升势头，一方面是来自去年一月推动的视觉界面设计，另一方面也是数据隐私日活动的鞭策。正是因为DuckDuckGo对数据隐私的保护，黑客甚至都集体使用它。</p>
<p>因此有人提出建议，谷歌应该对此感到重视。不过分析人士指出，DuckDuckGo目前对谷歌而言还不是最大的威胁，谷歌现在还没精力关注它，这对DuckDuckGo来说无异是一件好事。</p>
<p>就目前许多人而言，DuckDuckGo已经被定位成一个有友好隐私保护的搜索引擎。因此它不会在众多竞争中被淘汰掉，也更不用说非要和其他的搜索引擎一决高下，因为DuckDuckgo推动发展方向和战略是对的。</p>
<p>此外在早些访谈中，DuckDuckGo创始人Gabriel Weinberg称，目前的工作重点将是对搜索算法进行改进和加快相应速度。</p>
<p>据悉DuckDuckGo是一个综合性搜索引擎，它索引的搜索结果包括了Google、Bing、维基百科、亚马逊等。另外DuckDuckGo把隐私放在第一位，它不存储用户IP地址、也不记录用户信息，同时DuckDuckGo搜索结果更加实时化，Spam也更少。<img src="" alt="">http://duckduckgo.com/</p>
<p> 
 </div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>搜索引擎爬虫的基本需求和考核标准</title>
		<link>https://uzzz.org/article/2211.html</link>
				<pubDate>Fri, 09 Jul 2010 01:33:00 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[互联网]]></category>
		<category><![CDATA[工作]]></category>
		<category><![CDATA[搜索引擎]]></category>
		<category><![CDATA[百度]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2211.html</guid>
				<description><![CDATA[需要包含以下基本功能： （1）网站下载流速控制 国内国外的搜索爬虫，科研机构爬虫数量很多，不同的站点抗抓取能力大相径庭，对网站的下载做好控制，避免将网站抓死。 （2）网页抓全 将互联网网页抓全，是极大的挑战，暗网暂且不提，就是明网抓全也不是容易的事情，新站发现，sitemap协议等用站长主动提交的支持等等。 （3）网页抓新（更新及时性） 网页总在不断变化中，如何当网页变化后（更新，消亡）能够及时更新，实时性和死链率等是表征这方面工作的重要指标。 （4）网页重复抓取的避免 为了及时捕捉网页的更新，对同一个网址必须经常去抓取，同样网络是一个网状结构，同一个网址可能被多次引用，这些都导致重复抓取的可能性，如果避免网页抓重，同时控制合理的更新频率，是非常关键的。 （5）DNS自动解析 如果抓取每个网页都进行一次DNS解析，那成本就太大了，维护一个DNS自动解析系统，可以大大降低域名服务器的负担，且大大提高效率。 （6）镜像站点的识别 网页内容相同，但域名不同的情况比比皆是，其中镜像站点的识别尤为关键 （7）抓取的优先级调整 抓取队列总是满的，周而复始，但在抓取的时候会出现，重要的，紧急的，不重要的，不紧急的内容，如何处理好排队的关系尤为重要，是单独开辟绿色通道，还是将其排队号前提都是需要细心打磨的。 （8）抓取深度控制 链接展开的深度控制，避免出现单个站点过分抓取，而使得其他站点持续饥饿 （9）多爬虫的协作 爬虫间的通行量要尽可能少，爬虫出现故障后的自动恢复，抓取主机的异地化等等，据说百度在国外部署的爬虫来抓取国外的站点。 （10）网页下载的存储 网页下载后的本地存储，链接提取，锚文本，链接关系的存储等等。 （11）死链、跳转的识别和处理 在抓取网页失败后，判断是死链还是当机，错误下载的网址再次抓取的时间间隔的控制，redirect的网页收集等等。 考核标准 （1）总有效的网页数（单机） （2）新站发现数（单机） （3）无效抓取的网页数（单机） （4）镜像站点数（单机） （5）全网站点的基本信息（更新周期，死链率，错误率） （6）重要网页的抓取及时性（随机抽取盲测） （7）抓取稳定性，故障率等]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
<p>需要包含以下基本功能：</p>
<p>（1）网站下载流速控制</p>
<p> 国内国外的搜索爬虫，科研机构爬虫数量很多，不同的站点抗抓取能力大相径庭，对网站的下载做好控制，避免将网站抓死。</p>
<p>（2）网页抓全</p>
<p> 将互联网网页抓全，是极大的挑战，暗网暂且不提，就是明网抓全也不是容易的事情，新站发现，sitemap协议等用站长主动提交的支持等等。</p>
<p>（3）网页抓新（更新及时性）</p>
<p> 网页总在不断变化中，如何当网页变化后（更新，消亡）能够及时更新，实时性和死链率等是表征这方面工作的重要指标。</p>
<p>（4）网页重复抓取的避免</p>
<p> 为了及时捕捉网页的更新，对同一个网址必须经常去抓取，同样网络是一个网状结构，同一个网址可能被多次引用，这些都导致重复抓取的可能性，如果避免网页抓重，同时控制合理的更新频率，是非常关键的。</p>
<p>（5）DNS自动解析</p>
</p>
<p> 如果抓取每个网页都进行一次DNS解析，那成本就太大了，维护一个DNS自动解析系统，可以大大降低域名服务器的负担，且大大提高效率。</p>
<p>（6）镜像站点的识别</p>
<p> 网页内容相同，但域名不同的情况比比皆是，其中镜像站点的识别尤为关键</p>
<p>（7）抓取的优先级调整</p>
</p>
<p> 抓取队列总是满的，周而复始，但在抓取的时候会出现，重要的，紧急的，不重要的，不紧急的内容，如何处理好排队的关系尤为重要，是单独开辟绿色通道，还是将其排队号前提都是需要细心打磨的。</p>
<p>（8）抓取深度控制</p>
<p> 链接展开的深度控制，避免出现单个站点过分抓取，而使得其他站点持续饥饿</p>
<p>（9）多爬虫的协作</p>
<p> 爬虫间的通行量要尽可能少，爬虫出现故障后的自动恢复，抓取主机的异地化等等，据说百度在国外部署的爬虫来抓取国外的站点。</p>
<p>（10）网页下载的存储</p>
<p> 网页下载后的本地存储，链接提取，锚文本，链接关系的存储等等。</p>
<p>（11）死链、跳转的识别和处理</p>
<p> 在抓取网页失败后，判断是死链还是当机，错误下载的网址再次抓取的时间间隔的控制，redirect的网页收集等等。</p>
<p>考核标准</p>
<p> （1）总有效的网页数（单机）</p>
<p> （2）新站发现数（单机）</p>
<p> （3）无效抓取的网页数（单机）</p>
<p> （4）镜像站点数（单机）</p>
<p> （5）全网站点的基本信息（更新周期，死链率，错误率）</p>
<p> （6）重要网页的抓取及时性（随机抽取盲测）</p>
<p> （7）抓取稳定性，故障率等</p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>CompletePlanet (动态数据库-搜索引擎)</title>
		<link>https://uzzz.org/article/1591.html</link>
				<pubDate>Thu, 01 Jul 2010 09:10:26 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[未分类]]></category>
		<category><![CDATA[web]]></category>
		<category><![CDATA[搜索引擎]]></category>

		<guid isPermaLink="false">http://wp.uzzz.org/article/1591.html</guid>
				<description><![CDATA[www.completeplanet.com 隐匿查询 &#160;&#160;&#160;&#160;&#160; 数据库里存储的大量的信息对标准的搜索引擎来说是不可见的，标准的搜索引擎只是索引网站上的内容，从一个链接到另一个链接。 隐匿搜索引擎专门用来搜索被称作Deep Web上的隐藏数据。 * 能查找动态数据库。 * 能在一定数据范围内查询。 * 有很好的帮助文档。]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-3019150162.css">
<div class="htmledit_views" id="content_views">
<p><a href="http://www.completeplanet.com" rel="nofollow" data-token="ee7d32fc8169e36279f24f080e706f7c">www.completeplanet.com</a> </p>
<p> 隐匿查询<br /> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 数据库里存储的大量的信息对标准的搜索引擎来说是不可见的，标准的搜索引擎只是索引网站上的内容，从一个链接到另一个链接。 隐匿搜索引擎专门用来搜索被称作Deep Web上的隐藏数据。</p>
<p> * 能查找动态数据库。 <br /> * 能在一定数据范围内查询。 <br /> * 有很好的帮助文档。 </p>
</p></div>
</div>
]]></content:encoded>
										</item>
		<item>
		<title>robots.txt</title>
		<link>https://uzzz.org/article/2284.html</link>
				<pubDate>Fri, 07 Dec 2007 09:02:00 +0000</pubDate>
		<dc:creator><![CDATA[fandyvon]]></dc:creator>
				<category><![CDATA[未分类]]></category>
		<category><![CDATA[google]]></category>
		<category><![CDATA[windows]]></category>
		<category><![CDATA[搜索引擎]]></category>

		<guid isPermaLink="false">https://uzzz.org/article/2284.html</guid>
				<description><![CDATA[robots.txt是一个纯文本文件，在这个文件中网站管理者可以声明该网站中不想被robots访问的部分，或者指定搜索引擎只收录指定的内容。 当一个搜索机器人（有的叫搜索蜘蛛）访问一个站点时，它会首先检查该站点根目录下是否存在robots.txt，如果存在，搜索机器人就会按照该文件中的内容来确定访问的范围；如果该文件不存在，那么搜索机器人就沿着链接抓取。 robots.txt必须放置在一个站点的根目录下，而且文件名必须全部小写。 robots.txt写法 我们来看一个robots.txt范例：http://www.w3.org/robots.txt 访问以上具体地址，我们可以看到robots.txt的具体内容如下： # # robots.txt for http://www.w3.org/ # # $Id: robots.txt,v 1.48 2007/10/16 05:31:15 gerald Exp $ # # For use by search.w3.org User-agent: W3C-gsa Disallow: /Out-Of-Date User-agent: W3T_SE Disallow: /Out-Of-Date User-agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT; MS Search 4.0 Robot) Disallow: / # W3C Link checker User-agent: W3C-checklink Disallow: # exclude some access-controlled areas User-agent: * Disallow: /2004/ontaria/basic Disallow: /Team Disallow: /Project Disallow: /Web Disallow: /Systems Disallow: /History Disallow: /Out-Of-Date Disallow: /2002/02/mid Disallow: /mid/ Disallow: /People/all/ Disallow: /RDF/Validator/ARPServlet Disallow: /2003/03/Translations/byLanguage Disallow: /2003/03/Translations/byTechnology Disallow: /2005/11/Translations/Query Disallow: /2003/glossary/subglossary/ #Disallow: /2005/06/blog/ #Disallow: /2001/07/pubrules-checker #shouldnt get transparent proxies but will ml links of things like pubrules Disallow: /2000/06/webdata/xslt Disallow: /2000/09/webdata/xslt Disallow: /2005/08/online_xslt/xslt Disallow: /Bugs/ Disallow: /Search/Mail/Public/ Disallow: /2006/02/chartergen 以上文本表达的意思是允许所有的搜索机器人访问www.w3.org站点下的所有文件。 具体语法分析：其中#后面文字为说明信息；User-agent:后面为搜索机器人的名称，后面如果是*，则泛指所有的搜索机器人；Disallow:后面为不允许访问的文件目录。 下面，我将列举一些robots.txt的具体用法： 允许所有的robot访问 User-agent: * Disallow: 或者也可以建一个空文件 “/robots.txt” file 禁止所有搜索引擎访问网站的任何部分 User-agent: * Disallow: / 禁止所有搜索引擎访问网站的几个部分（下例中的a、b目录） User-agent: * Disallow: /a/ Disallow: /b/ 禁止某个搜索引擎的访问（下例中的BadBot） User-agent: BadBot Disallow: / 只允许某个搜索引擎的访问（下例中的Crawler） User-agent: Crawler Disallow: User-agent: * Disallow: / 另外，我觉得有必要进行拓展说明，对robots meta进行一些介绍： Robots META标签则主要是针对一个个具体的页面。和其他的META标签（如使用的语言、页面的描述、关键词等）一样，Robots META标签也是放在页面的＜head＞＜/head＞中，专门用来告诉搜索引擎ROBOTS如何抓取该页的内容。 Robots META标签的写法： Robots META标签中没有大小写之分，name=”Robots”表示所有的搜索引擎，可以针对某个具体搜索引擎写为name=”BaiduSpider”。 content部分有四个指令选项：index、noindex、follow、nofollow，指令间以“,”分隔。 INDEX 指令告诉搜索机器人抓取该页面； FOLLOW 指令表示搜索机器人可以沿着该页面上的链接继续抓取下去； Robots Meta标签的缺省值是INDEX和FOLLOW，只有inktomi除外，对于它，缺省值是INDEX,NOFOLLOW。 这样，一共有四种组合： ＜META NAME=”ROBOTS” CONTENT=”INDEX,FOLLOW”＞ ＜META NAME=”ROBOTS” CONTENT=”NOINDEX,FOLLOW”＞ ＜META NAME=”ROBOTS”]]></description>
								<content:encoded><![CDATA[<div id="article_content" class="article_content clearfix">
 <!--一个博主专栏付费入口--><br />
 <!--一个博主专栏付费入口结束-->
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
 <link rel="stylesheet" href="https://csdnimg.cn/release/phoenix/template/css/ck_htmledit_views-d284373521.css">
<div class="htmledit_views" id="content_views">
<p>robots.txt是一个纯文本文件，在这个文件中网站管理者可以声明该网站中不想被robots访问的部分，或者指定搜索引擎只收录指定的内容。</p>
<p>当一个搜索机器人（有的叫搜索蜘蛛）访问一个站点时，它会首先检查该站点根目录下是否存在robots.txt，如果存在，搜索机器人就会按照该文件中的内容来确定访问的范围；如果该文件不存在，那么搜索机器人就沿着链接抓取。<img width="100%" height="10" src="http://wz.mygogou.com/wp-includes/js/tinymce/themes/advanced/images/spacer.gif" alt="More..." title="More..." class="mce_plugin_wordpress_more" name="mce_plugin_wordpress_more"></p>
<p>robots.txt必须放置在一个站点的根目录下，而且文件名必须全部小写。</p>
<p>robots.txt写法</p>
<p>我们来看一个robots.txt范例：http://www.w3.org/robots.txt</p>
<p>访问以上具体地址，我们可以看到robots.txt的具体内容如下：</p>
<p>#<br /> # robots.txt for http://www.w3.org/<br /> #<br /> # $Id: robots.txt,v 1.48 2007/10/16 05:31:15 gerald Exp $<br /> #</p>
<p># For use by search.w3.org<br /> User-agent: W3C-gsa<br /> Disallow: /Out-Of-Date</p>
<p>User-agent: W3T_SE<br /> Disallow: /Out-Of-Date</p>
<p>User-agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT; MS Search 4.0 Robot)<br /> Disallow: /</p>
<p># W3C Link checker<br /> User-agent: W3C-checklink<br /> Disallow:</p>
<p># exclude some access-controlled areas<br /> User-agent: *<br /> Disallow: /2004/ontaria/basic<br /> Disallow: /Team<br /> Disallow: /Project<br /> Disallow: /Web<br /> Disallow: /Systems<br /> Disallow: /History<br /> Disallow: /Out-Of-Date<br /> Disallow: /2002/02/mid<br /> Disallow: /mid/<br /> Disallow: /People/all/<br /> Disallow: /RDF/Validator/ARPServlet<br /> Disallow: /2003/03/Translations/byLanguage<br /> Disallow: /2003/03/Translations/byTechnology<br /> Disallow: /2005/11/Translations/Query<br /> Disallow: /2003/glossary/subglossary/<br /> #Disallow: /2005/06/blog/<br /> #Disallow: /2001/07/pubrules-checker<br /> #shouldnt get transparent proxies but will ml links of things like pubrules<br /> Disallow: /2000/06/webdata/xslt<br /> Disallow: /2000/09/webdata/xslt<br /> Disallow: /2005/08/online_xslt/xslt<br /> Disallow: /Bugs/<br /> Disallow: /Search/Mail/Public/<br /> Disallow: /2006/02/chartergen</p>
<p>以上文本表达的意思是允许所有的搜索机器人访问www.w3.org站点下的所有文件。</p>
<p>具体语法分析：其中#后面文字为说明信息；User-agent:后面为搜索机器人的名称，后面如果是*，则泛指所有的搜索机器人；Disallow:后面为不允许访问的文件目录。</p>
<p>下面，我将列举一些robots.txt的具体用法：</p>
<p>允许所有的robot访问</p>
<p>User-agent: *<br /> Disallow:</p>
<p>或者也可以建一个空文件 “/robots.txt” file</p>
<p>禁止所有搜索引擎访问网站的任何部分</p>
<p>User-agent: *<br /> Disallow: /</p>
<p>禁止所有搜索引擎访问网站的几个部分（下例中的a、b目录）</p>
<p>User-agent: *<br /> Disallow: /a/<br /> Disallow: /b/</p>
<p>禁止某个搜索引擎的访问（下例中的BadBot）</p>
<p>User-agent: BadBot<br /> Disallow: /</p>
<p>只允许某个搜索引擎的访问（下例中的Crawler）</p>
<p>User-agent: Crawler<br /> Disallow:</p>
<p>User-agent: *<br /> Disallow: /</p>
<p>另外，我觉得有必要进行拓展说明，对robots meta进行一些介绍：</p>
<p>Robots META标签则主要是针对一个个具体的页面。和其他的META标签（如使用的语言、页面的描述、关键词等）一样，Robots META标签也是放在页面的＜head＞＜/head＞中，专门用来告诉搜索引擎ROBOTS如何抓取该页的内容。</p>
<p>Robots META标签的写法：</p>
<p>Robots META标签中没有大小写之分，name=”Robots”表示所有的搜索引擎，可以针对某个具体搜索引擎写为name=”BaiduSpider”。 content部分有四个指令选项：index、noindex、follow、nofollow，指令间以“,”分隔。</p>
<p>INDEX 指令告诉搜索机器人抓取该页面；</p>
<p>FOLLOW 指令表示搜索机器人可以沿着该页面上的链接继续抓取下去；</p>
<p>Robots Meta标签的缺省值是INDEX和FOLLOW，只有inktomi除外，对于它，缺省值是INDEX,NOFOLLOW。</p>
<p>这样，一共有四种组合：</p>
<p>＜META NAME=”ROBOTS” CONTENT=”INDEX,FOLLOW”＞<br /> ＜META NAME=”ROBOTS” CONTENT=”NOINDEX,FOLLOW”＞<br /> ＜META NAME=”ROBOTS” CONTENT=”INDEX,NOFOLLOW”＞<br /> ＜META NAME=”ROBOTS” CONTENT=”NOINDEX,NOFOLLOW”＞</p>
<p>其中</p>
<p>＜META NAME=”ROBOTS” CONTENT=”INDEX,FOLLOW”＞可以写成＜META NAME=”ROBOTS” CONTENT=”ALL”＞；</p>
<p>＜META NAME=”ROBOTS” CONTENT=”NOINDEX,NOFOLLOW”＞可以写成＜META NAME=”ROBOTS” CONTENT=”NONE”＞</p>
<p>目前看来，绝大多数的搜索引擎机器人都遵守robots.txt的规则，而对于Robots META标签，目前支持的并不多，但是正在逐渐增加，如著名搜索引擎GOOGLE就完全支持，而且GOOGLE还增加了一个指令“archive”，可以 限制GOOGLE是否保留网页快照。例如：</p>
<p>＜META NAME=”googlebot” CONTENT=”index,follow,noarchive”＞</p>
<p>表示抓取该站点中页面并沿着页面中链接抓取，但是不在GOOLGE上保留该页面的网页快照。</p>
<p>  <a href="http://wiki.mygogou.com/doc-view-815.html" rel="nofollow" data-token="738e81d8c1508e43a0b38b3ca63b892d">&nbsp; robot.txt http://wiki.mygogou.com/doc-view-815.html</a>
 </div>
</div>
]]></content:encoded>
										</item>
	</channel>
</rss>
