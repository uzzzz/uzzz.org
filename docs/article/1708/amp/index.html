<!DOCTYPE html>
<html amp lang="en-US">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1">
	<script type="application/ld+json" class="yoast-schema-graph yoast-schema-graph--main">{"@context":"https://schema.org","@graph":[{"@type":"WebSite","@id":"https://uzzz.org/#website","url":"https://uzzz.org/","name":"\u6709\u7ec4\u7ec7\u5728!","potentialAction":{"@type":"SearchAction","target":"https://uzzz.org/?s={search_term_string}","query-input":"required name=search_term_string"}},{"@type":"ImageObject","@id":"https://uzzz.org/article/1708/#primaryimage","url":"https://uzshare.com/_p?https://img-blog.csdn.net/20170817211846681?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveWluZ2h1YTIwMTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center"},{"@type":"WebPage","@id":"https://uzzz.org/article/1708/#webpage","url":"https://uzzz.org/article/1708/","inLanguage":"en-US","name":"Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?\u8bba\u6587\u7b14\u8bb0 - \u6709\u7ec4\u7ec7\u5728!","isPartOf":{"@id":"https://uzzz.org/#website"},"primaryImageOfPage":{"@id":"https://uzzz.org/article/1708/#primaryimage"},"datePublished":"2017-08-18T04:24:22+00:00","dateModified":"2017-08-18T04:24:22+00:00","author":{"@id":"https://uzzz.org/#/schema/person/29673f1347b0abda5882803c72ee5a3f"}},{"@type":["Person"],"@id":"https://uzzz.org/#/schema/person/29673f1347b0abda5882803c72ee5a3f","name":"fandyvon","sameAs":[]}]}</script>
	<title>Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?论文笔记 - 有组织在!</title>
		<link rel="canonical" href="https://uzzz.org/article/1708/">
	<script type="text/javascript" src="https://cdn.ampproject.org/v0.js" async></script>
<style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
	<style amp-custom>
		/* Generic WP styling */

.alignright {
	float: right;
}

.alignleft {
	float: left;
}

.aligncenter {
	display: block;
	text-align: center;
	margin-left: auto;
	margin-right: auto;
}

.amp-wp-enforced-sizes {
	/** Our sizes fallback is 100vw, and we have a padding on the container; the max-width here prevents the element from overflowing. **/
	max-width: 100%;
	margin: 0 auto;
}


/*
 * Prevent cases of amp-img converted from img to appear with stretching by using object-fit to scale.
 * See <https://github.com/ampproject/amphtml/issues/21371#issuecomment-475443219>.
 * Also use object-fit:contain in worst case scenario when we can't figure out dimensions for an image.
 * Additionally, in side of \AMP_Img_Sanitizer::determine_dimensions() it could $amp_img->setAttribute( 'object-fit', 'contain' )
 * so that the following rules wouldn't be needed.
 */
amp-img.amp-wp-enforced-sizes[layout="intrinsic"] > img,
amp-anim.amp-wp-enforced-sizes[layout="intrinsic"] > img {
	object-fit: contain;
}

amp-fit-text blockquote,
amp-fit-text h1,
amp-fit-text h2,
amp-fit-text h3,
amp-fit-text h4,
amp-fit-text h5,
amp-fit-text h6 {
	font-size: inherit;
}

/**
 * Override a style rule in Twenty Sixteen and Twenty Seventeen.
 * It set display:none for audio elements.
 * This selector is the same, though it adds body and uses amp-audio instead of audio.
 */
body amp-audio:not([controls]) {
	display: inline-block;
	height: auto;
}

/*
 * Style the default template messages for submit-success, submit-error, and submitting. These elements are inserted
 * by the form sanitizer when a POST form lacks the action-xhr attribute.
 */
.amp-wp-default-form-message > p {
	margin: 1em 0;
	padding: 0.5em;
}

.amp-wp-default-form-message[submitting] > p,
.amp-wp-default-form-message[submit-success] > p.amp-wp-form-redirecting {
	font-style: italic;
}

.amp-wp-default-form-message[submit-success] > p:not(.amp-wp-form-redirecting) {
	border: solid 1px #008000;
	background-color: #90ee90;
	color: #000;
}

.amp-wp-default-form-message[submit-error] > p {
	border: solid 1px #f00;
	background-color: #ffb6c1;
	color: #000;
}

/* Prevent showing empty success message in the case of an AMP-Redirect-To response header. */
.amp-wp-default-form-message[submit-success] > p:empty {
	display: none;
}

amp-carousel .amp-wp-gallery-caption {
	position: absolute;
	bottom: 0;
	left: 0;
	right: 0;
	text-align: center;
	background-color: rgba(0, 0, 0, 0.5);
	color: #fff;
	padding: 1rem;
}

.wp-block-gallery[data-amp-carousel="true"] {
	display: block;
	flex-wrap: unset;
}

/* Template Styles */

.amp-wp-content,
.amp-wp-title-bar div {
		margin: 0 auto;
	max-width: 840px;
	}

html {
	background: #0a89c0;
}

body {
	background: #fff;
	color: #353535;
	font-family: Georgia, 'Times New Roman', Times, Serif;
	font-weight: 300;
	line-height: 1.75em;
}

p,
ol,
ul,
figure {
	margin: 0 0 1em;
	padding: 0;
}

a,
a:visited {
	color: #0a89c0;
}

a:hover,
a:active,
a:focus {
	color: #353535;
}

/* Quotes */

blockquote {
	color: #353535;
	background: rgba(127,127,127,.125);
	border-left: 2px solid #0a89c0;
	margin: 8px 0 24px 0;
	padding: 16px;
}

blockquote p:last-child {
	margin-bottom: 0;
}

/* UI Fonts */

.amp-wp-meta,
.amp-wp-header div,
.amp-wp-title,
.wp-caption-text,
.amp-wp-tax-category,
.amp-wp-tax-tag,
.amp-wp-comments-link,
.amp-wp-footer p,
.back-to-top {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen-Sans", "Ubuntu", "Cantarell", "Helvetica Neue", sans-serif;
}

/* Header */

.amp-wp-header {
	background-color: #0a89c0;
}

.amp-wp-header div {
	color: #fff;
	font-size: 1em;
	font-weight: 400;
	margin: 0 auto;
	max-width: calc(840px - 32px);
	padding: .875em 16px;
	position: relative;
}

.amp-wp-header a {
	color: #fff;
	text-decoration: none;
}


.amp-wp-header .amp-wp-site-icon {
	/** site icon is 32px **/
	background-color: #fff;
	border: 1px solid #fff;
	border-radius: 50%;
	position: absolute;
	right: 18px;
	top: 10px;
}

/* Article */

.amp-wp-article {
	color: #353535;
	font-weight: 400;
	margin: 1.5em auto;
	max-width: 840px;
	overflow-wrap: break-word;
	word-wrap: break-word;
}

/* Article Header */

.amp-wp-article-header {
	align-items: center;
	align-content: stretch;
	display: flex;
	flex-wrap: wrap;
	justify-content: space-between;
	margin: 1.5em 16px 0;
}

.amp-wp-title {
	color: #353535;
	display: block;
	flex: 1 0 100%;
	font-weight: 900;
	margin: 0 0 .625em;
	width: 100%;
}

/* Article Meta */

.amp-wp-meta {
	color: #696969;
	display: inline-block;
	flex: 2 1 50%;
	font-size: .875em;
	line-height: 1.5em;
	margin: 0 0 1.5em;
	padding: 0;
}

.amp-wp-article-header .amp-wp-meta:last-of-type {
	text-align: right;
}

.amp-wp-article-header .amp-wp-meta:first-of-type {
	text-align: left;
}

.amp-wp-byline amp-img,
.amp-wp-byline .amp-wp-author {
	display: inline-block;
	vertical-align: middle;
}

.amp-wp-byline amp-img {
	border: 1px solid #0a89c0;
	border-radius: 50%;
	position: relative;
	margin-right: 6px;
}

.amp-wp-posted-on {
	text-align: right;
}

/* Featured image */

.amp-wp-article-featured-image {
	margin: 0 0 1em;
}
.amp-wp-article-featured-image amp-img {
	margin: 0 auto;
}
.amp-wp-article-featured-image.wp-caption .wp-caption-text {
	margin: 0 18px;
}

/* Article Content */

.amp-wp-article-content {
	margin: 0 16px;
}

.amp-wp-article-content ul,
.amp-wp-article-content ol {
	margin-left: 1em;
}

.amp-wp-article-content .wp-caption {
	max-width: 100%;
}

.amp-wp-article-content amp-img {
	margin: 0 auto;
}

.amp-wp-article-content amp-img.alignright {
	margin: 0 0 1em 16px;
}

.amp-wp-article-content amp-img.alignleft {
	margin: 0 16px 1em 0;
}

/* Captions */

.wp-caption {
	padding: 0;
}

.wp-caption.alignleft {
	margin-right: 16px;
}

.wp-caption.alignright {
	margin-left: 16px;
}

.wp-caption .wp-caption-text {
	border-bottom: 1px solid #c2c2c2;
	color: #696969;
	font-size: .875em;
	line-height: 1.5em;
	margin: 0;
	padding: .66em 10px .75em;
}

/* AMP Media */

.alignwide,
.alignfull {
	clear: both;
}

amp-carousel {
	background: #c2c2c2;
	margin: 0 -16px 1.5em;
}
amp-iframe,
amp-youtube,
amp-instagram,
amp-vine {
	background: #c2c2c2;
	margin: 0 -16px 1.5em;
}

.amp-wp-article-content amp-carousel amp-img {
	border: none;
}

amp-carousel > amp-img > img {
	object-fit: contain;
}

.amp-wp-iframe-placeholder {
	background: #c2c2c2 url( https://uzzz.org/wp-content/plugins/amp/assets/images/placeholder-icon.png ) no-repeat center 40%;
	background-size: 48px 48px;
	min-height: 48px;
}

/* Article Footer Meta */

.amp-wp-article-footer .amp-wp-meta {
	display: block;
}

.amp-wp-tax-category,
.amp-wp-tax-tag {
	color: #696969;
	font-size: .875em;
	line-height: 1.5em;
	margin: 1.5em 16px;
}

.amp-wp-comments-link {
	color: #696969;
	font-size: .875em;
	line-height: 1.5em;
	text-align: center;
	margin: 2.25em 0 1.5em;
}

.amp-wp-comments-link a {
	border-style: solid;
	border-color: #c2c2c2;
	border-width: 1px 1px 2px;
	border-radius: 4px;
	background-color: transparent;
	color: #0a89c0;
	cursor: pointer;
	display: block;
	font-size: 14px;
	font-weight: 600;
	line-height: 18px;
	margin: 0 auto;
	max-width: 200px;
	padding: 11px 16px;
	text-decoration: none;
	width: 50%;
	-webkit-transition: background-color 0.2s ease;
			transition: background-color 0.2s ease;
}

/* AMP Footer */

.amp-wp-footer {
	border-top: 1px solid #c2c2c2;
	margin: calc(1.5em - 1px) 0 0;
}

.amp-wp-footer div {
	margin: 0 auto;
	max-width: calc(840px - 32px);
	padding: 1.25em 16px 1.25em;
	position: relative;
}

.amp-wp-footer h2 {
	font-size: 1em;
	line-height: 1.375em;
	margin: 0 0 .5em;
}

.amp-wp-footer p {
	color: #696969;
	font-size: .8em;
	line-height: 1.5em;
	margin: 0 85px 0 0;
}

.amp-wp-footer a {
	text-decoration: none;
}

.back-to-top {
	bottom: 1.275em;
	font-size: .8em;
	font-weight: 600;
	line-height: 2em;
	position: absolute;
	right: 16px;
}
		/* Inline stylesheets */
.htmledit_views{font-family:-apple-system,SF UI Text,Arial,PingFang SC,Hiragino Sans GB,Microsoft YaHei,WenQuanYi Micro Hei,sans-serif,SimHei,SimSun}.htmledit_views p{font-size:16px;color:#4d4d4d;font-weight:400;line-height:26px;margin:0 0 16px;overflow-x:auto}.htmledit_views amp-img{max-width:100%}.htmledit_views *{box-sizing:border-box}:root:not(#_):not(#_):not(#_):not(#_):not(#_) .amp-wp-db18732{font-size:11pt}:root:not(#_):not(#_):not(#_):not(#_):not(#_) .amp-wp-2ef9d92{line-height:normal}	</style>
</head>

<body class="">

<header id="top" class="amp-wp-header">
	<div>
		<a href="https://uzzz.org/">
										<amp-img src="https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png" width="32" height="32" class="amp-wp-site-icon"></amp-img>
						<span class="amp-site-title">
				有组织在!			</span>
		</a>

					</div>
</header>

<article class="amp-wp-article">
	<header class="amp-wp-article-header">
		<h1 class="amp-wp-title">Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?论文笔记</h1>
			<div class="amp-wp-meta amp-wp-byline">
					<amp-img src="https://secure.gravatar.com/avatar/e786821a74ef0467825a7d60183307bc?s=24&d=mm&r=g" alt="fandyvon" width="24" height="24" layout="fixed"></amp-img>
				<span class="amp-wp-author author vcard">fandyvon</span>
	</div>
<div class="amp-wp-meta amp-wp-posted-on">
	<time datetime="2017-08-18T12:24:22+00:00">
		3 years ago	</time>
</div>
	</header>

	
	<div class="amp-wp-article-content">
		<div id="article_content" class="article_content clearfix">
 
 
<div class="htmledit_views" id="content_views">
<p><span class="fontstyle0">                                   Do Deep Neural Networks Learn Facial Action Units When Doing Expression Recognition?</span></p>
<p><span class="fontstyle0">摘要：</span></p>
<p><span class="fontstyle0">  首先，我们对面部表情数据进行零偏CNN训练，并根据我们的知识，在两个表达式识别基准上实现最先进的表现：扩展Cohn-Kanade（CK +）数据集和多伦多面部数据集（TFD） 。然后，我们通过可视化最大程度地激发卷积层中的不同神经元的空间模式来定性分析网络，并显示它们如何类似于面部动作单元（FAU）。最后，我们使用CK +数据集中提供的FAU标签来验证在我们的过滤器可视化中观察到的FAU确实与被摄取的面部动作一致。<br></span></p>
<p><span class="fontstyle0">1. 引言<br></span></p>
<p><span class="fontstyle0">Paul Ekman提出了面部动作编码系统（FACS），其中列举了这些区域，并描述了每个面部表情如何被描述为多个动作单元（AU）的组合，每个动作单元对应于脸部特定的肌肉组。 然而，让计算机准确地学习传达情感的脸部部分被证明是一件不平凡的事情。<br>   以前的面部表情识别工作可以分为两大类：基于AU的/基于规则的方法和基于外观的方法。 基于AU的方法将明确地检测个体AU的存在，然后基于Friesen和Ekman提出的组合对一个人的情感进行分类。 不幸的是，每个AU检测器都需要仔细的手工工程来确保良好的性能。 另一方面，基于外观的方法从一般的面部形状和纹理模拟了一个人的表情。<br></span></p>
<p><span class="fontstyle0">  在过去几年中，计算机视觉中许多已经存在的问题，从卷积神经网络（CNN）作为一种基于外观的分类器的兴起而受益匪浅。 不幸的是，很少的工作去看看有CNN到底对于识别的提升有多大的帮助。</span></p>
<p><span class="fontstyle0">  在本文中，我们寻求以下问题的答案：CNN可以提高情绪识别数据集/基线的性能，他们学习什么？ 我们建议通过对已建立的面部表情数据集进行CNN训练，然后通过可视化网络中的各个过滤器来分析他们学到的内容来做到这一点。 在这项工作中，我们应用了Zeiler和Fergus [32]和Springenberg等人提出的可视化技术。 其中网络中的单个神经元被激发，并且它们对应的空间模式使用解卷积网络在像素空间中显示。 当可视化这些歧视性的空间格局时，我们发现许多过滤器是由与面部动作单元（FAU）相对应的面部激动的。 这些空间模式的一个子集如图1所示。</span></p>
<p><span class="fontstyle0"><amp-img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170817211846681?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveWluZ2h1YTIwMTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" width="393" height="438" class="amp-wp-enforced-sizes" layout="intrinsic"><noscript><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170817211846681?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveWluZ2h1YTIwMTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" width="393" height="438" class=""></noscript></amp-img><br></span></p>
<p><span class="fontstyle0"><span class="fontstyle0">2. Related Work</span> <br class="amp-wp-2ef9d92"> 在大多数面部表情识别系统中，主要机械与传统机器学习管道相当吻合。 更具体地说，将面部图像传递给试图将其分类为几个（通常为7个）表达类之一的分类器：1.愤怒，2.厌恶，3.恐惧，4.中性，5.快乐，6.悲伤 ，和7.惊喜。 在大多数情况下，在传递给分类器之前，将面部图像预先处理并提供给特征提取器。 直到最近，大多数基于外观的表情识别技术依赖于手工制作的特征，特别是Gabor小波，Haar特征和LBP特征，以便使不同表达类的表示更多辨别。<br></span></p>
<p><span class="fontstyle0"><span class="fontstyle0">3. Our Approach<br></span><span class="fontstyle0 amp-wp-db18732">3.1. Network Architecture</span><br class="amp-wp-2ef9d92"> 对于我们在本文中提出的所有实验，我们使用经典的前馈卷积神经网络。 我们使用的网络在图2中以可视方式显示，分别由具有64,128和256个滤波器的三个卷积层组成，滤波器尺寸分别为5×5，后跟ReLU（整流线性单元）激活功能。<span class="fontstyle0">Max pooling layers </span> 放置在前两个卷积层之后，而在第三个之后应用<span class="fontstyle0">quadrant pooling </span> 。 <span class="fontstyle0">quadrant pooling </span> <br class="amp-wp-2ef9d92"> 之后是一个具有300个隐藏单元的全连接层，最后是一个用于分类的softmax层。<br></span></p>
<p><span class="fontstyle0"><span class="fontstyle0">3.2. Network Training</span> <br class="amp-wp-2ef9d92"> 具体参数 参见论文。</span></p>
<p><span class="fontstyle0"><span class="fontstyle0">4. Experiments and Analysis</span> <br></span></p>
<p><span class="fontstyle0">我们在实验中使用两个面部表情数据集：扩展的Cohn-Kanade数据库（CK +）和多伦多面部数据集（TFD）<br></span></p>
<p><span class="fontstyle0"><span class="fontstyle0">4.1. Performance on Toronto Face Database (TFD)</span> </span></p>
<p><span class="fontstyle0">首先，我们通过对TFD集评估其性能分析CNN。<br class="amp-wp-2ef9d92"><amp-img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170818121615169?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveWluZ2h1YTIwMTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" width="354" height="217" class="amp-wp-enforced-sizes" layout="intrinsic"><noscript><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170818121615169?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveWluZ2h1YTIwMTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" width="354" height="217" class=""></noscript></amp-img><br></span></p>
<p><span class="fontstyle0"><span class="fontstyle0">4.2. Performance on the Extended Cohn-Kanade Dataset (CK+)</span> </span></p>
<p><span class="fontstyle0">我们现在将结果呈现在CK +数据集上。 CK +数据集通常包含八个标签（愤怒，蔑视，厌恶，恐惧，快乐，中立，悲伤和惊喜）。 然而，许多作品忽略了标签为中性或蔑视的样本，只评估了六种基本情绪。 因此，为了确保公平比较，我们培训了两种不同的模式。</span></p>
<p><span class="fontstyle0"><amp-img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170818121835181?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveWluZ2h1YTIwMTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" width="359" height="362" class="amp-wp-enforced-sizes" layout="intrinsic"><noscript><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20170818121835181?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQveWluZ2h1YTIwMTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" width="359" height="362" class=""></noscript></amp-img><br class="amp-wp-2ef9d92"><br></span></p>
<p><span class="fontstyle0">4.3. Visualization of higher-level neurons</span> <br class="amp-wp-2ef9d92"></p>
<p>读者会注意到，CK +区别的空间格局非常明确，并且与面部动作单元很好地对应，例如：AU12：唇角拉拔器（行2,6和9），AU9：鼻子皱纹器（第3行）和AU27： 口腔伸展（第8行）</p>
<p><span class="fontstyle0">4.4. Finding Correspondences Between Filter Activations and the Ground Truth Facial Action Units (FAUs)</span> </p>
<p>因此，我们显示神经网络中的某些神经元隐含地学习在给定相对“松散”的监控信号（即情绪类型：愤怒，快乐，悲伤等）时检测脸部图像中的特定FAU。 最令人鼓舞的是，这些结果似乎证实了我们对CNN如何作为基于外观的分类器的直觉。</p>
<p><span class="fontstyle0">5. Conclusions</span> <br class="amp-wp-2ef9d92"> 在这项工作中，我们在质量和数量上都展现了CNNs进行情感识别的培训，确实能够建立与FAU强烈对应的高级功能。 定性地，我们通过可视化在我们学习的网络的卷积层中最大程度地激发不同过滤器的空间模式，显示了脸部的哪些部分产生了最具歧视性的信息。 同时，定量地，我们使用CK +数据集中给出的FAU标签将可视化滤波器的数值激活与对象的实际面部动作相关联。 最后，我们演示了零偏置CNN如何在扩展的Cohn-Kanade（CK +）数据集和多伦多面部数据集（TFD）上实现最先进的识别精度，<br class="amp-wp-2ef9d92"></p>
</div>
</div>
	</div>

	<footer class="amp-wp-article-footer">
			<div class="amp-wp-meta amp-wp-tax-category">
		Categories: Uncategorized	</div>

	</footer>
</article>

<footer class="amp-wp-footer">
	<div>
		<h2>有组织在!</h2>
		<a href="#top" class="back-to-top">Back to top</a>
	</div>
</footer>



</body>
</html>
