<!DOCTYPE html>
<html amp lang="en-US">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1">
		<title>【论文阅读笔记】Learning to see in the dark – 有组织在!</title>
		<link rel="canonical" href="https://uzzz.org/article/1395/">
	<script type="text/javascript" src="https://cdn.ampproject.org/v0.js" async></script>
<style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>	<script type="application/ld+json">{"@context":"http:\/\/schema.org","publisher":{"@type":"Organization","name":"有组织在!","logo":"https:\/\/uzzz.org\/wp-content\/uploads\/2019\/10\/cropped-icon.png"},"@type":"BlogPosting","mainEntityOfPage":"https:\/\/uzzz.org\/article\/1395\/","headline":"【论文阅读笔记】Learning to see in the dark","datePublished":"2018-05-31T03:15:59+00:00","dateModified":"2018-05-31T03:15:59+00:00","author":{"@type":"Person","name":"fandyvon"}}</script>
	
	<style amp-custom>
		/* Generic WP styling */

.alignright {
	float: right;
}

.alignleft {
	float: left;
}

.aligncenter {
	display: block;
	text-align: center;
	margin-left: auto;
	margin-right: auto;
}

.amp-wp-enforced-sizes {
	/** Our sizes fallback is 100vw, and we have a padding on the container; the max-width here prevents the element from overflowing. **/
	max-width: 100%;
	margin: 0 auto;
}


/*
 * Prevent cases of amp-img converted from img to appear with stretching by using object-fit to scale.
 * See <https://github.com/ampproject/amphtml/issues/21371#issuecomment-475443219>.
 * Also use object-fit:contain in worst case scenario when we can't figure out dimensions for an image.
 * Additionally, in side of \AMP_Img_Sanitizer::determine_dimensions() it could $amp_img->setAttribute( 'object-fit', 'contain' )
 * so that the following rules wouldn't be needed.
 */
amp-img.amp-wp-enforced-sizes[layout="intrinsic"] > img,
amp-anim.amp-wp-enforced-sizes[layout="intrinsic"] > img {
	object-fit: contain;
}

amp-fit-text blockquote,
amp-fit-text h1,
amp-fit-text h2,
amp-fit-text h3,
amp-fit-text h4,
amp-fit-text h5,
amp-fit-text h6 {
	font-size: inherit;
}

/**
 * Override a style rule in Twenty Sixteen and Twenty Seventeen.
 * It set display:none for audio elements.
 * This selector is the same, though it adds body and uses amp-audio instead of audio.
 */
body amp-audio:not([controls]) {
	display: inline-block;
	height: auto;
}

/*
 * Style the default template messages for submit-success, submit-error, and submitting. These elements are inserted
 * by the form sanitizer when a POST form lacks the action-xhr attribute.
 */
.amp-wp-default-form-message > p {
	margin: 1em 0;
	padding: 0.5em;
}

.amp-wp-default-form-message[submitting] > p,
.amp-wp-default-form-message[submit-success] > p.amp-wp-form-redirecting {
	font-style: italic;
}

.amp-wp-default-form-message[submit-success] > p:not(.amp-wp-form-redirecting) {
	border: solid 1px #008000;
	background-color: #90ee90;
	color: #000;
}

.amp-wp-default-form-message[submit-error] > p {
	border: solid 1px #f00;
	background-color: #ffb6c1;
	color: #000;
}

/* Prevent showing empty success message in the case of an AMP-Redirect-To response header. */
.amp-wp-default-form-message[submit-success] > p:empty {
	display: none;
}

amp-carousel .amp-wp-gallery-caption {
	position: absolute;
	bottom: 0;
	left: 0;
	right: 0;
	text-align: center;
	background-color: rgba(0, 0, 0, 0.5);
	color: #fff;
	padding: 1rem;
}

.wp-block-gallery[data-amp-carousel="true"] {
	display: block;
	flex-wrap: unset;
}

/* Template Styles */

.amp-wp-content,
.amp-wp-title-bar div {
		margin: 0 auto;
	max-width: 840px;
	}

html {
	background: #0a89c0;
}

body {
	background: #fff;
	color: #353535;
	font-family: Georgia, 'Times New Roman', Times, Serif;
	font-weight: 300;
	line-height: 1.75em;
}

p,
ol,
ul,
figure {
	margin: 0 0 1em;
	padding: 0;
}

a,
a:visited {
	color: #0a89c0;
}

a:hover,
a:active,
a:focus {
	color: #353535;
}

/* Quotes */

blockquote {
	color: #353535;
	background: rgba(127,127,127,.125);
	border-left: 2px solid #0a89c0;
	margin: 8px 0 24px 0;
	padding: 16px;
}

blockquote p:last-child {
	margin-bottom: 0;
}

/* UI Fonts */

.amp-wp-meta,
.amp-wp-header div,
.amp-wp-title,
.wp-caption-text,
.amp-wp-tax-category,
.amp-wp-tax-tag,
.amp-wp-comments-link,
.amp-wp-footer p,
.back-to-top {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen-Sans", "Ubuntu", "Cantarell", "Helvetica Neue", sans-serif;
}

/* Header */

.amp-wp-header {
	background-color: #0a89c0;
}

.amp-wp-header div {
	color: #fff;
	font-size: 1em;
	font-weight: 400;
	margin: 0 auto;
	max-width: calc(840px - 32px);
	padding: .875em 16px;
	position: relative;
}

.amp-wp-header a {
	color: #fff;
	text-decoration: none;
}


.amp-wp-header .amp-wp-site-icon {
	/** site icon is 32px **/
	background-color: #fff;
	border: 1px solid #fff;
	border-radius: 50%;
	position: absolute;
	right: 18px;
	top: 10px;
}

/* Article */

.amp-wp-article {
	color: #353535;
	font-weight: 400;
	margin: 1.5em auto;
	max-width: 840px;
	overflow-wrap: break-word;
	word-wrap: break-word;
}

/* Article Header */

.amp-wp-article-header {
	align-items: center;
	align-content: stretch;
	display: flex;
	flex-wrap: wrap;
	justify-content: space-between;
	margin: 1.5em 16px 0;
}

.amp-wp-title {
	color: #353535;
	display: block;
	flex: 1 0 100%;
	font-weight: 900;
	margin: 0 0 .625em;
	width: 100%;
}

/* Article Meta */

.amp-wp-meta {
	color: #696969;
	display: inline-block;
	flex: 2 1 50%;
	font-size: .875em;
	line-height: 1.5em;
	margin: 0 0 1.5em;
	padding: 0;
}

.amp-wp-article-header .amp-wp-meta:last-of-type {
	text-align: right;
}

.amp-wp-article-header .amp-wp-meta:first-of-type {
	text-align: left;
}

.amp-wp-byline amp-img,
.amp-wp-byline .amp-wp-author {
	display: inline-block;
	vertical-align: middle;
}

.amp-wp-byline amp-img {
	border: 1px solid #0a89c0;
	border-radius: 50%;
	position: relative;
	margin-right: 6px;
}

.amp-wp-posted-on {
	text-align: right;
}

/* Featured image */

.amp-wp-article-featured-image {
	margin: 0 0 1em;
}
.amp-wp-article-featured-image amp-img {
	margin: 0 auto;
}
.amp-wp-article-featured-image.wp-caption .wp-caption-text {
	margin: 0 18px;
}

/* Article Content */

.amp-wp-article-content {
	margin: 0 16px;
}

.amp-wp-article-content ul,
.amp-wp-article-content ol {
	margin-left: 1em;
}

.amp-wp-article-content .wp-caption {
	max-width: 100%;
}

.amp-wp-article-content amp-img {
	margin: 0 auto;
}

.amp-wp-article-content amp-img.alignright {
	margin: 0 0 1em 16px;
}

.amp-wp-article-content amp-img.alignleft {
	margin: 0 16px 1em 0;
}

/* Captions */

.wp-caption {
	padding: 0;
}

.wp-caption.alignleft {
	margin-right: 16px;
}

.wp-caption.alignright {
	margin-left: 16px;
}

.wp-caption .wp-caption-text {
	border-bottom: 1px solid #c2c2c2;
	color: #696969;
	font-size: .875em;
	line-height: 1.5em;
	margin: 0;
	padding: .66em 10px .75em;
}

/* AMP Media */

.alignwide,
.alignfull {
	clear: both;
}

amp-carousel {
	background: #c2c2c2;
	margin: 0 -16px 1.5em;
}
amp-iframe,
amp-youtube,
amp-instagram,
amp-vine {
	background: #c2c2c2;
	margin: 0 -16px 1.5em;
}

.amp-wp-article-content amp-carousel amp-img {
	border: none;
}

amp-carousel > amp-img > img {
	object-fit: contain;
}

.amp-wp-iframe-placeholder {
	background: #c2c2c2 url( https://uzzz.org/wp-content/plugins/amp/assets/images/placeholder-icon.png ) no-repeat center 40%;
	background-size: 48px 48px;
	min-height: 48px;
}

/* Article Footer Meta */

.amp-wp-article-footer .amp-wp-meta {
	display: block;
}

.amp-wp-tax-category,
.amp-wp-tax-tag {
	color: #696969;
	font-size: .875em;
	line-height: 1.5em;
	margin: 1.5em 16px;
}

.amp-wp-comments-link {
	color: #696969;
	font-size: .875em;
	line-height: 1.5em;
	text-align: center;
	margin: 2.25em 0 1.5em;
}

.amp-wp-comments-link a {
	border-style: solid;
	border-color: #c2c2c2;
	border-width: 1px 1px 2px;
	border-radius: 4px;
	background-color: transparent;
	color: #0a89c0;
	cursor: pointer;
	display: block;
	font-size: 14px;
	font-weight: 600;
	line-height: 18px;
	margin: 0 auto;
	max-width: 200px;
	padding: 11px 16px;
	text-decoration: none;
	width: 50%;
	-webkit-transition: background-color 0.2s ease;
			transition: background-color 0.2s ease;
}

/* AMP Footer */

.amp-wp-footer {
	border-top: 1px solid #c2c2c2;
	margin: calc(1.5em - 1px) 0 0;
}

.amp-wp-footer div {
	margin: 0 auto;
	max-width: calc(840px - 32px);
	padding: 1.25em 16px 1.25em;
	position: relative;
}

.amp-wp-footer h2 {
	font-size: 1em;
	line-height: 1.375em;
	margin: 0 0 .5em;
}

.amp-wp-footer p {
	color: #696969;
	font-size: .8em;
	line-height: 1.5em;
	margin: 0 85px 0 0;
}

.amp-wp-footer a {
	text-decoration: none;
}

.back-to-top {
	bottom: 1.275em;
	font-size: .8em;
	font-weight: 600;
	line-height: 2em;
	position: absolute;
	right: 16px;
}
		/* Inline stylesheets */
.htmledit_views{font-family:-apple-system,SF UI Text,Arial,PingFang SC,Hiragino Sans GB,Microsoft YaHei,WenQuanYi Micro Hei,sans-serif,SimHei,SimSun}.htmledit_views a>amp-img{padding:1px;margin:1px;border:none;outline:#0782c1 solid 1px}.htmledit_views p{font-size:16px;color:#4d4d4d;font-weight:400;line-height:26px;margin:0 0 16px;overflow-x:auto}p[align=center]{text-align:center}.htmledit_views amp-img{max-width:100%}.htmledit_views *{box-sizing:border-box}.htmledit_views pre{white-space:pre-wrap;word-wrap:break-word;margin:0 0 24px;overflow-x:auto;padding:8px}.htmledit_views pre{font-family:Consolas,Inconsolata,Courier,monospace;font-size:14px;line-height:22px;color:#000}.htmledit_views a{color:#4ea1db;text-decoration:none}.htmledit_views a:focus,.htmledit_views a:hover{color:#ca0c16}.htmledit_views a:visited{color:#6795b5}:root:not(#_):not(#_):not(#_):not(#_):not(#_) .amp-wp-1811b95{padding-right:0px;padding-left:0px;font-size:14px;line-height:22px;text-align:center;background:#fff}:root:not(#_):not(#_):not(#_):not(#_):not(#_) .amp-wp-9bd93d3{font-size:12px}:root:not(#_):not(#_):not(#_):not(#_):not(#_) .amp-wp-4d7a92b{text-align:center;background:#fff}:root:not(#_):not(#_):not(#_):not(#_):not(#_) .amp-wp-cf38033{color:#666;font-size:12px}:root:not(#_):not(#_):not(#_):not(#_):not(#_) .amp-wp-fae3f9f{font-size:12px;color:#666}:root:not(#_):not(#_):not(#_):not(#_):not(#_) .amp-wp-cc568fe{font-weight:700}:root:not(#_):not(#_):not(#_):not(#_):not(#_) .amp-wp-1edd6f9{background:#fff}:root:not(#_):not(#_):not(#_):not(#_):not(#_) .amp-wp-7a0a0f9{background-color:#fff}:root:not(#_):not(#_):not(#_):not(#_):not(#_) .amp-wp-902aa38{color:#666}:root:not(#_):not(#_):not(#_):not(#_):not(#_) .amp-wp-2e9f4e7{color:#666}	</style>
</head>

<body class="">

<header id="top" class="amp-wp-header">
	<div>
		<a href="https://uzzz.org/">
										<amp-img src="https://uzzz.org/wp-content/uploads/2019/10/cropped-icon-32x32.png" width="32" height="32" class="amp-wp-site-icon"></amp-img>
						<span class="amp-site-title">
				有组织在!			</span>
		</a>

					</div>
</header>

<article class="amp-wp-article">
	<header class="amp-wp-article-header">
		<h1 class="amp-wp-title">【论文阅读笔记】Learning to see in the dark</h1>
			<div class="amp-wp-meta amp-wp-byline">
					<amp-img src="https://secure.gravatar.com/avatar/e786821a74ef0467825a7d60183307bc?s=24&d=mm&r=g" alt="fandyvon" width="24" height="24" layout="fixed"></amp-img>
				<span class="amp-wp-author author vcard">fandyvon</span>
	</div>
<div class="amp-wp-meta amp-wp-posted-on">
	<time datetime="2018-05-31T11:15:59+00:00">
		2 years ago	</time>
</div>
	</header>

	
	<div class="amp-wp-article-content">
		<div id="article_content" class="article_content clearfix">
 
 
<div class="htmledit_views" id="content_views">
<p>     本文是CVPR2018论文，主要提出一种通过FCN方法将在黑暗环境中进行的拍摄还原的方法，实现让机器<span class="amp-wp-2e9f4e7">让机器“看破”黑暗。本文的主要创新点为：</span></p>

<p><span class="amp-wp-2e9f4e7">      1.提出了一个新的照片数据集，包含原始的</span><span class="amp-wp-902aa38">short-exposure low-light图像，并附有</span><span class="amp-wp-902aa38">long-exposure reference图像作为Groud truth，以往类似的研究使用的都是人工合成的图像；</span></p>
<p><span class="amp-wp-902aa38">       2.与以往方法使用相机拍摄出的sRGB图像进行复原不同，本文使用的是原始的传感器数据。</span></p>
<p><span class="amp-wp-902aa38">       3.<span class="amp-wp-2e9f4e7">提出了一种端到端的学习方法，通过训练一个全卷积网络FCN</span>来直接处理快速成像系统中的低亮度图像。结构如图：</span></p>
<p><span class="amp-wp-902aa38"><amp-img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180531110855773" alt="" width="554" height="141" class="amp-wp-enforced-sizes" layout="intrinsic"><noscript><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180531110855773" alt="" width="554" height="141" class=""></noscript></amp-img><br></span></p>
<p><span class="amp-wp-902aa38">      本文最后提出了该模型待改进的几个地方：</span></p>
<p><span class="amp-wp-902aa38">      1.数据集中目前不包含人和运动物体；</span></p>
<p><span class="amp-wp-902aa38">      2.模型中的放大率amplification ratio是人工选择的，如果能根据图像自动选择，效果会更好。</span></p>
<p><span class="amp-wp-902aa38">      3.可以进行进一步的运行时优化，目前处理一幅照片的时间不能满足实时处理的时限要求。</span></p>
<p><span class="amp-wp-2e9f4e7">………………………………………………………………………………………………………………………………………………………………………………….</span></p>

<p><span class="amp-wp-2e9f4e7">下面的内容转载自：</span><a href="https://blog.csdn.net/linchunmian/article/details/80291921" rel="nofollow" data-token="41360b0a5552a1423ce411fbc87bbcd0">https://blog.csdn.net/linchunmian/article/details/80291921</a>，个人认为是对本文比较好的一篇翻译：</p>

<p class="amp-wp-7a0a0f9"><span class="amp-wp-902aa38">整理下最近一篇论文的学习笔记。这是由UIUC的陈晨和Intel Labs的陈启峰、许佳、Vladlen Koltun 合作提出的一种在黑暗中也能快速、清晰的成像系统，让机器“看破”黑暗。以下是论文的主要部分。</span></p>
<p class="amp-wp-7a0a0f9">
</p><p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38"><span class="amp-wp-cc568fe">摘要</span></span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">在暗光条件下，受到低信噪比和低亮度的影响，图片的质量会受到很大的影响。此外，低曝光率的照片会出现很多噪声，而长曝光时间会让照片变得模糊、不真实。目前，很多关于去噪、去模糊、图像增强等技术的研究已被相继提出，但是在一些极端条件下，这些技术的作用就很有限了。为了发展基于学习的低亮度图像处理技术，本文提出了一种在黑暗中也能快速、清晰的成像系统，效果令人非常惊讶。此外，我们引入了一个数据集，包含有原始的低曝光率、低亮度图片，同时还有对应的长曝光率图像。利用该数据集，提出了一种端到端训练模式的全卷积网络结构，用于处理低亮度图像。该网络直接使用原始传感器数据，并替代了大量的传统图像处理流程。最终，实验结果表明这种网络结构在新数据集上能够表现出出色的性能，并在未来工作中有很大前途。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38"><span class="amp-wp-cc568fe">简介</span></span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">任何的图像成像系统都存在噪声，但这很大地影响在弱光条件下图像的质量。高ISO 可以用于增加亮度，但它同时也会放大噪音。诸如缩放或直方图拉伸等图像后处理可以缓解这种噪声影响，但这并不能从根本上解决低信噪比 (SNR) 问题。在物理学上，这可以解释为在弱光条件下增加SNR，包括开放光圈，延长曝光时间以及使用闪光灯等，但这些也都有其自身的缺陷。例如，曝光时间的延长可能会引起相机抖动或物体运动模糊。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">众所周知，暗光条件下的快速成像系统一直都是计算摄影界的一大挑战，也是一直以来开放性的研究领域。目前，许多关于图像去噪，去模糊和低光图像增强等技术相继提出，但这些技术通常假设这些在昏暗环境下捕获到的图像带有中等程度的噪音。相反，我们更感兴趣的是在极端低光条件下，如光照严重受限 (例如月光) 和短时间曝光 (理想情况下是视频率) 等条件下的图像成像系统。在这种情况下，传统相机的处理方式显然已不适用，图像必须根据原始的传感器数据来重建。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">为此，本文提出了一种新的图像处理技术：通过一种数据驱动的方法来解决极端低光条件下快速成像系统的挑战。具体来说，我们训练深度神经网络来学习低光照条件下原始数据的图像处理技术，包括颜色转换，去马赛克，降噪和图像增强等。我们通过端对端的训练方式来避免放大噪声，还能表征这种环境下传统相机处理的累积误差。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">据我们所知，现有用于处理低光图像的方法，在合成数据或真实的低光图像上测试都缺乏事实根据。此外，用于处理不同真实环境下的低光图像数据集也相当匮乏。因此，我们收集了一个在低光条件下快速曝光的原始图像数据集。每个低光图像都有对应的长曝光时间的高质量图像用于参考。在新的数据集上我们的方法表现出不出色的结果：将低光图像放大300倍，成功减少了图像中的噪音并正确实现了颜色转换。我们系统地分析方法中的关键要素并讨论未来的研究方向。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">下图1展示了我们的设置。我们可以看到，在很高的ISO 8,000条件下，尽管使用全帧的索尼高光灵敏度相机，但相机仍会产生全黑的图像。在ISO 409,600条件下，图像仍会产生朦胧，嘈杂，颜色扭曲等现象。换而言之，即使是当前最先进的图像去噪技术也无法消除这种噪音，也无法解决颜色偏差问题。而我们提出的全卷积网络结构能够有效地克服这些问题。</span></p>
<p align="left" class="amp-wp-4d7a92b"><span class="amp-wp-902aa38"><amp-img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153439309?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" width="550" height="108" class="amp-wp-enforced-sizes" layout="intrinsic"><noscript><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153439309?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" width="550" height="108" class=""></noscript></amp-img><br></span></p>
<pre class="amp-wp-1811b95"><span class="amp-wp-9bd93d3"><span class="amp-wp-902aa38">图1卷积网络下的极端低光成像。黑暗的室内环境：:相机的照度 <0.1 lux。Sony α7S II传感器曝光1/30秒。</span>左图：<span class="amp-wp-902aa38">ISO 8,000</span><span class="amp-wp-902aa38">相机产生的图像。中间图：</span><span class="amp-wp-902aa38">ISO 409,600</span><span class="amp-wp-902aa38">相机产生的图像，图像受到噪声和颜色偏差的影响。右图：由我们的全卷积网络生生的图像。</span></span></pre>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-cc568fe"><span class="amp-wp-902aa38">数据集 (SID)</span></span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">我们收集了一个新的数据集，用于原始低光图像的训练和基准测试。See-in-the-Dark(SID) 数据集包含5094张原始的短曝光图像，每张都有相应的长曝光时间的参考图像。值得注意的是，多张短曝光的图像可以对应于相同的长曝光时间的参考图像。例如，我们收集了短时间曝光图像用于评估去燥方法。序列中的每张图像都可视为一张独特的低光图像，这样包含真实世界伪像的图片能够更有利于模型的训练和培训测试。SID 数据集中长时间曝光的参考图像是424。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">此外，我们的数据集包含了室内和室外图像。室外图像通常是在月光或街道照明条件下拍摄。在室外场景下，相机的亮度一般在0.2 lux 和5 lux 之间。室内图像通常更暗。在室内场景中的相机亮度一般在0.03 lux 和0.3 lux 之间。输入图像的曝光时间设置为1/30和1/10秒。相应的参考图像 (真实图像) 的曝光时间通常会延长100到300倍：即10至30秒。各数据集的具体情况如下表1中所示。</span></p>
<p align="left" class="amp-wp-4d7a92b"><span class="amp-wp-902aa38"><amp-img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153544352?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" width="378" height="182" class="amp-wp-enforced-sizes" layout="intrinsic"><noscript><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153544352?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" width="378" height="182" class=""></noscript></amp-img><br></span></p>
<p align="center" class="amp-wp-1edd6f9"><span class="amp-wp-fae3f9f">表1. SID 数据集包含5094个原始的短曝光率图像，每张图像都有一个长曝光的参考图像。图像由顶部和底部两台相机收集得到。表中的指标参数分别是(从左到右)：输入与参考图像之间的曝光时间率，滤波器阵列，输入图像的曝光时间以及在每种条件下的图像数量。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">下图2显示了数据集中一部分的参考图像。在每种条件下，我们随机选择大约20％的图像是组成测试集，另外选定10％的数据用于模型验证。</span></p>
<p align="left" class="amp-wp-4d7a92b"><span class="amp-wp-902aa38"><amp-img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153622727?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" width="335" height="293" class="amp-wp-enforced-sizes" layout="intrinsic"><noscript><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153622727?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" width="335" height="293" class=""></noscript></amp-img><br></span></p>
<p align="center" class="amp-wp-1edd6f9"><span class="amp-wp-fae3f9f">图2 SID 数据库的实例。前两行是SID 数据集中室外的图像，底部两行是室内的图像。长曝光时间的参考图像 (地面实况) 显示在前面。短曝光的输入图像(基本上是黑色) 显示在背部。室外场景下相机的亮度一般在0.2到5 lux，而室内的相机亮度在0.03和0.3 lux 之间。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">数据采集时，相机固定在三脚架上。我们用无反光镜相机来避免由于镜面拍打引起的振动。在每个场景中，相机设置 (如光圈，ISO，焦距和焦距) 进行了调整，以最大限度地提高参考图像(长曝光时间)的质量。此外，利用远程的智能手机 app 将曝光时间缩短一倍缩小后的曝光时间为100至300。该相机专门用于参考图像 (长曝光时间) 的拍摄，而没有触及短曝光的图像。我们收集了一系列短曝光的图像用于方法的比较和评估，以突出我们方法的优势。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">虽然，数据集中的参考图像仍可能存在一些噪音，但感知质量足够高。我们目的是为了在光线不足的条件下产生在感知良好的图像，而不是彻底删除图像中所有噪音或最大化图像对比度。因此，这种参考图像的存在不会影响我们的方法评估。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-cc568fe"><span class="amp-wp-902aa38">方法</span></span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">从成像传感器中得到原始数据后，传统图像处理过程会应用一系列模块，例如白平衡、去马赛克、去噪、增加图像锐度、 γ 矫正等等。而这些图像处理模块只在某些特定相机中才有。一些研究提出使用局部线性、可学习的L3 过滤器来模拟现代成像系统中复杂的非线性流程，但是这些方法都无法成功解决在低光条件中快速成像的问题，也无法解决极低的SNR 问题。此外，通过智能手机相机拍摄的照片，利用bursting imaging成像方法，结合多张图像也可以生成效果较好的图像，但是这种方法的复杂程度较高。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">因此，我们提出了的端到端的学习方法，即训练一个全卷积网络FCN 来直接处理快速成像系统中的低亮度图像。纯粹的FCN 结构可以有效地代表许多图像处理算法。受此启发，我们调查并研究这种方法在极端低光条件下成像系统的应用。相比于传统图像处理方法使用的sRGB 图像，在这里我们使用原始传感器数据。下图3展示了我们所提出的方法结构。</span></p>
<p align="left" class="amp-wp-4d7a92b"><span class="amp-wp-902aa38"><amp-img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153658118?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" width="554" height="141" class="amp-wp-enforced-sizes" layout="intrinsic"><noscript><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153658118?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" width="554" height="141" class=""></noscript></amp-img><br></span></p>
<p align="center" class="amp-wp-1edd6f9"><span class="amp-wp-fae3f9f">图3 我们提出的图像处理方法</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">对于 Bayer 数组，我们将输入打包为四个通道并在每个通道上将空间分辨率降低一半。对于X-Trans 数组(图中未显示出)，原始数据以6×6排列块组成;我们通过交换相邻通道元素的方法将36个通道的数组打包成9个通道。此外，我们消除黑色像素并按照期望的倍数缩放数据(例如，x100或x300)。将处理后数据作为 FCN 模型的输入，输出是一个带12通道的图像，其空间分辨率只有输入的一半。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">我们将两个标准的 FCN 结构作为我们模型的核心架构：用于快速图像处理的多尺度上下文聚合网络 (CAN) 和U-net 网络。影响我们模型选择的另一个因素是内存消耗：在 GPU 中，我们选择的模型结构可以处理全分辨率的图像(例如，在4240×2832或6000×4000分辨率)。同时，我们避免使用完全连接结构及模型集成方式。我们的默认架构是 U-net。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">放大比率决定了模型的亮度输出。在我们的方法中，放大比率设置在外部指定并作为输入提供给模型，这类似于相机中的 ISO 设置。下图4显示了不同放大比率的影响。用户可以通过设置不同的放大率来调整输出图像的亮度。在测试时间，我们的方法能够抑制盲点噪声并实现颜色转换，并在sRGB 空间网络直接处理图像，得到网络的输出。</span></p>
<p align="left" class="amp-wp-4d7a92b"><span class="amp-wp-902aa38"><amp-img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153724857?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" width="554" height="114" class="amp-wp-enforced-sizes" layout="intrinsic"><noscript><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153724857?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" width="554" height="114" class=""></noscript></amp-img><br></span></p>
<p align="center" class="amp-wp-1edd6f9"><span class="amp-wp-fae3f9f">图4 SID 数据集中放大系数对室内图像 (Sony x100子集) 的影响。类似于摄像机中的ISO设置，这里的放大系数是作为外部输入提供给我们的模型。更高的放大倍数可以产生更明亮的图像。我们在我们的方法中引入了不同的放大因子，并展示了模型的输出图像。</span></p>
<p class="amp-wp-1edd6f9"><span class="amp-wp-cc568fe"><span class="amp-wp-902aa38">模型训练</span></span></p>
<p class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">我们使用 L1 损失和 Adam 优化器，从零开始训练我们的网络。在训练期间，网络输入是原始的短曝光图像，在 sRGB 空间中的真实数据是相应的长曝光时间图像(由一个原始图像处理库 libraw 处理过得参考图像)。我们为每台相机训练一个网络，并将原始图像和参考图像之间曝光时间的倍数差作为我们的放大因子(例如，x100，x250，或x300)。在每次训练迭代中，我们随机裁剪一个512×512的补丁用于训练并利用翻转、旋转等操作来随机增强数据。初始学习率设定为0.0001，在2000次迭代后学习率降为0.00001，训练一共进行4000次迭代。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-cc568fe"><span class="amp-wp-902aa38">实验结果分析</span></span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">首先，与传统方法的对比，我们提出的方法具有放大的功能。如下图5,6,7所示，传统的图像处理方法在极端低光条件下容易受到严重的噪声影响，导致生成图像颜色失真。我们提出的方法能够有效地抑制图像噪声，生成色彩均衡、逼真的图像。此外，由于在数据集中对齐数据序列，我们的方法也更优于 post-hocdenoising、brust denoising 等图像去燥方法</span></p>
<p align="center" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38"><amp-img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153750652?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" width="555" height="141" class="amp-wp-enforced-sizes" layout="intrinsic"><noscript><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153750652?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" width="555" height="141" class=""></noscript></amp-img></span></p>
<p align="center" class="amp-wp-1edd6f9"><span class="amp-wp-fae3f9f">图5 (a) 由富士胶片 X-T2 相机拍摄的夜间图像，ISO 800，光圈f / 7.1，曝光时间1/30s，相机亮度约为1 lux。(b) 传统的图像处理方法不能有效处理原始数据中的噪声和颜色偏差。(c) 基于相同的数据，我们方法处理的结果。</span></p>
<p align="center" class="amp-wp-1edd6f9"><span class="amp-wp-fae3f9f"><amp-img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153810914?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" width="385" height="279" class="amp-wp-enforced-sizes" layout="intrinsic"><noscript><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153810914?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" width="385" height="279" class=""></noscript></amp-img><br></span></p>
<p align="center" class="amp-wp-1edd6f9"><span class="amp-wp-fae3f9f">图6 将 SID 数据集训练好的模型应用于用 iPhone 6s智能手机拍摄的原始低光图像。(a) iPhone 6s 在夜间所拍摄的原始图像，ISO 400，光圈f/2.2，曝光时间0. 05s。经传统的图像处理方法处理后的图像及缩放到相匹配的亮度的参考图像。</span><span class="amp-wp-cf38033">(b)</span><span class="amp-wp-cf38033">我们提出的方法处理后的结果</span></p>
<p align="center" class="amp-wp-1edd6f9"><span class="amp-wp-cf38033"><amp-img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153840238?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" width="554" height="143" class="amp-wp-enforced-sizes" layout="intrinsic"><noscript><img src="https://uzshare.com/_p?https://img-blog.csdn.net/20180512153840238?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpbmNodW5taWFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" width="554" height="143" class=""></noscript></amp-img><br></span></p>
<p align="center" class="amp-wp-1edd6f9"><span class="amp-wp-fae3f9f">图7 Sony x300拍摄的图像。(a) 由传统图像处理方法处理的低光图像及其线性缩放的结果。(b) 同样用传统方法，并通过 BM3D 去噪方法处理后的结果。 (c) 我们提出的方法处理后的结果。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">此外，我们还进行了一系列的控制实验，来分析方法中各组分对模型性能的影响，包括模型结构，输入的颜色空间，损失函数，数据排列，图像后处理等因素。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-cc568fe"><span class="amp-wp-902aa38">结语</span></span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">由于图像低光子数和低信噪比的影响，快速低光成像系统是一个艰巨的挑战。黑暗中快速成像系统更是被认为是一种不切实际、与传统的信号处理相悖的技术。在本文中，我们创建了一个黑暗中的图像数据集 (SID) 以支持数据驱动方法的研究。利用 SID 数据集，我们提出一种基于 FCN 模型结构，通过端到端训练，改善了传统的处理低光图像的方法。实验结果表明我们的方法能够成功抑制噪声并正确地实现颜色转换，表现出不错的性能，并展现了不错的研究前景。未来的工作我们可以进一步研究低光成像网络的泛化能力。此外，对于模型的性能优化也是值得研究的一个热点方向。我们还将在未来的工作中进一步改善图像质量，如通过系统优化网络架构和训练程序。我们希望SID 数据集和我们的实验结果可以支持并刺激未来该领域的研究。</span></p>
<p align="left" class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">原文链接：</span><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1805.01934" rel="nofollow noopener noreferrer" data-token="10354b74d4deff9e5f377b2ee3e5bfbf">arxiv.org/abs/1805.01934</a></p>
<p class="amp-wp-1edd6f9"><span class="amp-wp-902aa38">项目地址：<a href="https://link.zhihu.com/?target=http%3A//web.engr.illinois.edu/~cchen156/SID.html" rel="nofollow noopener noreferrer" data-token="0d2205017e859bc5979af930e9998c38">web.engr.illinois.edu/~cchen156/SID.html</a></span></p>
<p class="amp-wp-7a0a0f9"><span class="amp-wp-902aa38">GitHub地址：<a href="https://link.zhihu.com/?target=https%3A//github.com/cchen156/Learning-to-See-in-the-Dark" rel="nofollow noopener noreferrer" data-token="1e92eca0f5ba1ab05c99e2ebc61066ef">github.com/cchen156/Learning-to-See-in-the-Dark</a></span></p>
<p class="amp-wp-7a0a0f9"><span class="amp-wp-902aa38">以上是这篇论文解读的内容。欢迎指点补充。</span></p>
<p>
  <span class="amp-wp-2e9f4e7"></span></p>
</div>
</div>
	</div>

	<footer class="amp-wp-article-footer">
			<div class="amp-wp-meta amp-wp-tax-category">
		Categories: <a href="https://uzzz.org/category/deeplearning/" rel="category tag">DeepLearning</a>	</div>

	</footer>
</article>

<footer class="amp-wp-footer">
	<div>
		<h2>有组织在!</h2>
		<a href="#top" class="back-to-top">Back to top</a>
	</div>
</footer>



</body>
</html>
